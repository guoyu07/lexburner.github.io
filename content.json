{"meta":{"title":"徐靖峰|个人博客","subtitle":null,"description":null,"author":"徐靖峰","url":"http://lexburner.github.io"},"pages":[{"title":"Tags","date":"2017-08-21T06:03:05.883Z","updated":"2017-08-21T06:03:05.883Z","comments":true,"path":"tags/index.html","permalink":"http://lexburner.github.io/tags/index.html","excerpt":"","text":""},{"title":"徐靖峰","date":"2017-08-22T04:47:32.980Z","updated":"2017-08-22T04:47:32.980Z","comments":true,"path":"about/index.html","permalink":"http://lexburner.github.io/about/index.html","excerpt":"","text":"生于1995年，江苏泰州人，毕业于常州大学，目前就职于中科软，地点位于上海浦东新区，从事业务开发和基础架构研发工作。 大一时有幸加入校ACM队，使用C/C++，对算法和编程产生了浓厚的兴趣。大四来上海中科软担任JAVA后端实习后，就职至今。对如今主流的互联网技术均有不同程度的掌握，尤其擅长后端技术，对软件设计有自己的理解，运维测试方面有过涉猎，前端为弱势项。平时喜欢分享知识，交流见闻，博客旧址：徐靖峰的CSDN。 coding之外，有如下的兴趣爱好： 看动漫(bilibili)，看电影，看直播(douyu) 游戏(毕业后已经很少接触) 写日记&amp;感想，思考 旅游摄影 最近在研究领域驱动设计，ELK，openresty。在项目不忙时，会经常更新博客，记录自己对开发设计的理解。如果你对博客中有任何的疑问&amp;建议，欢迎与我交流。"},{"title":"Categories","date":"2017-08-21T10:10:46.417Z","updated":"2017-08-21T06:03:05.876Z","comments":true,"path":"categories/index.html","permalink":"http://lexburner.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"中文文案排版指北","slug":"chinese-copywriting-guidelines","date":"2018-01-16T12:16:28.000Z","updated":"2018-01-16T02:04:55.874Z","comments":true,"path":"2018/01/16/chinese-copywriting-guidelines/","link":"","permalink":"http://lexburner.github.io/2018/01/16/chinese-copywriting-guidelines/","excerpt":"","text":"统一中文文案、排版的相关用法，降低团队成员之间的沟通成本，增强网站气质。原文出处：https://github.com/mzlogin/chinese-copywriting-guidelines 目录 空格 中英文之间需要增加空格 中文与数字之间需要增加空格 数字与单位之间需要增加空格 全角标点与其他字符之间不加空格 -ms-text-autospace to the rescue? 标点符号 不重复使用标点符号 全角和半角 使用全角中文标点 数字使用半角字符 遇到完整的英文整句、特殊名词，其內容使用半角标点 名词 专有名词使用正确的大小写 不要使用不地道的缩写 争议 链接之间增加空格 简体中文使用直角引号 工具 谁在这样做？ 参考文献 空格「有研究显示，打字的时候不喜欢在中文和英文之间加空格的人，感情路都走得很辛苦，有七成的比例会在 34 岁的时候跟自己不爱的人结婚，而其余三成的人最后只能把遗产留给自己的猫。毕竟爱情跟书写都需要适时地留白。 与大家共勉之。」——vinta/paranoid-auto-spacing 中英文之间需要增加空格正确： 在 LeanCloud 上，数据存储是围绕 AVObject 进行的。 错误： 在LeanCloud上，数据存储是围绕AVObject进行的。 在 LeanCloud上，数据存储是围绕AVObject 进行的。 完整的正确用法： 在 LeanCloud 上，数据存储是围绕 AVObject 进行的。每个 AVObject 都包含了与 JSON 兼容的 key-value 对应的数据。数据是 schema-free 的，你不需要在每个 AVObject 上提前指定存在哪些键，只要直接设定对应的 key-value 即可。 例外：「豆瓣FM」等产品名词，按照官方所定义的格式书写。 中文与数字之间需要增加空格正确： 今天出去买菜花了 5000 元。 错误： 今天出去买菜花了 5000元。 今天出去买菜花了5000元。 数字与单位之间需要增加空格正确： 我家的光纤入户宽带有 10 Gbps，SSD 一共有 20 TB。 错误： 我家的光纤入户宽带有 10Gbps，SSD 一共有 10TB。 例外：度／百分比与数字之间不需要增加空格： 正确： 今天是 233° 的高温。 新 MacBook Pro 有 15% 的 CPU 性能提升。 错误： 今天是 233 ° 的高温。 新 MacBook Pro 有 15 % 的 CPU 性能提升。 全角标点与其他字符之间不加空格正确： 刚刚买了一部 iPhone，好开心！ 错误： 刚刚买了一部 iPhone ，好开心！ -ms-text-autospace to the rescue?Microsoft 有个 -ms-text-autospace.aspx) 的 CSS 属性可以实现自动为中英文之间增加空白。不过目前并未普及，另外在其他应用场景，例如 OS X、iOS 的用户界面目前并不存在这个特性，所以请继续保持随手加空格的习惯。 标点符号不重复使用标点符号正确： 德国队竟然战胜了巴西队！ 她竟然对你说「喵」？！ 错误： 德国队竟然战胜了巴西队！！ 德国队竟然战胜了巴西队！！！！！！！！ 她竟然对你说「喵」？？！！ 她竟然对你说「喵」？！？！？？！！ 全角和半角不明白什么是全角（全形）与半角（半形）符号？请查看维基百科词条『全角和半角』。 使用全角中文标点正确： 嗨！你知道嘛？今天前台的小妹跟我说「喵」了哎！ 核磁共振成像（NMRI）是什么原理都不知道？JFGI！ 错误： 嗨! 你知道嘛? 今天前台的小妹跟我说 “喵” 了哎! 嗨!你知道嘛?今天前台的小妹跟我说”喵”了哎! 核磁共振成像 (NMRI) 是什么原理都不知道? JFGI! 核磁共振成像(NMRI)是什么原理都不知道?JFGI! 数字使用半角字符正确： 这件蛋糕只卖 1000 元。 错误： 这件蛋糕只卖 １０００ 元。 例外：在设计稿、宣传海报中如出现极少量数字的情形时，为方便文字对齐，是可以使用全角数字的。 遇到完整的英文整句、特殊名词，其內容使用半角标点正确： 乔布斯那句话是怎么说的？「Stay hungry, stay foolish.」 推荐你阅读《Hackers &amp; Painters: Big Ideas from the Computer Age》，非常的有趣。 错误： 乔布斯那句话是怎么说的？「Stay hungry，stay foolish。」 推荐你阅读《Hackers＆Painters：Big Ideas from the Computer Age》，非常的有趣。 名词专有名词使用正确的大小写大小写相关用法原属于英文书写范畴，不属于本 wiki 讨论內容，在这里只对部分易错用法进行简述。 正确： 使用 GitHub 登录 我们的客户有 GitHub、Foursquare、Microsoft Corporation、Google、Facebook, Inc.。 错误： 使用 github 登录 使用 GITHUB 登录 使用 Github 登录 使用 gitHub 登录 使用 gｲんĤЦ8 登录 我们的客户有 github、foursquare、microsoft corporation、google、facebook, inc.。 我们的客户有 GITHUB、FOURSQUARE、MICROSOFT CORPORATION、GOOGLE、FACEBOOK, INC.。 我们的客户有 Github、FourSquare、MicroSoft Corporation、Google、FaceBook, Inc.。 我们的客户有 gitHub、fourSquare、microSoft Corporation、google、faceBook, Inc.。 我们的客户有 gｲんĤЦ8、ｷouЯƧquﾑгє、๓เςг๏ร๏Ŧt ς๏гק๏гคtเ๏ภn、900913、ƒ4ᄃëв๏๏к, IПᄃ.。 注意：当网页中需要配合整体视觉风格而出现全部大写／小写的情形，HTML 中请使用标准的大小写规范进行书写；并通过 text-transform: uppercase;／text-transform: lowercase; 对表现形式进行定义。 不要使用不地道的缩写正确： 我们需要一位熟悉 JavaScript、HTML5，至少理解一种框架（如 Backbone.js、AngularJS、React 等）的前端开发者。 错误： 我们需要一位熟悉 Js、h5，至少理解一种框架（如 backbone、angular、RJS 等）的 FED。 争议以下用法略带有个人色彩，即：无论是否遵循下述规则，从语法的角度来讲都是正确的。 链接之间增加空格用法： 请 提交一个 issue 并分配给相关同事。 访问我们网站的最新动态，请 点击这里 进行订阅！ 对比用法： 请提交一个 issue 并分配给相关同事。 访问我们网站的最新动态，请点击这里进行订阅！ 简体中文使用直角引号用法： 「老师，『有条不紊』的『紊』是什么意思？」 对比用法： “老师，‘有条不紊’的‘紊’是什么意思？” 工具 仓库 语言 vinta/paranoid-auto-spacing JavaScript huei90/pangu.node Node.js huacnlee/auto-correct Ruby sparanoid/space-lover PHP (WordPress) nauxliu/auto-correct PHP ricoa/copywriting-correct PHP hotoo/pangu.vim Vim sparanoid/grunt-auto-spacing Node.js (Grunt) hjiang/scripts/add-space-between-latin-and-cjk Python 谁在这样做？ 网站 文案 UGC Apple 中国 Yes N/A Apple 香港 Yes N/A Apple 台湾 Yes N/A Microsoft 中国 Yes N/A Microsoft 香港 Yes N/A Microsoft 台湾 Yes N/A LeanCloud Yes N/A 知乎 Yes 部分用户达成 V2EX Yes Yes SegmentFault Yes 部分用户达成 Apple4us Yes N/A 豌豆荚 Yes N/A Ruby China Yes 标题达成 PHPHub Yes 标题达成 少数派 Yes N/A 参考文献 Guidelines for Using Capital Letters Letter case - Wikipedia Punctuation - Oxford Dictionaries Punctuation - The Purdue OWL How to Use English Punctuation Corrently - wikiHow 格式 - openSUSE 全角和半角 - 维基百科 引号 - 维基百科 疑问惊叹号 - 维基百科","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/categories/技术杂谈/"}],"tags":[{"name":"中文排版","slug":"中文排版","permalink":"http://lexburner.github.io/tags/中文排版/"}]},{"title":"研究优雅停机时的一点思考","slug":"gracefully-shutdown","date":"2018-01-14T12:16:28.000Z","updated":"2018-01-15T13:15:38.802Z","comments":true,"path":"2018/01/14/gracefully-shutdown/","link":"","permalink":"http://lexburner.github.io/2018/01/14/gracefully-shutdown/","excerpt":"","text":"开头先废话几句，有段时间没有更新博客了，除了公司项目比较忙之外，还有个原因就是开始思考如何更好地写作。远的来说，我从大一便开始在 CSDN 上写博客，回头看那时的文笔还很稚嫩，一心想着反正只有自己看，所以更多的是随性发挥，随意吐槽，内容也很简陋：刷完一道算法题记录下解题思路，用 JAVA 写完一个 demo 之后，记录下配置步骤。近的来看，工作之后开始维护自己的博客站点: www.cnkirito.moe，也会同步更新自己公众号。相比圈子里其他前辈来说，读者会少很多，但毕竟有人看，每次动笔之前便会开始思考一些事。除了给自己的学习经历做一个归档，还多了一些顾虑：会不会把知识点写错？会不会误人子弟？自己的理解会不会比较片面，不够深刻？等等等等。但自己的心路历程真的发生了一些改变。在我还是个小白的时候，学习技术：第一个想法是百度，搜别人的博客，一步步跟着别人后面配置，把 demo run 起来。而现在，遇到问题的第一思路变成了：源码 debug，官方文档。我便开始思考官方文档和博客的区别，官方文档的优势除了更加全面之外，还有就是：“它只教你怎么做”，对于一个有经验有阅历的程序员来说，这反而是好事，这可以让你有自己的思考。而博客则不一样，如果这个博主特别爱 BB，便会产生很多废话（就像本文的第一段），它会有很多作者自己思考的产物，一方面它比官方文档更容易出错，更容易片面，一方面它比官方文档更容易启发人，特别是读到触动到我的好文时，会抑制不住内心的喜悦想要加到作者的好友，这便是共情。我之后的文章也会朝着这些点去努力：不避重就轻，多思考不想当然，求精。 最近瞥了一眼项目的重启脚本，发现运维一直在使用 kill -9 &lt;pid&gt; 的方式重启 springboot embedded tomcat，其实大家几乎一致认为：kill -9 &lt;pid&gt; 的方式比较暴力，但究竟会带来什么问题却很少有人能分析出个头绪。这篇文章主要记录下自己的思考过程。 kill -9 和 kill -15 有什么区别？在以前，我们发布 WEB 应用通常的步骤是将代码打成 war 包，然后丢到一个配置好了应用容器（如 Tomcat，Weblogic）的 Linux 机器上，这时候我们想要启动/关闭应用，方式很简单，运行其中的启动/关闭脚本即可。而 springboot 提供了另一种方式，将整个应用连同内置的 tomcat 服务器一起打包，这无疑给发布应用带来了很大的便捷性，与之而来也产生了一个问题：如何关闭 springboot 应用呢？一个显而易见的做法便是，根据应用名找到进程 id，杀死进程 id 即可达到关闭应用的效果。 上述的场景描述引出了我的疑问：怎么优雅地杀死一个 springboot 应用进程呢？这里仅仅以最常用的 Linux 操作系统为例，在 Linux 中 kill 指令负责杀死进程，其后可以紧跟一个数字，代表信号编号(Signal)，执行 kill -l 指令，可以一览所有的信号编号。 12xu@ntzyz-qcloud ~ % kill -l HUP INT QUIT ILL TRAP ABRT BUS FPE KILL USR1 SEGV USR2 PIPE ALRM TERM STKFLT CHLD CONT STOP TSTP TTIN TTOU URG XCPU XFSZ VTALRM PROF WINCH POLL PWR SYS 本文主要介绍下第 9 个信号编码 KILL，以及第 15 个信号编号 TERM 。 先简单理解下这两者的区别：kill -9 pid 可以理解为操作系统从内核级别强行杀死某个进程，kill -15 pid 则可以理解为发送一个通知，告知应用主动关闭。这么对比还是有点抽象，那我们就从应用的表现来看看，这两个命令杀死应用到底有啥区别。 代码准备 由于笔者 springboot 接触较多，所以以一个简易的 springboot 应用为例展开讨论，添加如下代码。 1 增加一个实现了 DisposableBean 接口的类 1234567@Componentpublic class TestDisposableBean implements DisposableBean&#123; @Override public void destroy() throws Exception &#123; System.out.println(\"测试 Bean 已销毁 ...\"); &#125;&#125; 2 增加 JVM 关闭时的钩子 1234567891011121314@SpringBootApplication@RestControllerpublic class TestShutdownApplication implements DisposableBean &#123; public static void main(String[] args) &#123; SpringApplication.run(TestShutdownApplication.class, args); Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"执行 ShutdownHook ...\"); &#125; &#125;)); &#125;&#125; 测试步骤 执行 java -jar test-shutdown-1.0.jar 将应用运行起来 测试 kill -9 pid，kill -15 pid，ctrl + c 后输出日志内容 测试结果 kill -15 pid &amp; ctrl + c，效果一样，输出结果如下 123452018-01-14 16:55:32.424 INFO 8762 --- [ Thread-3] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2cdf8d8a: startup date [Sun Jan 14 16:55:24 UTC 2018]; root of context hierarchy2018-01-14 16:55:32.432 INFO 8762 --- [ Thread-3] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown执行 ShutdownHook ...测试 Bean 已销毁 ...java -jar test-shutdown-1.0.jar 7.46s user 0.30s system 80% cpu 9.674 total kill -9 pid，没有输出任何应用日志 12[1] 8802 killed java -jar test-shutdown-1.0.jarjava -jar test-shutdown-1.0.jar 7.74s user 0.25s system 41% cpu 19.272 total 可以发现，kill -9 pid 是给应用杀了个措手不及，没有留给应用任何反应的机会。而反观 kill -15 pid，则比较优雅，先是由AnnotationConfigEmbeddedWebApplicationContext （一个 ApplicationContext 的实现类）收到了通知，紧接着执行了测试代码中的 Shutdown Hook，最后执行了 DisposableBean#destory() 方法。孰优孰劣，立判高下。 一般我们会在应用关闭时处理一下“善后”的逻辑，比如 关闭 socket 链接 清理临时文件 发送消息通知给订阅方，告知自己下线 将自己将要被销毁的消息通知给子进程 各种资源的释放 等等 而 kill -9 pid 则是直接模拟了一次系统宕机，系统断电，这对于应用来说太不友好了，不要用收割机来修剪花盆里的花。取而代之，便是使用 kill -15 pid 来代替。如果在某次实际操作中发现：kill -15 pid 无法关闭应用，则可以考虑使用内核级别的 kill -9 pid ，但请事后务必排查出是什么原因导致 kill -15 pid 无法关闭。 springboot 如何处理 -15 TERM Signal上面解释过了，使用 kill -15 pid 的方式可以比较优雅的关闭 springboot 应用，我们可能有以下的疑惑： springboot/spring 是如何响应这一关闭行为的呢？是先关闭了 tomcat，紧接着退出 JVM，还是相反的次序？它们又是如何互相关联的？ 尝试从日志开始着手分析，AnnotationConfigEmbeddedWebApplicationContext 打印出了 Closing 的行为，直接去源码中一探究竟，最终在其父类 AbstractApplicationContext 中找到了关键的代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243@Overridepublic void registerShutdownHook() &#123; if (this.shutdownHook == null) &#123; this.shutdownHook = new Thread() &#123; @Override public void run() &#123; synchronized (startupShutdownMonitor) &#123; doClose(); &#125; &#125; &#125;; Runtime.getRuntime().addShutdownHook(this.shutdownHook); &#125;&#125;@Overridepublic void close() &#123; synchronized (this.startupShutdownMonitor) &#123; doClose(); if (this.shutdownHook != null) &#123; Runtime.getRuntime().removeShutdownHook(this.shutdownHook); &#125; &#125;&#125;protected void doClose() &#123; if (this.active.get() &amp;&amp; this.closed.compareAndSet(false, true)) &#123; LiveBeansView.unregisterApplicationContext(this); // 发布应用内的关闭事件 publishEvent(new ContextClosedEvent(this)); // Stop all Lifecycle beans, to avoid delays during individual destruction. if (this.lifecycleProcessor != null) &#123; this.lifecycleProcessor.onClose(); &#125; // spring 的 BeanFactory 可能会缓存单例的 Bean destroyBeans(); // 关闭应用上下文&amp;BeanFactory closeBeanFactory(); // 执行子类的关闭逻辑 onClose(); this.active.set(false); &#125;&#125; 为了方便排版以及便于理解，我去除了源码中的部分异常处理代码，并添加了相关的注释。在容器初始化时，ApplicationContext 便已经注册了一个 Shutdown Hook，这个钩子调用了 Close() 方法，于是当我们执行 kill -15 pid 时，JVM 接收到关闭指令，触发了这个 Shutdown Hook，进而由 Close() 方法去处理一些善后手段。具体的善后手段有哪些，则完全依赖于 ApplicationContext 的 doClose() 逻辑，包括了注释中提及的销毁缓存单例对象，发布 close 事件，关闭应用上下文等等，特别的，当 ApplicationContext 的实现类是 AnnotationConfigEmbeddedWebApplicationContext 时，还会处理一些 tomcat/jetty 一类内置应用服务器关闭的逻辑。 窥见了 springboot 内部的这些细节，更加应该了解到优雅关闭应用的必要性。JAVA 和 C 都提供了对 Signal 的封装，我们也可以手动捕获操作系统的这些 Signal，在此不做过多介绍，有兴趣的朋友可以自己尝试捕获下。 还有其他优雅关闭应用的方式吗？spring-boot-starter-actuator 模块提供了一个 restful 接口，用于优雅停机。 添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 添加配置 1234#启用shutdownendpoints.shutdown.enabled=true#禁用密码验证endpoints.shutdown.sensitive=false 生产中请注意该端口需要设置权限，如配合 spring-security 使用。 执行 curl -X POST host:port/shutdown 指令，关闭成功便可以获得如下的返回： 1&#123;\"message\":\"Shutting down, bye...\"&#125; 虽然 springboot 提供了这样的方式，但按我目前的了解，没见到有人用这种方式停机，kill -15 pid 的方式达到的效果与此相同，将其列于此处只是为了方案的完整性。 如何销毁作为成员变量的线程池？尽管 JVM 关闭时会帮我们回收一定的资源，但一些服务如果大量使用异步回调，定时任务，处理不当很有可能会导致业务出现问题，在这其中，线程池如何关闭是一个比较典型的问题。 123456789101112@Servicepublic class SomeService &#123; ExecutorService executorService = Executors.newFixedThreadPool(10); public void concurrentExecute() &#123; executorService.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"executed...\"); &#125; &#125;); &#125;&#125; 我们需要想办法在应用关闭时（JVM 关闭，容器停止运行），关闭线程池。 初始方案：什么都不做。在一般情况下，这不会有什么大问题，因为 JVM 关闭，会释放之，但显然没有做到本文一直在强调的两个字，没错—-优雅。 方法一的弊端在于线程池中提交的任务以及阻塞队列中未执行的任务变得极其不可控，接收到停机指令后是立刻退出？还是等待任务执行完成？抑或是等待一定时间任务还没执行完成则关闭？ 方案改进： 发现初始方案的劣势后，我立刻想到了使用 DisposableBean 接口，像这样： 1234567891011121314151617181920@Servicepublic class SomeService implements DisposableBean&#123; ExecutorService executorService = Executors.newFixedThreadPool(10); public void concurrentExecute() &#123; executorService.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"executed...\"); &#125; &#125;); &#125; @Override public void destroy() throws Exception &#123; executorService.shutdownNow(); //executorService.shutdown(); &#125;&#125; 紧接着问题又来了，是 shutdown 还是 shutdownNow 呢？这两个方法还是经常被误用的，简单对比这两个方法。 ThreadPoolExecutor 在 shutdown 之后会变成 SHUTDOWN 状态，无法接受新的任务，随后等待正在执行的任务执行完成。意味着，shutdown 只是发出一个命令，至于有没有关闭还是得看线程自己。 ThreadPoolExecutor 对于 shutdownNow 的处理则不太一样，方法执行之后变成 STOP 状态，并对执行中的线程调用 Thread.interrupt() 方法（但如果线程未处理中断，则不会有任何事发生），所以并不代表“立刻关闭”。 查看 shutdown 和 shutdownNow 的 java doc，会发现如下的提示： shutdown() ：Initiates an orderly shutdown in which previously submitted tasks are executed, but no new tasks will be accepted.Invocation has no additional effect if already shut down.This method does not wait for previously submitted tasks to complete execution.Use {@link #awaitTermination awaitTermination} to do that. shutdownNow()：Attempts to stop all actively executing tasks, halts the processing of waiting tasks, and returns a list of the tasks that were awaiting execution. These tasks are drained (removed) from the task queue upon return from this method.This method does not wait for actively executing tasks to terminate. Use {@link #awaitTermination awaitTermination} to do that.There are no guarantees beyond best-effort attempts to stop processing actively executing tasks. This implementation cancels tasks via {@link Thread#interrupt}, so any task that fails to respond to interrupts may never terminate. 两者都提示我们需要额外执行 awaitTermination 方法，仅仅执行 shutdown/shutdownNow 是不够的。 最终方案：参考 spring 中线程池的回收策略，我们得到了最终的解决方案。 1234567891011121314151617181920212223242526272829303132333435363738public abstract class ExecutorConfigurationSupport extends CustomizableThreadFactory implements DisposableBean&#123; @Override public void destroy() &#123; shutdown(); &#125; /** * Perform a shutdown on the underlying ExecutorService. * @see java.util.concurrent.ExecutorService#shutdown() * @see java.util.concurrent.ExecutorService#shutdownNow() * @see #awaitTerminationIfNecessary() */ public void shutdown() &#123; if (this.waitForTasksToCompleteOnShutdown) &#123; this.executor.shutdown(); &#125; else &#123; this.executor.shutdownNow(); &#125; awaitTerminationIfNecessary(); &#125; /** * Wait for the executor to terminate, according to the value of the * &#123;@link #setAwaitTerminationSeconds \"awaitTerminationSeconds\"&#125; property. */ private void awaitTerminationIfNecessary() &#123; if (this.awaitTerminationSeconds &gt; 0) &#123; try &#123; this.executor.awaitTermination(this.awaitTerminationSeconds, TimeUnit.SECONDS)); &#125; catch (InterruptedException ex) &#123; Thread.currentThread().interrupt(); &#125; &#125; &#125;&#125; 保留了注释，去除了一些日志代码，一个优雅关闭线程池的方案呈现在我们的眼前。 1 通过 waitForTasksToCompleteOnShutdown 标志来控制是想立刻终止所有任务，还是等待任务执行完成后退出。 2 executor.awaitTermination(this.awaitTerminationSeconds, TimeUnit.SECONDS)); 控制等待的时间，防止任务无限期的运行（前面已经强调过了，即使是 shutdownNow 也不能保证线程一定停止运行）。 更多需要我们的思考的优雅停机策略在我们分析 RPC 原理的系列文章里面曾经提到，服务治理框架一般会考虑到优雅停机的问题。通常的做法是事先隔断流量，接着关闭应用。常见的做法是将服务节点从注册中心摘除，订阅者接收通知，移除节点，从而优雅停机；涉及到数据库操作，则可以使用事务的 ACID 特性来保证即使 crash 停机也能保证不出现异常数据，正常下线则更不用说了；又比如消息队列可以依靠 ACK 机制+消息持久化，或者是事务消息保障；定时任务较多的服务，处理下线则特别需要注意优雅停机的问题，因为这是一个长时间运行的服务，比其他情况更容易受停机问题的影响，可以使用幂等和标志位的方式来设计定时任务… 事务和 ACK 这类特性的支持，即使是宕机，停电，kill -9 pid 等情况，也可以使服务尽量可靠；而同样需要我们思考的还有 kill -15 pid，正常下线等情况下的停机策略。最后再补充下整理这个问题时，自己对 jvm shutdown hook 的一些理解。 When the virtual machine begins its shutdown sequence it will start all registered shutdown hooks in some unspecified order and let them run concurrently. When all the hooks have finished it will then run all uninvoked finalizers if finalization-on-exit has been enabled. Finally, the virtual machine will halt. shutdown hook 会保证 JVM 一直运行，知道 hook 终止 (terminated)。这也启示我们，如果接收到 kill -15 pid 命令时，执行阻塞操作，可以做到等待任务执行完成之后再关闭 JVM。同时，也解释了一些应用执行 kill -15 pid 无法退出的问题，没错，中断被阻塞了。 参考资料 [1] https://stackoverflow.com/questions/2921945/useful-example-of-a-shutdown-hook-in-java [2] spring 源码 [3] jdk 文档","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"}]},{"title":"深入理解RPC之服务注册与发现篇","slug":"rpc-registry","date":"2018-01-05T12:16:28.000Z","updated":"2018-01-08T01:36:15.674Z","comments":true,"path":"2018/01/05/rpc-registry/","link":"","permalink":"http://lexburner.github.io/2018/01/05/rpc-registry/","excerpt":"","text":"在我们之前 RPC 原理的分析中，主要将笔墨集中在 Client 和 Server 端。而成熟的服务治理框架中不止存在这两个角色，一般还会有一个 Registry（注册中心）的角色。一张图就可以解释注册中心的主要职责。 注册中心，用于服务端注册远程服务以及客户端发现服务 服务端，对外提供后台服务，将自己的服务信息注册到注册中心 客户端，从注册中心获取远程服务的注册信息，然后进行远程过程调用 目前主要的注册中心可以借由 zookeeper，eureka，consul，etcd 等开源框架实现。互联网公司也会因为自身业务的特性自研，如美团点评自研的 MNS，新浪微博自研的 vintage。 本文定位是对注册中心有一定了解的读者，所以不过多阐述注册中心的基础概念。 注册中心的抽象借用开源框架中的核心接口，可以帮助我们从一个较为抽象的高度去理解注册中心。例如 motan 中的相关接口： 服务注册接口 123456789101112public interface RegistryService &#123; //1. 向注册中心注册服务 void register(URL url); //2. 从注册中心摘除服务 void unregister(URL url); //3. 将服务设置为可用，供客户端调用 void available(URL url); //4. 禁用服务，客户端无法发现该服务 void unavailable(URL url); //5. 获取已注册服务的集合 Collection&lt;URL&gt; getRegisteredServiceUrls();&#125; 服务发现接口 12345678public interface DiscoveryService &#123; //1. 订阅服务 void subscribe(URL url, NotifyListener listener); //2. 取消订阅 void unsubscribe(URL url, NotifyListener listener); //3. 发现服务列表 List&lt;URL&gt; discover(URL url);&#125; 主要使用的方法是 RegistryService#register(URL) 和 DiscoveryService#discover(URL)。其中这个 URL 参数被传递，显然也是很重要的一个类。 123456789public class URL &#123; private String protocol;//协议名称 private String host; private int port; // interfaceName,也代表着路径 private String path; private Map&lt;String, String&gt; parameters; private volatile transient Map&lt;String, Number&gt; numbers;&#125; 注册中心也没那么玄乎，其实可以简单理解为：提供一个存储介质，供服务提供者和服务消费者共同连接，而存储的主要信息就是这里的 URL。但是具体 URL 都包含了什么实际信息，我们还没有一个直观的感受。 注册信息概览以元老级别的注册中心 zookeeper 为例，看看它实际都存储了什么信息以及它是如何持久化上一节的 URL。 为了测试，我创建了一个 RPC 服务接口 com.sinosoft.student.api.DemoApi ,并且在 6666 端口暴露了这个服务的实现类，将其作为服务提供者。在 6667 端口远程调用这个服务，作为服务消费者。两者都连接本地的 zookeeper，本机 ip 为 192.168.150.1。 使用 zkClient.bash 或者 zkClient.sh 作为客户端连接到本地的 zookeeper，执行如下的命令： 12[zk: localhost:2181(CONNECTED) 1] ls /motan/demo_group/com.sinosoft.student.api.DemoApi&gt; [client, server, unavailableServer] zookeeper 有着和 linux 类似的命令和结构，其中 motan，demo_group，com.sinosoft.student.api.DemoApi，client, server, unavailableServer 都是一个个节点。可以从上述命令看出他们的父子关系。 /motan/demo_group/com.sinosoft.student.api.DemoApi 的结构为 /框架标识/分组名/接口名，其中的分组是 motan 为了隔离不同组的服务而设置的。这样，接口名称相同，分组不同的服务无法互相发现。如果此时有一个分组名为 demo_group2 的服务，接口名称为 DemoApi2，则 motan 会为其创建一个新的节点 /motan/demo_group2/com.sinosoft.student.api.DemoApi2 而 client，server，unavailableServer 则就是服务注册与发现的核心节点了。我们先看看这些节点都存储了什么信息。 server 节点： 12345[zk: localhost:2181(CONNECTED) 2] ls /motan/demo_group/com.sinosoft.student.api.DemoApi/server&gt; [192.168.150.1:6666][zk: localhost:2181(CONNECTED) 3] get /motan/demo_group/com.sinosoft.student.api.DemoApi/server/192.168.150.1:6666&gt; motan://192.168.150.1:6666/com.sinosoft.student.api.DemoApi?serialization=hessian2&amp;protocol=motan&amp;isDefault=true&amp;maxContentLength=1548576&amp;shareChannel=true&amp;refreshTimestamp=1515122649835&amp;id=motanServerBasicConfig&amp;nodeType=service&amp;export=motan:6666&amp;requestTimeout=9000000&amp;accessLog=false&amp;group=demo_group&amp; client 节点： 1234[zk: localhost:2181(CONNECTED) 4] ls /motan/demo_group/com.sinosoft.student.api.DemoApi/client&gt; [192.168.150.1][zk: localhost:2181(CONNECTED) 5] get /motan/demo_group/com.sinosoft.student.api.DemoApi/client/192.168.150.1&gt; motan://192.168.150.1:0/com.sinosoft.student.api.DemoApi?singleton=true&amp;maxContentLength=1548576&amp;check=false&amp;nodeType=service&amp;version=1.0&amp;throwException=true&amp;accessLog=false&amp;serialization=hessian2&amp;retries=0&amp;protocol=motan&amp;isDefault=true&amp;refreshTimestamp=1515122631758&amp;id=motanClientBasicConfig&amp;requestTimeout=9000&amp;group=demo_group&amp; unavailableServer 节点是一个过渡节点，所以在一切正常的情况下不会存在信息，它的具体作用在下面会介绍。 从这些输出数据可以发现，注册中心承担的一个职责就是存储服务调用中相关的信息，server 向 zookeeper 注册信息，保存在 server 节点，而 client 实际和 server 共享同一个接口，接口名称就是路径名，所以也到达了同样的 server 节点去获取信息。并且同时注册到了 client 节点下（为什么需要这么做在下面介绍）。 注册信息详解Server 节点server 节点承担着最重要的职责，它由服务提供者创建，以供服务消费者获取节点中的信息，从而定位到服务提供者真正网络拓扑位置以及得知如何调用。demo 中我只在本机 [192.168.150.1:6666] 启动了一个实例，所以在server 节点之下，只存在这么一个节点，继续 get 这个节点，可以获取更详细的信息 1motan://192.168.150.1:6666/com.sinosoft.student.api.DemoApi?serialization=hessian2&amp;protocol=motan&amp;isDefault=true&amp;maxContentLength=1548576&amp;shareChannel=true&amp;refreshTimestamp=1515122649835&amp;id=motanServerBasicConfig&amp;nodeType=service&amp;export=motan:6666&amp;requestTimeout=9000000&amp;accessLog=false&amp;group=demo_group&amp; 作为一个 value 值，它和 http 协议的请求十分相似，不过是以 motan:// 开头，表达的意图也很明确，这是 motan 协议和相关的路径及参数，关于 RPC 中的协议，可以翻看我的上一篇文章《深入理解RPC之协议篇》。 serialization 对应序列化方式，protocol 对应协议名称，maxContentLength 对应 RPC 传输中数据报文的最大长度，shareChannel 是传输层用到的参数，netty channel 中的一个属性，group 对应分组名称。 上述的 value 包含了 RPC 调用中所需要的全部信息。 Client 节点在 motan 中使用 zookeeper 作为注册中心时，客户端订阅服务时会向 zookeeper 注册自身，主要是方便对调用方进行统计、管理。但订阅时是否注册 client 不是必要行为，和不同的注册中心实现有关，例如使用 consul 时便没有注册。 由于我们使用 zookeeper，也可以分析下 zookeeper 中都注册了什么信息。 1motan://192.168.150.1:0/com.sinosoft.student.api.DemoApi?singleton=true&amp;maxContentLength=1548576&amp;check=false&amp;nodeType=service&amp;version=1.0&amp;throwException=true&amp;accessLog=false&amp;serialization=hessian2&amp;retries=0&amp;protocol=motan&amp;isDefault=true&amp;refreshTimestamp=1515122631758&amp;id=motanClientBasicConfig&amp;requestTimeout=9000&amp;group=demo_group 和 Server 节点的值类似，但也有客户独有的一些属性，如 singleton 代表服务是否单例，check 检查服务提供者是否存在，retries 代表重试次数，这也是 RPC 中特别需要注意的一点。 UnavailableServer 节点unavailableServer 节点也不是必须存在的一个节点，它主要用来做 server 端的延迟上线，优雅关机。 延迟上线：一般推荐的服务端启动流程为：server 向注册中心的 unavailableServer 注册，状态为 unavailable，此时整个服务处于启动状态，但不对外提供服务，在服务验证通过，预热完毕，此时打开心跳开关，此时正式提供服务。 优雅关机：当需要对 server 方进行维护升级时，如果直接关闭，则会影响到客户端的请求。所以理想的情况应当是首先切断流量，再进行 server 的下线。具体的做法便是：先关闭心跳开关，客户端感知停止调用后，再关闭服务进程。 感知服务的下线服务上线时自然要注册到注册中心，但下线时也得从注册中心中摘除。注册是一个主动的行为，这没有特别要注意的地方，但服务下线却是一个值得思考的问题。服务下线包含了主动下线和系统宕机等异常方式的下线。 临时节点+长连接在 zookeeper 中存在持久化节点和临时节点的概念。持久化节点一经创建，只要不主动删除，便会一直持久化存在；临时节点的生命周期则是和客户端的连接同生共死的，应用连接到 zookeeper 时创建一个临时节点，使用长连接维持会话，这样无论何种方式服务发生下线，zookeeper 都可以感知到，进而删除临时节点。zookeeper 的这一特性和服务下线的需求契合的比较好，所以临时节点被广泛应用。 ###主动下线+心跳检测 并不是所有注册中心都有临时节点的概念，另外一种感知服务下线的方式是主动下线。例如在 eureka 中，会有 eureka-server 和 eureka-client 两个角色，其中 eureka-server 保存注册信息，地位等同于 zookeeper。当 eureka-client 需要关闭时，会发送一个通知给 eureka-server，从而让 eureka-server 摘除自己这个节点。但这么做最大的一个问题是，如果仅仅只有主动下线这么一个手段，一旦 eureka-client 非正常下线（如断电，断网），eureka-server 便会一直存在一个已经下线的服务节点，一旦被其他服务发现进而调用，便会带来问题。为了避免出现这样的情况，需要给 eureka-server 增加一个心跳检测功能，它会对服务提供者进行探测，比如每隔30s发送一个心跳，如果三次心跳结果都没有返回值，就认为该服务已下线。 注册中心对比 Feature Consul zookeeper etcd euerka 服务健康检查 服务状态，内存，硬盘等 (弱)长连接，keepalive 连接心跳 可配支持 多数据中心 支持 — — — kv存储服务 支持 支持 支持 — 一致性 raft paxos raft — cap ca cp cp ap 使用接口(多语言能力) 支持http和dns 客户端 http/grpc http（sidecar） watch支持 全量/支持long polling 支持 支持 long polling 支持 long polling/大部分增量 自身监控 metrics — metrics metrics 安全 acl /https acl https支持（弱） — spring cloud集成 已支持 已支持 已支持 已支持 一般而言注册中心的特性决定了其使用的场景，例如很多框架支持 zookeeper，在我自己看来是因为其老牌，易用，但业界也有很多人认为 zookeeper 不适合做注册中心，它本身是一个分布式协调组件，并不是为注册服务而生，server 端注册一个服务节点，client 端并不需要在同一时刻拿到完全一致的服务列表，只要最终一致性即可。在跨IDC，多数据中心等场景下 consul 发挥了很大的优势，这也是很多互联网公司选择使用 consul 的原因。 eureka 是 ap 注册中心，并且是 spring cloud 默认使用的组件，spring cloud eureka 较为贴近 spring cloud 生态。 总结注册中心主要用于解耦服务调用中的定位问题，是分布式系统必须面对的一个问题。更多专业性的对比，可以期待 spring4all.com 的注册中心专题讨论，相信会有更为细致地对比。","categories":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/categories/RPC/"}],"tags":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/tags/RPC/"}]},{"title":"深入理解RPC之协议篇","slug":"rpc-protocol","date":"2017-12-28T12:16:28.000Z","updated":"2017-12-29T07:40:41.416Z","comments":true,"path":"2017/12/28/rpc-protocol/","link":"","permalink":"http://lexburner.github.io/2017/12/28/rpc-protocol/","excerpt":"","text":"协议（Protocol）是个很广的概念，RPC 被称为远程过程调用协议，HTTP 和 TCP 也是大家熟悉的协议，也有人经常拿 RPC 和 RESTFUL 做对比，后者也可以被理解为一种协议… 我个人偏向于把“协议”理解为不同厂家不同用户之间的“约定”，而在 RPC 中，协议的含义也有多层。 Protocol 在 RPC 中的层次关系翻看 dubbo 和 motan 两个国内知名度数一数二的 RPC 框架（或者叫服务治理框架可能更合适）的文档，他们都有专门的一章介绍自身对多种协议的支持。RPC 框架是一个分层结构，从我的这个《深入理解RPC》系列就可以看出，是按照分层来介绍 RPC 的原理的，前面已经介绍过了传输层，序列化层，动态代理层，他们各自负责 RPC 调用生命周期中的一环，而协议层则是凌驾于它们所有层之上的一层。简单描述下各个层之间的关系： protocol 层主要用于配置 refer（发现服务） 和 exporter（暴露服务） 的实现方式，transport 层定义了传输的方式，codec 层诠释了具体传输过程中报文解析的方式，serialize 层负责将对象转换成字节，以用于传输，proxy 层负责将这些细节屏蔽。 它们的包含关系如下：protocol &gt; transport &gt; codec &gt; serialize motan 的 Protocol 接口可以佐证这一点： 12345public interface Protocol &#123; &lt;T&gt; Exporter&lt;T&gt; export(Provider&lt;T&gt; provider, URL url); &lt;T&gt; Referer&lt;T&gt; refer(Class&lt;T&gt; clz, URL url, URL serviceUrl); void destroy();&#125; 我们都知道 RPC 框架支持多种协议，由于协议处于框架层次的较高位置，任何一种协议的替换，都可能会导致服务发现和服务注册的方式，传输的方式，以及序列化的方式，而不同的协议也给不同的业务场景带来了更多的选择，下面就来看看一些常用协议。 Dubbo 中的协议dubbo://Dubbo 缺省协议采用单一长连接和 NIO 异步通讯，适合于小数据量高并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。 反之，Dubbo 缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。 适用场景：常规远程服务方法调用 rmi://RMI 协议采用 JDK 标准的 java.rmi.* 实现，采用阻塞式短连接和 JDK 标准序列化方式。 适用场景：常规远程服务方法调用，与原生RMI服务互操作 hessian://Hessian 协议用于集成 Hessian 的服务，Hessian 底层采用 Http 通讯，采用 Servlet 暴露服务，Dubbo 缺省内嵌 Jetty 作为服务器实现。 Dubbo 的 Hessian 协议可以和原生 Hessian 服务互操作，即： 提供者用 Dubbo 的 Hessian 协议暴露服务，消费者直接用标准 Hessian 接口调用 或者提供方用标准 Hessian 暴露服务，消费方用 Dubbo 的 Hessian 协议调用。 Hessian 在之前介绍过，当时仅仅是用它来作为序列化工具，但其本身其实就是一个协议，可以用来做远程通信。 适用场景：页面传输，文件传输，或与原生hessian服务互操作 http://基于 HTTP 表单的远程调用协议，采用 Spring 的 HttpInvoker 实现 适用场景：需同时给应用程序和浏览器 JS 使用的服务。 webserivice://基于 WebService 的远程调用协议，基于 Apache CXF 的 frontend-simple 和 transports-http 实现。 可以和原生 WebService 服务互操作，即： 提供者用 Dubbo 的 WebService 协议暴露服务，消费者直接用标准 WebService 接口调用， 或者提供方用标准 WebService 暴露服务，消费方用 Dubbo 的 WebService 协议调用 适用场景：系统集成，跨语言调用 thrift://当前 dubbo 支持的 thrift 协议是对 thrift 原生协议的扩展，在原生协议的基础上添加了一些额外的头信息，比如 service name，magic number 等。 memcached://基于 memcached 实现的 RPC 协议 redis://基于 Redis 实现的 RPC 协议。 dubbo 支持的众多协议详见 http://dubbo.io/books/dubbo-user-book/references/protocol/dubbo.html dubbo的一个分支 dangdangdotcom/dubbox 扩展了 REST 协议 rest://JAX-RS 是标准的 Java REST API，得到了业界的广泛支持和应用，其著名的开源实现就有很多，包括 Oracle 的 Jersey，RedHat 的 RestEasy，Apache 的 CXF 和 Wink，以及 restlet 等等。另外，所有支持 JavaEE 6.0 以上规范的商用 JavaEE 应用服务器都对 JAX-RS 提供了支持。因此，JAX-RS 是一种已经非常成熟的解决方案，并且采用它没有任何所谓 vendor lock-in 的问题。 JAX-RS 在网上的资料非常丰富，例如下面的入门教程： Oracle 官方的 tutorial：http://docs.oracle.com/javaee/7/tutorial/doc/jaxrs.htm IBM developerWorks 中国站文章：http://www.ibm.com/developerworks/cn/java/j-lo-jaxrs/ 更多的资料请自行 google 或者百度一下。就学习 JAX-RS 来说，一般主要掌握其各种 annotation 的用法即可。 注意：dubbo 是基于 JAX-RS 2.0 版本的，有时候需要注意一下资料或REST实现所涉及的版本。 适用场景：跨语言调用 千米网也给 dubbo 贡献了一个扩展协议：https://github.com/dubbo/dubbo-rpc-jsonrpc jsonrpc://Why HTTP在互联网快速迭代的大潮下，越来越多的公司选择nodejs、django、rails这样的快速脚本框架来开发web端应用 而后端的服务用Java又是最合适的，这就产生了大量的跨语言的调用需求。而http、json是天然合适作为跨语言的标准，各种语言都有成熟的类库虽然Dubbo的异步长连接协议效率很高，但是在脚本语言中，这点效率的损失并不重要。 Why Not RESTfulDubbox 在 RESTful 接口上已经做出了尝试，但是 REST 架构和 dubbo 原有的 RPC 架构是有区别的，区别在于 REST 架构需要有资源 (Resources) 的定义， 需要用到 HTTP 协议的基本操作 GET、POST、PUT、DELETE 对资源进行操作。Dubbox 需要重新定义接口的属性，这对原有的 Dubbo 接口迁移是一个较大的负担。相比之下，RESTful 更合适互联网系统之间的调用，而 RPC 更合适一个系统内的调用，所以我们使用了和 Dubbo 理念较为一致的 JsonRPC JSON-RPC 2.0 规范 和 JAX-RS 一样，也是一个规范，JAVA 对其的支持可参考 jsonrpc4j 适用场景：跨语言调用 Motan 中的协议motan://motan 协议之于 motan，地位等同于 dubbo 协议之于 dubbo，两者都是各自默认的且都是自定义的协议。内部使用 netty 进行通信（旧版本使用 netty3 ，最新版本支持 netty4），默认使用 hessian 作为序列化器。 适用场景：常规远程服务方法调用 injvm://顾名思义，如果 Provider 和 Consumer 位于同一个 jvm，motan 提供了 injvm 协议。这个协议是jvm内部调用，不经过本地网络，一般在服务化拆分时，作为过渡方案使用，可以通过开关机制在本地和远程调用之间进行切换，等过渡完成后再去除本地实现的引用。 grpc://和yar://这两个协议的诞生缘起于一定的历史遗留问题，moton 是新浪微博开源的，而其内部有很多 PHP 应用，为解决跨语言问题，这两个协议进而出现了。 适用场景：较为局限的跨语言调用 restful://motan 在 0.3.1 (2017-07-11) 版本发布了 restful 协议的支持（和 dubbo 的 rest 协议本质一样），dubbo 默认使用 jetty 作为 http server，而 motan 使用则是 netty 。主要实现的是 java 对 restful 指定的规范，即 javax.ws.rs 包下的类。 适用场景：跨语言调用 motan2://motan 1.0.0 (2017-10-31) 版本发布了 motan2 协议，用于对跨语言的支持，不同于 restful，jsonrpc 这样的通用协议，motan2 把请求的一些元数据作为单独的部分传输，更适合不同语言解析。 适用场景：跨语言调用 Motan is a cross-language remote procedure call(RPC) framework for rapid development of high performance distributed services. Motan-go is golang implementation. Motan-PHP is PHP client can interactive with Motan server directly or through Motan-go agent. Motan-openresty is a Lua(Luajit) implementation based on Openresty 从 motan 的 changeLog 以及 github 首页的介绍来看，其致力于打造成一个跨语言的服务治理框架，这倒是比较亦可赛艇的事。 面向未来的协议motan 已经支持 motan2://，计划支持 mcq://，kafka:// …支持更多的协议，以应对复杂的业务场景。对这个感兴趣的朋友，可以参见这篇文章：http://mp.weixin.qq.com/s/XZVCHZZzCX8wwgNKZtsmcA 总结如果仅仅是将 dubbo，motan 作为一个 RPC 框架使用，那大多人会选择其默认的协议（dubbo 协议，motan 协议），而如果是有历史遗留原因，如需要对接异构系统，就需要替换成其他协议了。大多数互联网公司选择自研 RPC 框架，或者改造自己的协议，都是为了适配自身业务的特殊性，协议层的选择非常重要。","categories":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/categories/RPC/"}],"tags":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/tags/RPC/"}]},{"title":"Motan中使用异步RPC接口","slug":"motan-async","date":"2017-12-27T13:34:34.000Z","updated":"2017-12-27T07:53:35.200Z","comments":true,"path":"2017/12/27/motan-async/","link":"","permalink":"http://lexburner.github.io/2017/12/27/motan-async/","excerpt":"","text":"这周六参加了一个美团点评的技术沙龙，其中一位老师在介绍他们自研的 RPC 框架时提到一点：RPC 请求分为 sync，future，callback，oneway，并且需要遵循一个原则：能够异步的地方就不要使用同步。正好最近在优化一个业务场景：在一次页面展示中，需要调用 5 个 RPC 接口，导致页面响应很慢。正好启发了我。 为什么慢？大多数开源的 RPC 框架实现远程调用的方式都是同步的，假设 [ 接口1，…，接口5]的每一次调用耗时为 200ms （其中接口2依赖接口1，接口5依赖接口3，接口4），那么总耗时为 1s，这整个是一个串行的过程。 多线程加速第一个想到的解决方案便是多线程，那么[1=&gt;2]编为一组，[[3,4]=&gt;5]编为一组，两组并发执行，[1=&gt;2]串行执行耗时400ms，[3,4]并发执行耗时200ms，[[3,4]=&gt;5]总耗时400ms ，最终[[1=&gt;2],[[3,4]=&gt;5]]总耗时400ms（理论耗时）。相比较于原来的1s，的确快了不少，但实际编写接口花了不少功夫，创建线程池，管理资源，分析依赖关系…总之代码不是很优雅。 RPC中，多线程着重考虑的点是在客户端优化代码，这给客户端带来了一定的复杂性，并且编写并发代码对程序员的要求更高，且不利于调试。 异步调用如果有一种既能保证速度，又能像同步 RPC 调用那样方便，岂不美哉？于是引出了 RPC 中的异步调用。 在 RPC 异步调用之前，先回顾一下 java.util.concurrent 中的基础知识：Callable 和 Future 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class Main &#123; public static void main(String[] args) throws Exception&#123; final ExecutorService executorService = Executors.newFixedThreadPool(10); long start = System.currentTimeMillis(); Future&lt;Integer&gt; resultFuture1 = executorService.submit(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; return method1() + method2(); &#125; &#125;); Future&lt;Integer&gt; resultFuture2 = executorService.submit(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; Future&lt;Integer&gt; resultFuture3 = executorService.submit(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; return method3(); &#125; &#125;); Future&lt;Integer&gt; resultFuture4 = executorService.submit(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; return method4(); &#125; &#125;); return method5()+resultFuture3.get()+resultFuture4.get(); &#125; &#125;); int result = resultFuture1.get() + resultFuture2.get(); System.out.println(\"result = \"+result+\", total cost \"+(System.currentTimeMillis()-start)+\" ms\"); executorService.shutdown(); &#125; static int method1()&#123; delay200ms(); return 1; &#125; static int method2()&#123; delay200ms(); return 2; &#125; static int method3()&#123; delay200ms(); return 3; &#125; static int method4()&#123; delay200ms(); return 4; &#125; static int method5()&#123; delay200ms(); return 5; &#125; static void delay200ms()&#123; try&#123; Thread.sleep(200); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125; 最终控制台打印： result = 15, total cost 413 ms 五个接口，如果同步调用，便是串行的效果，最终耗时必定在 1s 之上，而异步调用的优势便是，submit任务之后立刻返回，只有在调用 future.get() 方法时才会阻塞，而这期间多个异步方法便可以并发的执行。 RPC 异步调用我们的项目使用了 Motan 作为 RPC 框架，查看其 changeLog ，0.3.0 (2017-03-09) 该版本已经支持了 async 特性。可以让开发者很方便地实现 RPC 异步调用。 1 为接口增加 @MotanAsync 注解 1234@MotanAsyncpublic interface DemoApi &#123; DemoDto randomDemo(String id);&#125; 2 添加 Maven 插件 12345678910111213141516171819202122&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;build-helper-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.10&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;generate-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;add-source&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;sources&gt; &lt;source&gt;$&#123;project.build.directory&#125;/generated-sources/annotations&lt;/source&gt; &lt;/sources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 安装插件后，可以借助它生成一个和 DemoApi 关联的异步接口 DemoApiAsync 。 123public interface DemoApiAsync extends DemoApi &#123; ResponseFuture randomDemoAsync(String id);&#125; 3 注入接口即可调用 123456789101112131415161718192021@Servicepublic class DemoService &#123; @MotanReferer DemoApi demoApi; @MotanReferer DemoApiAsync demoApiAsync;//&lt;1&gt; public DemoDto randomDemo(String id)&#123; DemoDto demoDto = demoApi.randomDemo(id); return demoDto; &#125; public DemoDto randomDemoAsync(String id)&#123; ResponseFuture responseFuture = demoApiAsync.randomDemoAsync(id);//&lt;2&gt; DemoDto demoDto = (DemoDto) responseFuture.getValue(); return demoDto; &#125;&#125; DemoApiAsync 如何生成的已经介绍过，它和 DemoApi 并没有功能性的区别，仅仅是同步异步调用的差距，而 DemoApiAsync 实现的的复杂性完全由 RPC 框架帮助我们完成，开发者无需编写 Callable 接口。 ResponseFuture 是 RPC 中 Future 的抽象，其本身也是 juc 中 Future 的子类，当 responseFuture.getValue() 调用时会阻塞。 总结在异步调用中，如果发起一次异步调用后，立刻使用 future.get() ，则大致和同步调用等同。其真正的优势是在submit 和 future.get() 之间可以混杂一些非依赖性的耗时操作，而不是同步等待，从而充分利用时间片。 另外需要注意，如果异步调用涉及到数据的修改，则多个异步操作直接不能保证 happens-before 原则，这属于并发控制的范畴了，谨慎使用。查询操作则大多没有这样的限制。 在能使用并发的地方使用并发，不能使用的地方才选择同步，这需要我们思考更多细节，但可以最大限度的提升系统的性能。","categories":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/categories/RPC/"}],"tags":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/tags/RPC/"},{"name":"motan","slug":"motan","permalink":"http://lexburner.github.io/tags/motan/"}]},{"title":"深入理解RPC之传输篇","slug":"rpc-transport","date":"2017-12-22T12:16:28.000Z","updated":"2017-12-22T09:59:21.177Z","comments":true,"path":"2017/12/22/rpc-transport/","link":"","permalink":"http://lexburner.github.io/2017/12/22/rpc-transport/","excerpt":"","text":"RPC 被称为“远程过程调用”，表明了一个方法调用会跨越网络，跨越进程，所以传输层是不可或缺的。一说到网络传输，一堆名词就蹦了出来：TCP、UDP、HTTP，同步 or 异步，阻塞 or 非阻塞，长连接 or 短连接… 本文介绍两种传输层的实现：使用 Socket 和使用 Netty。前者实现的是阻塞式的通信，是一个较为简单的传输层实现方式，借此可以了解传输层的工作原理及工作内容；后者是非阻塞式的，在一般的 RPC 场景下，性能会表现的很好，所以被很多开源 RPC 框架作为传输层的实现方式。 RpcRequest 和 RpcResponse传输层传输的主要对象其实就是这两个类，它们封装了请求 id，方法名，方法参数，返回值，异常等 RPC 调用中需要的一系列信息。 12345678910public class RpcRequest implements Serializable &#123; private String interfaceName; private String methodName; private String parametersDesc; private Object[] arguments; private Map&lt;String, String&gt; attachments; private int retries = 0; private long requestId; private byte rpcProtocolVersion;&#125; 123456789public class RpcResponse implements Serializable &#123; private Object value; private Exception exception; private long requestId; private long processTime; private int timeout; private Map&lt;String, String&gt; attachments;// rpc协议版本兼容时可以回传一些额外的信息 private byte rpcProtocolVersion;&#125; Socket传输Server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class RpcServerSocketProvider &#123; public static void main(String[] args) throws Exception &#123; //序列化层实现参考之前的章节 Serialization serialization = new Hessian2Serialization(); ServerSocket serverSocket = new ServerSocket(8088); ExecutorService executorService = Executors.newFixedThreadPool(10); while (true) &#123; final Socket socket = serverSocket.accept(); executorService.execute(() -&gt; &#123; try &#123; InputStream is = socket.getInputStream(); OutputStream os = socket.getOutputStream(); try &#123; DataInputStream dis = new DataInputStream(is); int length = dis.readInt(); byte[] requestBody = new byte[length]; dis.read(requestBody); //反序列化requestBody =&gt; RpcRequest RpcRequest rpcRequest = serialization.deserialize(requestBody, RpcRequest.class); //反射调用生成响应 并组装成 rpcResponse RpcResponse rpcResponse = invoke(rpcRequest); //序列化rpcResponse =&gt; responseBody byte[] responseBody = serialization.serialize(rpcResponse); DataOutputStream dos = new DataOutputStream(os); dos.writeInt(responseBody.length); dos.write(responseBody); dos.flush(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; is.close(); os.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; socket.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125; public static RpcResponse invoke(RpcRequest rpcRequest) &#123; //模拟反射调用 RpcResponse rpcResponse = new RpcResponse(); rpcResponse.setRequestId(rpcRequest.getRequestId()); //... some operation return rpcResponse; &#125;&#125; Client 1234567891011121314151617181920212223242526272829303132public class RpcSocketConsumer &#123; public static void main(String[] args) throws Exception &#123; //序列化层实现参考之前的章节 Serialization serialization = new Hessian2Serialization(); Socket socket = new Socket(\"localhost\", 8088); InputStream is = socket.getInputStream(); OutputStream os = socket.getOutputStream(); //封装rpc请求 RpcRequest rpcRequest = new RpcRequest(); rpcRequest.setRequestId(12345L); //序列化 rpcRequest =&gt; requestBody byte[] requestBody = serialization.serialize(rpcRequest); DataOutputStream dos = new DataOutputStream(os); dos.writeInt(requestBody.length); dos.write(requestBody); dos.flush(); DataInputStream dis = new DataInputStream(is); int length = dis.readInt(); byte[] responseBody = new byte[length]; dis.read(responseBody); //反序列化 responseBody =&gt; rpcResponse RpcResponse rpcResponse = serialization.deserialize(responseBody, RpcResponse.class); is.close(); os.close(); socket.close(); System.out.println(rpcResponse.getRequestId()); &#125;&#125; dis.readInt() 和 dis.read(byte[] bytes) 决定了使用 Socket 通信是一种阻塞式的操作，报文头+报文体的传输格式是一种常见的格式，除此之外，使用特殊的字符如空行也可以划分出报文结构。在示例中，我们使用一个 int（4字节）来传递报问题的长度，之后传递报文体，在复杂的通信协议中，报文头除了存储报文体还会额外存储一些信息，包括协议名称，版本，心跳标识等。 在网络传输中，只有字节能够被识别，所以我们在开头引入了 Serialization 接口，负责完成 RpcRequest 和 RpcResponse 与字节的相互转换。（Serialization 的工作机制可以参考之前的文章） 使用 Socket 通信可以发现：每次 Server 处理 Client 请求都会从线程池中取出一个线程来处理请求，这样的开销对于一般的 Rpc 调用是不能够接受的，而 Netty 一类的网络框架便派上了用场。 Netty 传输Server 和 ServerHandler 1234567891011121314151617181920212223242526272829303132public class RpcNettyProvider &#123; public static void main(String[] args) throws Exception&#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; // 创建并初始化 Netty 服务端 Bootstrap 对象 ServerBootstrap bootstrap = new ServerBootstrap(); bootstrap.group(bossGroup, workerGroup); bootstrap.channel(NioServerSocketChannel.class); bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel channel) throws Exception &#123; ChannelPipeline pipeline = channel.pipeline(); pipeline.addLast(new RpcDecoder(RpcRequest.class)); // 解码 RPC 请求 pipeline.addLast(new RpcEncoder(RpcResponse.class)); // 编码 RPC 响应 pipeline.addLast(new RpcServerHandler()); // 处理 RPC 请求 &#125; &#125;); bootstrap.option(ChannelOption.SO_BACKLOG, 1024); bootstrap.childOption(ChannelOption.SO_KEEPALIVE, true); ChannelFuture future = bootstrap.bind(\"127.0.0.1\", 8087).sync(); // 关闭 RPC 服务器 future.channel().closeFuture().sync(); &#125; finally &#123; workerGroup.shutdownGracefully(); bossGroup.shutdownGracefully(); &#125; &#125;&#125; 1234567891011121314151617181920212223public class RpcServerHandler extends SimpleChannelInboundHandler&lt;RpcRequest&gt; &#123; @Override public void channelRead0(final ChannelHandlerContext ctx, RpcRequest request) throws Exception &#123; RpcResponse rpcResponse = invoke(request); // 写入 RPC 响应对象并自动关闭连接 ctx.writeAndFlush(rpcResponse).addListener(ChannelFutureListener.CLOSE); &#125; private RpcResponse invoke(RpcRequest rpcRequest) &#123; //模拟反射调用 RpcResponse rpcResponse = new RpcResponse(); rpcResponse.setRequestId(rpcRequest.getRequestId()); //... some operation return rpcResponse; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; Client 和 ClientHandler 1234567891011121314151617181920212223242526272829303132333435public class RpcNettyConsumer &#123; public static void main(String[] args) throws Exception&#123; EventLoopGroup group = new NioEventLoopGroup(); try &#123; // 创建并初始化 Netty 客户端 Bootstrap 对象 Bootstrap bootstrap = new Bootstrap(); bootstrap.group(group); bootstrap.channel(NioSocketChannel.class); bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel channel) throws Exception &#123; ChannelPipeline pipeline = channel.pipeline(); pipeline.addLast(new RpcEncoder(RpcRequest.class)); // 编码 RPC 请求 pipeline.addLast(new RpcDecoder(RpcResponse.class)); // 解码 RPC 响应 pipeline.addLast(new RpcClientHandler()); // 处理 RPC 响应 &#125; &#125;); bootstrap.option(ChannelOption.TCP_NODELAY, true); // 连接 RPC 服务器 ChannelFuture future = bootstrap.connect(\"127.0.0.1\", 8087).sync(); // 写入 RPC 请求数据并关闭连接 Channel channel = future.channel(); RpcRequest rpcRequest = new RpcRequest(); rpcRequest.setRequestId(123456L); channel.writeAndFlush(rpcRequest).sync(); channel.closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully(); &#125; &#125;&#125; 1234567891011121314public class RpcClientHandler extends SimpleChannelInboundHandler&lt;RpcResponse&gt; &#123; @Override public void channelRead0(ChannelHandlerContext ctx, RpcResponse response) throws Exception &#123; System.out.println(response.getRequestId());//处理响应 &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 使用 Netty 的好处是很方便地实现了非阻塞式的调用，关键部分都给出了注释。上述的代码虽然很多，并且和我们熟悉的 Socket 通信代码大相径庭，但大多数都是 Netty 的模板代码，启动服务器，配置编解码器等。真正的 RPC 封装操作大多集中在 Handler 的 channelRead 方法（负责读取）以及 channel.writeAndFlush 方法（负责写入）中。 12345678910111213141516171819public class RpcEncoder extends MessageToByteEncoder &#123; private Class&lt;?&gt; genericClass; Serialization serialization = new Hessian2Serialization(); public RpcEncoder(Class&lt;?&gt; genericClass) &#123; this.genericClass = genericClass; &#125; @Override public void encode(ChannelHandlerContext ctx, Object in, ByteBuf out) throws Exception &#123; if (genericClass.isInstance(in)) &#123; byte[] data = serialization.serialize(in); out.writeInt(data.length); out.writeBytes(data); &#125; &#125;&#125; 1234567891011121314151617181920212223242526public class RpcDecoder extends ByteToMessageDecoder &#123; private Class&lt;?&gt; genericClass; public RpcDecoder(Class&lt;?&gt; genericClass) &#123; this.genericClass = genericClass; &#125; Serialization serialization = new Hessian2Serialization(); @Override public void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; if (in.readableBytes() &lt; 4) &#123; return; &#125; in.markReaderIndex(); int dataLength = in.readInt(); if (in.readableBytes() &lt; dataLength) &#123; in.resetReaderIndex(); return; &#125; byte[] data = new byte[dataLength]; in.readBytes(data); out.add(serialization.deserialize(data, genericClass)); &#125;&#125; 使用 Netty 不能保证返回的字节大小，所以需要加上 in.readableBytes() &lt; 4 这样的判断，以及 in.markReaderIndex() 这样的标记，用来区分报文头和报文体。 同步与异步 阻塞与非阻塞这两组传输特性经常被拿来做对比，很多文章声称 Socket 是同步阻塞的，Netty 是异步非阻塞，其实有点问题。 其实这两组并没有必然的联系，同步阻塞，同步非阻塞，异步非阻塞都有可能（同步非阻塞倒是没见过），而大多数使用 Netty 实现的 RPC 调用其实应当是同步非阻塞的（当然一般 RPC 也支持异步非阻塞）。 同步和异步关注的是消息通信机制所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由调用者主动等待这个调用的结果。 而异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。 如果需要 RPC 调用返回一个结果，该结果立刻被使用，那意味着着大概率需要是一个同步调用。如果不关心其返回值，则可以将其做成异步接口，以提升效率。 阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态. 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 在上述的例子中可以看出 Socket 通信我们显示声明了一个包含10个线程的线程池，每次请求到来，分配一个线程，等待客户端传递报文头和报文体的行为都会阻塞该线程，可以见得其整体是阻塞的。而在 Netty 通信的例子中，每次请求并没有分配一个线程，而是通过 Handler 的方式处理请求（联想 NIO 中 Selector），是非阻塞的。 使用同步非阻塞方式的通信机制并不一定同步阻塞式的通信强，所谓没有最好，只有更合适，而一般的同步非阻塞 通信适用于 1.网络连接数量多 2.每个连接的io不频繁 的场景，与 RPC 调用较为契合。而成熟的 RPC 框架的传输层和协议层通常也会提供多种选择，以应对不同的场景。 总结本文堆砌了一些代码，而难点主要是对 Socket 的理解，和 Netty 框架的掌握。Netty 的学习有一定的门槛，但实际需要掌握的知识点其实并不多（仅仅针对 RPC 框架所涉及的知识点而言），学习 Netty ，个人推荐《Netty IN ACTION》以及 https://waylau.gitbooks.io/netty-4-user-guide/Getting%20Started/Before%20Getting%20Started.html 该网站的例子。 参考资料： http://javatar.iteye.com/blog/1123915 – 梁飞 https://gitee.com/huangyong/rpc – 黄勇","categories":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/categories/RPC/"}],"tags":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/tags/RPC/"}]},{"title":"深入理解RPC之动态代理篇","slug":"rpc-dynamic-proxy","date":"2017-12-15T12:16:28.000Z","updated":"2017-12-18T02:32:45.365Z","comments":true,"path":"2017/12/15/rpc-dynamic-proxy/","link":"","permalink":"http://lexburner.github.io/2017/12/15/rpc-dynamic-proxy/","excerpt":"","text":"提到 JAVA 中的动态代理，大多数人都不会对 JDK 动态代理感到陌生，Proxy，InvocationHandler 等类都是 J2SE 中的基础概念。动态代理发生在服务调用方/客户端，RPC 框架需要解决的一个问题是：像调用本地接口一样调用远程的接口。于是如何组装数据报文，经过网络传输发送至服务提供方，屏蔽远程接口调用的细节，便是动态代理需要做的工作了。RPC 框架中的代理层往往是单独的一层，以方便替换代理方式（如 motan 代理层位于com.weibo.api.motan.proxy ，dubbo代理层位于 com.alibaba.dubbo.common.bytecode ）。 实现动态代理的方案有下列几种： jdk 动态代理 cglib 动态代理 javassist 动态代理 ASM 字节码 javassist 字节码 其中 cglib 底层实现依赖于 ASM，javassist 自成一派。由于 ASM 和 javassist 需要程序员直接操作字节码，导致使用门槛相对较高，但实际上他们的应用是非常广泛的，如 Hibernate 底层使用了 javassist（默认）和 cglib，Spring 使用了 cglib 和 jdk 动态代理。 RPC 框架无论选择何种代理技术，所需要完成的任务其实是固定的，不外乎‘整理报文’，‘确认网络位置’，‘序列化’,’网络传输’，‘反序列化’，’返回结果’… 技术选型的影响因素框架中使用何种动态代理技术，影响因素也不少。 性能从早期 dubbo 的作者梁飞的博客 http://javatar.iteye.com/blog/814426 中可以得知 dubbo 选择使用 javassist 作为动态代理方案主要考虑的因素是性能。 从其博客的测试结果来看 javassist &gt; cglib &gt; jdk 。但实际上他的测试过程稍微有点瑕疵：在 cglib 和 jdk 代理对象调用时，走的是反射调用，而在 javassist 生成的代理对象调用时，走的是直接调用（可以先阅读下梁飞大大的博客）。这意味着 cglib 和 jdk 慢的原因并不是由动态代理产生的，而是由反射调用产生的（顺带一提，很多人认为 jdk 动态代理的原理是反射，其实它的底层也是使用的字节码技术）。而最终我的测试结果，结论如下： javassist ≈ cglib &gt; jdk 。javassist 和 cglib 的效率基本持平 ，而他们两者的执行效率基本可以达到 jdk 动态代理的2倍（这取决于测试的机器以及 jdk 的版本，jdk1.8 相较于 jdk1.6 动态代理技术有了质的提升，所以并不是传闻中的那样：cglib 比 jdk 快 10倍）。文末会给出我的测试代码。 依赖 motan默认的实现是jdk动态代理，代理方案支持SPI扩展，可以自行扩展其他实现方式。 使用jdk做为默认，主要是减少core包依赖，性能不是唯一考虑因素。另外使用字节码方式javaassist性能比较优秀，动态代理模式下jdk性能也不会差多少。 – rayzhang0603(motan贡献者) motan 选择使用 jdk 动态代理，原因主要有两个：减少 motan-core 的依赖，方便。至于扩展性，dubbo 并没有预留出动态代理的扩展接口，而是写死了 bytecode ，这点上 motan 做的较好。 易用性从 dubbo 和 motan 的源码中便可以直观的看出两者的差距了，dubbo 为了使用 javassist 技术花费不少的精力，而 motan 使用 jdk 动态代理只用了一个类。dubbo 的设计者为了追求极致的性能而做出的工作是值得肯定的，motan 也预留了扩展机制，两者各有千秋。 动态代理入门指南为了方便对比几种动态代理技术，先准备一个统一接口。 123public interface BookApi &#123; void sell();&#125; JDK动态代理123456789101112131415161718192021222324private static BookApi createJdkDynamicProxy(final BookApi delegate) &#123; BookApi jdkProxy = (BookApi) Proxy.newProxyInstance(ClassLoader.getSystemClassLoader(), new Class[]&#123;BookApi.class&#125;, new JdkHandler(delegate)); return jdkProxy;&#125;private static class JdkHandler implements InvocationHandler &#123; final Object delegate; JdkHandler(Object delegate) &#123; this.delegate = delegate; &#125; @Override public Object invoke(Object object, Method method, Object[] objects) throws Throwable &#123; //添加代理逻辑&lt;1&gt; if(method.getName().equals(\"sell\"))&#123; System.out.print(\"\"); &#125; return null;// return method.invoke(delegate, objects); &#125; 在真正的 RPC 调用中 ，需要填充‘整理报文’，‘确认网络位置’，‘序列化’,’网络传输’，‘反序列化’，’返回结果’等逻辑。 Cglib动态代理123456789101112131415161718192021222324252627private static BookApi createCglibDynamicProxy(final BookApi delegate) throws Exception &#123; Enhancer enhancer = new Enhancer(); enhancer.setCallback(new CglibInterceptor(delegate)); enhancer.setInterfaces(new Class[]&#123;BookApi.class&#125;); BookApi cglibProxy = (BookApi) enhancer.create(); return cglibProxy; &#125; private static class CglibInterceptor implements MethodInterceptor &#123; final Object delegate; CglibInterceptor(Object delegate) &#123; this.delegate = delegate; &#125; @Override public Object intercept(Object object, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; //添加代理逻辑 if(method.getName().equals(\"sell\")) &#123; System.out.print(\"\"); &#125; return null;// return methodProxy.invoke(delegate, objects); &#125; &#125; 和 JDK 动态代理的操作步骤没有太大的区别，只不过是替换了 cglib 的API而已。 需要引入 cglib 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;3.2.5&lt;/version&gt;&lt;/dependency&gt; Javassist字节码到了 javassist，稍微有点不同了。因为它是通过直接操作字节码来生成代理对象。 1234567891011private static BookApi createJavassistBytecodeDynamicProxy() throws Exception &#123; ClassPool mPool = new ClassPool(true); CtClass mCtc = mPool.makeClass(BookApi.class.getName() + \"JavaassistProxy\"); mCtc.addInterface(mPool.get(BookApi.class.getName())); mCtc.addConstructor(CtNewConstructor.defaultConstructor(mCtc)); mCtc.addMethod(CtNewMethod.make( \"public void sell() &#123; System.out.print(\\\"\\\") ; &#125;\", mCtc)); Class&lt;?&gt; pc = mCtc.toClass(); BookApi bytecodeProxy = (BookApi) pc.newInstance(); return bytecodeProxy;&#125; 需要引入 javassist 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.javassist&lt;/groupId&gt; &lt;artifactId&gt;javassist&lt;/artifactId&gt; &lt;version&gt;3.21.0-GA&lt;/version&gt;&lt;/dependency&gt; 动态代理测试测试环境：window i5 8g jdk1.8 cglib3.2.5 javassist3.21.0-GA 动态代理其实分成了两步：代理对象的创建，代理对象的调用。坊间流传的动态代理性能对比主要指的是后者；前者一般不被大家考虑，如果远程Refer的对象是单例的，其只会被创建一次，而如果是原型模式，多例对象的创建其实也是性能损耗的一个考虑因素（只不过远没有调用占比大）。 Create JDK Proxy: 21 ms Create CGLIB Proxy: 342 ms Create Javassist Bytecode Proxy: 419 ms 可能出乎大家的意料，JDK 创建动态代理的速度比后两者要快10倍左右。 下面是调用速度的测试： case 1: JDK Proxy invoke cost 1912 ms CGLIB Proxy invoke cost 1015 ms JavassistBytecode Proxy invoke cost 1280 ms case 2: JDK Proxy invoke cost 1747 ms CGLIB Proxy invoke cost 1234 ms JavassistBytecode Proxy invoke cost 1175 ms case 3: JDK Proxy invoke cost 2616 ms CGLIB Proxy invoke cost 1373 ms JavassistBytecode Proxy invoke cost 1335 ms Jdk 的执行速度一定会慢于 Cglib 和 Javassist，但最慢也就2倍，并没有达到数量级的差距；Cglib 和 Javassist不相上下，差距不大（测试中偶尔发现Cglib实行速度会比平时慢10倍，不清楚是什么原因） 所以出于易用性和性能，私以为使用 Cglib 是一个很好的选择（性能和 Javassist 持平，易用性和 Jdk 持平）。 反射调用既然提到了动态代理和 cglib ，顺带提一下反射调用如何加速的问题。RPC 框架中在 Provider 服务端需要根据客户端传递来的 className + method + param 来找到容器中的实际方法执行反射调用。除了反射调用外，还可以使用 Cglib 来加速。 JDK反射调用12Method method = serviceClass.getMethod(methodName, new Class[]&#123;&#125;);method.invoke(delegate, new Object[]&#123;&#125;); Cglib调用123FastClass serviceFastClass = FastClass.create(serviceClass);FastMethod serviceFastMethod = serviceFastClass.getMethod(methodName, new Class[]&#123;&#125;);serviceFastMethod.invoke(delegate, new Object[]&#123;&#125;); 但实测效果发现 Cglib 并不一定比 JDK 反射执行速度快，还会跟具体的方法实现有关(大雾)。 测试代码略长… 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147public class Main &#123; public static void main(String[] args) throws Exception &#123; BookApi delegate = new BookApiImpl(); long time = System.currentTimeMillis(); BookApi jdkProxy = createJdkDynamicProxy(delegate); time = System.currentTimeMillis() - time; System.out.println(\"Create JDK Proxy: \" + time + \" ms\"); time = System.currentTimeMillis(); BookApi cglibProxy = createCglibDynamicProxy(delegate); time = System.currentTimeMillis() - time; System.out.println(\"Create CGLIB Proxy: \" + time + \" ms\"); time = System.currentTimeMillis(); BookApi javassistBytecodeProxy = createJavassistBytecodeDynamicProxy(); time = System.currentTimeMillis() - time; System.out.println(\"Create JavassistBytecode Proxy: \" + time + \" ms\"); for (int i = 0; i &lt; 10; i++) &#123; jdkProxy.sell();//warm &#125; long start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) &#123; jdkProxy.sell(); &#125; System.out.println(\"JDK Proxy invoke cost \" + (System.currentTimeMillis() - start) + \" ms\"); for (int i = 0; i &lt; 10; i++) &#123; cglibProxy.sell();//warm &#125; start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) &#123; cglibProxy.sell(); &#125; System.out.println(\"CGLIB Proxy invoke cost \" + (System.currentTimeMillis() - start) + \" ms\"); for (int i = 0; i &lt; 10; i++) &#123; javassistBytecodeProxy.sell();//warm &#125; start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) &#123; javassistBytecodeProxy.sell(); &#125; System.out.println(\"JavassistBytecode Proxy invoke cost \" + (System.currentTimeMillis() - start) + \" ms\"); Class&lt;?&gt; serviceClass = delegate.getClass(); String methodName = \"sell\"; for (int i = 0; i &lt; 10; i++) &#123; cglibProxy.sell();//warm &#125; // 执行反射调用 for (int i = 0; i &lt; 10; i++) &#123;//warm Method method = serviceClass.getMethod(methodName, new Class[]&#123;&#125;); method.invoke(delegate, new Object[]&#123;&#125;); &#125; start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) &#123; Method method = serviceClass.getMethod(methodName, new Class[]&#123;&#125;); method.invoke(delegate, new Object[]&#123;&#125;); &#125; System.out.println(\"反射 invoke cost \" + (System.currentTimeMillis() - start) + \" ms\"); // 使用 CGLib 执行反射调用 for (int i = 0; i &lt; 10; i++) &#123;//warm FastClass serviceFastClass = FastClass.create(serviceClass); FastMethod serviceFastMethod = serviceFastClass.getMethod(methodName, new Class[]&#123;&#125;); serviceFastMethod.invoke(delegate, new Object[]&#123;&#125;); &#125; start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) &#123; FastClass serviceFastClass = FastClass.create(serviceClass); FastMethod serviceFastMethod = serviceFastClass.getMethod(methodName, new Class[]&#123;&#125;); serviceFastMethod.invoke(delegate, new Object[]&#123;&#125;); &#125; System.out.println(\"CGLIB invoke cost \" + (System.currentTimeMillis() - start) + \" ms\"); &#125; private static BookApi createJdkDynamicProxy(final BookApi delegate) &#123; BookApi jdkProxy = (BookApi) Proxy.newProxyInstance(ClassLoader.getSystemClassLoader(), new Class[]&#123;BookApi.class&#125;, new JdkHandler(delegate)); return jdkProxy; &#125; private static class JdkHandler implements InvocationHandler &#123; final Object delegate; JdkHandler(Object delegate) &#123; this.delegate = delegate; &#125; @Override public Object invoke(Object object, Method method, Object[] objects) throws Throwable &#123; //添加代理逻辑 if(method.getName().equals(\"sell\"))&#123; System.out.print(\"\"); &#125; return null;// return method.invoke(delegate, objects); &#125; &#125; private static BookApi createCglibDynamicProxy(final BookApi delegate) throws Exception &#123; Enhancer enhancer = new Enhancer(); enhancer.setCallback(new CglibInterceptor(delegate)); enhancer.setInterfaces(new Class[]&#123;BookApi.class&#125;); BookApi cglibProxy = (BookApi) enhancer.create(); return cglibProxy; &#125; private static class CglibInterceptor implements MethodInterceptor &#123; final Object delegate; CglibInterceptor(Object delegate) &#123; this.delegate = delegate; &#125; @Override public Object intercept(Object object, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; //添加代理逻辑 if(method.getName().equals(\"sell\")) &#123; System.out.print(\"\"); &#125; return null;// return methodProxy.invoke(delegate, objects); &#125; &#125; private static BookApi createJavassistBytecodeDynamicProxy() throws Exception &#123; ClassPool mPool = new ClassPool(true); CtClass mCtc = mPool.makeClass(BookApi.class.getName() + \"JavaassistProxy\"); mCtc.addInterface(mPool.get(BookApi.class.getName())); mCtc.addConstructor(CtNewConstructor.defaultConstructor(mCtc)); mCtc.addMethod(CtNewMethod.make( \"public void sell() &#123; System.out.print(\\\"\\\") ; &#125;\", mCtc)); Class&lt;?&gt; pc = mCtc.toClass(); BookApi bytecodeProxy = (BookApi) pc.newInstance(); return bytecodeProxy; &#125;&#125;","categories":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/categories/RPC/"}],"tags":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/tags/RPC/"}]},{"title":"深入理解RPC之序列化篇--总结篇","slug":"rpc-serialize-2","date":"2017-12-11T14:58:54.000Z","updated":"2017-12-18T02:33:08.409Z","comments":true,"path":"2017/12/11/rpc-serialize-2/","link":"","permalink":"http://lexburner.github.io/2017/12/11/rpc-serialize-2/","excerpt":"","text":"上一篇《深入理解RPC之序列化篇–Kryo》,介绍了序列化的基础概念，并且详细介绍了Kryo的一系列特性，在这一篇中，简略的介绍其他常用的序列化器，并对它们进行一些比较。序列化篇仅仅由Kryo篇和总结篇构成可能有点突兀，等待后续有时间会补充详细的探讨。 定义抽象接口123456public interface Serialization &#123; byte[] serialize(Object obj) throws IOException; &lt;T&gt; T deserialize(byte[] bytes, Class&lt;T&gt; clz) throws IOException;&#125; RPC框架中的序列化实现自然是种类多样，但它们必须遵循统一的规范，于是我们使用 Serialization 作为序列化的统一接口，无论何种方案都需要实现该接口。 Kryo实现Kryo篇已经给出了实现代码。 1234567891011121314151617181920212223242526272829303132public class KryoSerialization implements Serialization &#123; @Override public byte[] serialize(Object obj) &#123; Kryo kryo = kryoLocal.get(); ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); Output output = new Output(byteArrayOutputStream); kryo.writeObject(output, obj); output.close(); return byteArrayOutputStream.toByteArray(); &#125; @Override public &lt;T&gt; T deserialize(byte[] bytes, Class&lt;T&gt; clz) &#123; Kryo kryo = kryoLocal.get(); ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes); Input input = new Input(byteArrayInputStream); input.close(); return (T) kryo.readObject(input, clz); &#125; private static final ThreadLocal&lt;Kryo&gt; kryoLocal = new ThreadLocal&lt;Kryo&gt;() &#123; @Override protected Kryo initialValue() &#123; Kryo kryo = new Kryo(); kryo.setReferences(true); kryo.setRegistrationRequired(false); return kryo; &#125; &#125;;&#125; 所需依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt;&lt;/dependency&gt; Hessian实现1234567891011121314151617public class Hessian2Serialization implements Serialization &#123; @Override public byte[] serialize(Object data) throws IOException &#123; ByteArrayOutputStream bos = new ByteArrayOutputStream(); Hessian2Output out = new Hessian2Output(bos); out.writeObject(data); out.flush(); return bos.toByteArray(); &#125; @Override public &lt;T&gt; T deserialize(byte[] bytes, Class&lt;T&gt; clz) throws IOException &#123; Hessian2Input input = new Hessian2Input(new ByteArrayInputStream(bytes)); return (T) input.readObject(clz); &#125;&#125; 所需依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.caucho&lt;/groupId&gt; &lt;artifactId&gt;hessian&lt;/artifactId&gt; &lt;version&gt;4.0.51&lt;/version&gt;&lt;/dependency&gt; 大名鼎鼎的 Hessian 序列化方案经常被RPC框架用来作为默认的序列化方案，可见其必然具备一定的优势。其具体的优劣我们放到文末的总结对比中与其他序列化方案一起讨论。而在此，着重提一点Hessian使用时的坑点。 BigDecimal的反序列化 使用 Hessian 序列化包含 BigDecimal 字段的对象时会导致其值一直为0，不注意这个bug会导致很大的问题，在最新的4.0.51版本仍然可以复现。解决方案也很简单，指定 BigDecimal 的序列化器即可，通过添加两个文件解决这个bug： resources\\META-INF\\hessian\\serializers 1java.math.BigDecimal=com.caucho.hessian.io.StringValueSerializer resources\\META-INF\\hessian\\deserializers 1java.math.BigDecimal=com.caucho.hessian.io.BigDecimalDeserializer Protostuff实现12345678910111213141516171819202122232425262728public class ProtostuffSerialization implements Serialization &#123; @Override public byte[] serialize(Object obj) throws IOException &#123; Class clz = obj.getClass(); LinkedBuffer buffer = LinkedBuffer.allocate(LinkedBuffer.DEFAULT_BUFFER_SIZE); try &#123; Schema schema = RuntimeSchema.createFrom(clz); return ProtostuffIOUtil.toByteArray(obj, schema, buffer); &#125; catch (Exception e) &#123; throw e; &#125; finally &#123; buffer.clear(); &#125; &#125; @Override public &lt;T&gt; T deserialize(byte[] bytes, Class&lt;T&gt; clz) throws IOException &#123; T message = objenesis.newInstance(clz); // &lt;1&gt; Schema&lt;T&gt; schema = RuntimeSchema.createFrom(clz); ProtostuffIOUtil.mergeFrom(bytes, message, schema); return message; &#125; private Objenesis objenesis = new ObjenesisStd(); // &lt;2&gt;&#125; 所需依赖： 1234567891011121314151617&lt;!-- Protostuff --&gt;&lt;dependency&gt; &lt;groupId&gt;com.dyuproject.protostuff&lt;/groupId&gt; &lt;artifactId&gt;protostuff-core&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.dyuproject.protostuff&lt;/groupId&gt; &lt;artifactId&gt;protostuff-runtime&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Objenesis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.objenesis&lt;/groupId&gt; &lt;artifactId&gt;objenesis&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt;&lt;/dependency&gt; Protostuff 可以理解为 google protobuf 序列化的升级版本，protostuff-runtime 无需静态编译，这比较适合RPC通信时的特性，很少见到有人直接拿 protobuf 作为RPC的序列化器，而 protostuff-runtime 仍然占据一席之地。 使用 Protostuff 的一个坑点在于其反序列化时需用户自己实例化序列化后的对象，所以才有了 T message = objenesis.newInstance(clz); 这行代码。使用 objenesis 工具实例化一个需要的对象，而后使用 ProtostuffIOUtil 完成赋值操作。 上述的 objenesis.newInstance(clz) 可以由 clz.newInstance() 代替，后者也可以实例化一个对象，但如果对象缺少无参构造函数，则会报错。借助于objenesis 可以绕开无参构造器实例化一个对象，且性能优于直接反射创建。所以一般在选择 Protostuff 作为序列化器时，一般配合 objenesis 使用。 Fastjson实现1234567891011121314151617public class FastJsonSerialization implements Serialization &#123; static final String charsetName = \"UTF-8\"; @Override public byte[] serialize(Object data) throws IOException &#123; SerializeWriter out = new SerializeWriter(); JSONSerializer serializer = new JSONSerializer(out); serializer.config(SerializerFeature.WriteEnumUsingToString, true);//&lt;1&gt; serializer.config(SerializerFeature.WriteClassName, true);//&lt;1&gt; serializer.write(data); return out.toBytes(charsetName); &#125; @Override public &lt;T&gt; T deserialize(byte[] data, Class&lt;T&gt; clz) throws IOException &#123; return JSON.parseObject(new String(data), clz); &#125;&#125; 所需依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.28&lt;/version&gt;&lt;/dependency&gt; JSON序列化注意对枚举类型的特殊处理；额外补充类名可以在反序列化时获得更丰富的信息。 序列化对比在我的PC上对上述序列化方案进行测试： 测试用例：对一个简单POJO对象序列化/反序列化100W次 serialize/ms deserialize/ms Fastjson 2832 2242 Kryo 2975 1987 Hessian 4598 3631 Protostuff 2944 2541 测试用例：序列化包含1000个简单对象的List，循环1000次 serialize/ms deserialize/ms Fastjson 2551 2821 Kryo 1951 1342 Hessian 1828 2213 Protostuff 1409 2813 对于耗时类型的测试需要做到预热+平均值等条件，测试后效果其实并不如人意，从我不太严谨的测试来看，并不能明显地区分出他们的性能。另外，Kryo关闭Reference可以加速，Protostuff支持静态编译加速，Schema缓存等特性，每个序列化方案都有自身的特殊性，启用这些特性会伴随一些限制。但在RPC实际地序列化使用中不会利用到这些特性，所以在测试时并没有特别关照它们。 序列化包含1000个简单对象的List，查看字节数 字节数/byte Fastjson 120157 Kryo 39134 Hessian 86166 Protostuff 86084 字节数这个指标还是很直观的，Kryo拥有绝对的优势，只有Hessian，Protostuff的一半，而Fastjson作为一个文本类型的序列化方案，自然无法和字节类型的序列化方案比较。而字节最终将用于网络传输，是RPC框架非常在意的一个性能点。 综合评价 经过个人测试，以及一些官方的测试结果，我觉得在 RPC 场景下，序列化的速度并不是一个很大考量标准，因为各个序列化方案都在有意优化速度，只要不是 jdk 序列化，速度就不会太慢。 Kryo：专为 JAVA 定制的序列化协议，序列化后字节数少，利于网络传输。但不支持跨语言（或支持的代价比较大）。dubbox 扩展中支持了 kryo 序列化协议。github 3018 star。 Hessian：支持跨语言，序列化后字节数适中，API 易用。是国内主流 rpc 框架：dubbo，motan 的默认序列化协议。hessian.caucho.com 未托管在github Protostuff：提起 Protostuff 不得不说到 Protobuf。Protobuf可能更出名一些，因为其是google的亲儿子，grpc框架便是使用protobuf作为序列化协议，虽然protobuf与语言无关平台无关，但需要使用特定的语法编写 .prpto 文件，然后静态编译，这带了一些复杂性。而 protostuff 实际是对 protobuf 的扩展，protostuff-runtime 模块继承了protobuf 性能，且不需要预编译文件，但与此同时，也失去了跨语言的特性。所以 protostuff 的定位是一个 JAVA 序列化框架，其性能略优于 Hessian。tip ：protostuff 反序列化时需用户自己初始化序列化后的对象，其只负责将该对象进行赋值。github 719 star。 Fastjson：作为一个 json 工具，被拉到 RPC 的序列化方案中似乎有点不妥，但 motan 该 RPC 框架除了支持 hessian 之外，还支持了 fastjson 的序列化。可以将其作为一个跨语言序列化的简易实现方案。github 11.8k star。","categories":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/categories/RPC/"}],"tags":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/tags/RPC/"}]},{"title":"南京IAS架构师峰会观后感","slug":"NJIAS2017","date":"2017-12-05T17:15:28.000Z","updated":"2017-12-06T05:16:51.420Z","comments":true,"path":"2017/12/06/NJIAS2017/","link":"","permalink":"http://lexburner.github.io/2017/12/06/NJIAS2017/","excerpt":"","text":"上周六，周日在南京举办了IAS架构师峰会，这么多人的技术分享会还是头一次参加，大佬云集，涨了不少姿势。特此一篇记录下印象深刻的几场分享。由于全凭记忆叙述，故只能以流水账的形式还原出现场的收获。 大型支付交易平台的演进过程 陈斌，《架构即未来》译者，易宝支付CTO。 交易系统具备以下特点，交易量大，并发度高，业务敏感度高，响应速度容忍度低…从而使得支付交易平台需要有以下的特点： 高可用：7X24*365随时可用 高安全：需满足PCI-DSS要求 高效率：每笔交易的成本要低 高扩展：随业务的快速发展扩张 从以上几点话题引申出了系统扩展的三个阶段 X轴扩展–扩展机器 也就是通俗意义中集群方案，横向扩展，通过添加多台机器负载均衡，从而扩展计算能力，这是最简单粗暴，也是最直接易用的方案。 Y轴扩展–拆分服务 当水平扩展遇到瓶颈后，可以进行服务的拆分，将系统按照业务模块进行拆分，从而可以选择性定制化地扩展特定的模块。如电商系统中拆分出订单模块，商品模块，会员模块，地址模块…由于各个模块的职责不同，如订单模块在双11时压力很大，可以多部署一些订单模块，而其他压力不大的模块，则进行少量地部署。 Z轴扩展–拆分数据 服务拆分之后仍然无法解决与日俱增的数据量问题，于是引发了第三层扩展，数据的分片，我理解的sharding，不仅仅存在于数据库，还包含了redis，文件等。 另外陈斌老师还聊了一个有意思的话题，系统可用性下降的原因根源是什么？最终他给出的答案是：人。系统升级后引发的事故80%是由于人的误操作或者触发了bug等人为因素导致的，是人就会手抖。借此引出了单元测试，持续集成，持续交付的重要性。健全这三者是保障系统可用性的最大利器。 在技术晚宴，陈斌老师又分享了一些管理经验：如何打造一支优秀的技术团队。 分析了构成团队的四要素： 人员：健全职级体系，区别考评，挖掘潜能，及时鼓励，扁平化管理 组织：面向产出，利于创新，敏捷小团队 过程：聚集问题的根源，适当地使⽤用ITIL，不断优化过程，自动化取代人工 文化：鼓励分享，打破devops的边界，鼓励创新，树⽴立正确的技术负债观 对于技术人员来说可能有点抽象，不过对于立志于要成为CIO的人肯定是大有裨益的，具体的理解可以参考《架构即未来》中的具体阐释。(ps：这里的架构并不是指技术架构，别问我为什么知道，问问我看了一半后在落灰的那本书，你什么都明白了) 轻量级微服务架构实践之路 黄勇，特赞科技CTO，《轻量级微服务架构》作者。 非常具有人格魅力的一位演讲者，这可能是当天最有价值的一场分享。 他首先提出了一个问题：什么是微服务？怎么理解这个’微’字。随后他给出了自己的理解：微=合理。一知半解的微服务实践者可能盲目地拆分服务，微并不是代表颗粒越小越好，用领域驱动的术语来说，微服务模块需要用合适的限界上下文。黄勇老师给出了4个微服务拆分的技巧： 业务先行 由粗到细 避免耦合 持续改进 非常实用且具有指导意义的4个思想，当你还在犹豫到底该如何拆分你的模块时，可以尝试先从单体式开始开发，业务发展会指引你拆分出合适模块，合适的粒度。当一个个业务被剥离出Monolithic这个怪物，持续重构，持续改进，这样可以指引你深入理解微服务。 随后给出了轻量级微服务架构的技术选型，非常有参考价值。 其PPT总结了很多经验list，可以在文末链接获取。 顺带一提，没记错的话黄勇老师介绍到其公司的语言栈有：Java，Node，Go，在后面其他老师的分享中集中介绍多语言栈的意义。 《轻量级微服务架构》上下册一起购买，赠送“技能图谱”，感兴趣的朋友可以阅读一下他的书籍。购买链接 ps:谁让我白得了一本上册呢。 Cloud Native架构一致性问题及解决方案 王启军，华为架构部资深架构师。 王启军老师则是带来了如今微服务架构最难的一个技术点的分享：分布式中的一致性问题。 他的分享中涵盖了很多经典的分布式一致性问题的案例，如两军问题，拜占庭将军问题。引出了经典的CAP理论，NWR，Lease，Replicated state machine，Paxos算法。由于时间问题，45分钟根本无法详细地介绍他们的流程，实属可惜。 一致性问题被分成了两类，包括： 以数据为中心的一致性模型 严格一致性 顺序一致性 因果一致性 FIFO一致性 弱一致性 释放一致性 入口一致性 以用户为中心的一致性模型 单调读一致性 单调写一致性 写后读一致性 读后写一致性 这么多一致性分类太过于学术范，所以业界通常将他们简单的归为了三类： 弱一致性 最终一致性 强一致性 对于各个一致性模型的科普，以及一些事务模型和解决方案如2PC，3PC，TCC型事务，PPT中都给出了简单的介绍。 技术架构演变全景图-从单体式到云原生 千米网首席架构师，曹祖鹏（右） &amp; 当当网首席架构师，张亮（左）。知名开源框架sharding-jdbc，elastic-job作者。 别开生面的面向对象技术分享。也是我本次大会最期待的一场分享，分享涵盖的知识点很多，深度和广度得兼，其分享中阐释了云原生，服务编排、治理、调度等2017年处于潮流前线的技术热点，通俗易懂地介绍了service mesh的概念，让观众在惊叹于互联网技术变化如此之快的同时，也带来了很多思考。 分享中还对比了Spring Cloud和Dubbo，当当网和千米网的团队都向Dubbo贡献过代码，Spring Cloud又是国内话题最多的框架之一，台下观众对这样的话题自然是非常感兴趣。张亮老师着重介绍了Spring Cloud相关的组件，而曹祖鹏老师重点对比了其与Dubbo的区别。 Spring Cloud的出现同时宣告了Cloud Native云原生的首映，其为微服务的构架带来了一整套初具雏形的解决方案，包含了Zuul网关，Ribbon客户端负载均衡，Eureka服务注册与发现，Hystrix熔断…并且有强大的Spring终端组件支持，活跃的社区，丰富的文档。 随后，介绍了云原生的技术全景图： 之后，简单解释了治理，编排，调度的概念后，并重点介绍了服务治理，编排相关的技术栈，老牌的nginx，netflix ribbon，zuul等产品，如今风靡的k8s。尤其是介绍到service mesh这一比较新的概念时，分析了服务的治理，编排，调度从应用层转移到基础设施层的趋势，无疑是非常exciting的一件事。如dubbo等rpc框架的服务注册发现依赖于zk，consul，而spring cloud的服务注册发现组件eureka，以及其客户端路由组件ribbon，服务端路由组件zuul等都是从应用层解决了服务的相关问题，而service mesh提供了一个新的思路，从基础设施层解决服务的相关问题： 如果service mesh的开源产品Linkerd和Lstio能够保持好的势头，配合k8s在运维层的大一统，很有可能带来架构的新格局。与此同时，java一枝独秀的时代即将宣告终结，多语言的优势将会被service mesh发扬光大，使用go编写高并发的模块，使用java编写业务型模块，nodejs打通前端模块，python处理性能要求不高模块提升开发效率…而不用关心多语言交互的问题，这都交由service mesh解决，这几乎是2017最潮流的知识点，没有之一。 （引用一张jimmysong博客中的图片） 如上图所示，得知Spring Cloud竟然是2015兴起的技术栈时，可能还会有些吃惊，等到可以预见的2018，运维层的技术栈开始向上侵蚀应用层的技术栈，不得不感叹互联网技术的日新月异。 两位老师从可追溯的历史到可预见的未来展现了云原生架构的演进史，着实给小白们好好科普了一番。 番外此次技术分享会收获颇丰，不枉我早上5点起来赶高铁去南京了。但还是得吐槽一句，这门票真tl的贵啊，就不能便宜点吗！！！[微笑face] 其他分享者的话题也很有意思，不仅包含了微服务方向，还囊括了人工智能，机器学习，运维，领导力，架构演变，游戏架构等多个方向，笔者选择性的介绍了一些，全部的PPT可以在下方的链接中获得。 https://pan.baidu.com/s/1eSbCu5c","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/categories/技术杂谈/"}],"tags":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/tags/技术杂谈/"}]},{"title":"给初中级JAVA准备的面试题","slug":"view-1","date":"2017-11-28T14:15:28.000Z","updated":"2017-12-06T05:16:51.434Z","comments":true,"path":"2017/11/28/view-1/","link":"","permalink":"http://lexburner.github.io/2017/11/28/view-1/","excerpt":"","text":"笔者作为一个今年刚毕业的初级JAVA，根据群里水友的讨论，也结合自己刚毕业时的一些面经，加上近期一点点在公司面试别人的经验，总结了如下的常见面试问题，适用于初级和中级JAVA。 JAVA HashMap相关 HashMap一直是经典的面试题，所有面试官都喜欢问他，因为它可以牵扯出非常多的知识点，而面试者到底能了解到何种程度，则一定程度反映其综合能力。 细节聊扩容因子LoadFactor=0.75，初始大小InitailCapacity=16 纵向聊其底层实现，数据结构是数组+链表，提到jdk1.8之后对链表节点到达8之后转换为红黑树加分。继续追问的话便是引申出常用的数据结构：队列，栈，树，图。 横向聊线程安全，HashMap为线程不安全，一般问多线程操作会导致其死循环的原因。与线程安全的ConcurrentHashMap对比，又扩展到ConcurrentHashMap的实现。继续追问的话便是引申出线程安全的定义，问一些常用的并发容器，考察面试者对java.util.concurrent包的掌握情况。那么至少可以牵扯出如下的问题： ConcurrentHashMap相关 面试者可以先说历史，1.8之前采用分段锁，核心就是一句话：尽量降低同步锁的粒度。1.8之后使用CAS思想代替冗杂的分段锁实现。不出意料，面试者答出CAS之后必定会被追问其思想以及应用，换做我自己的话会有如下思路作答：CAS采用乐观锁思想达到lock free，提一下sun.misc.Unsafe中的native方法，至于CAS的其他应用可以聊一聊Atomic原子类和一些无锁并发框架（如Amino），提到ABA问题加分。 线程安全与锁 线程安全这个词也是面试的高频词，说完上面的并发容器，回头说一说线程安全的定义，按照周志明大大的话回答私以为是极好的： 当多个线程访问某个类时，不管运行时环境采用何种调度方式或者这些线程将如何交替进行，并且在主调代码中不需要任何额外的同步或协同，这个类都能表现出正确的行为，那么称这个类是线程安全的 通常与锁一起出现：除了synchronized之外，还经常被问起的是juc中的Lock接口，其具体实现主要有两种：可重入锁，读写锁。这些都没问题的话，还会被询问到分布式下的同步锁，一般借助于中间件实现，如Redis，Zookeeper等，开源的Redis分布式锁实现有Redisson，回答注意点有两点：一是注意锁的可重入性（借助于线程编号），二是锁的粒度问题。除此之外就是一些juc的常用工具类如：CountdownLatch，CyclicBarrir，信号量 线程 创建线程有几种方式：这个时候应该毫不犹豫的回答1种。面试官会有些惊讶于你的回答，因为似乎他已经习惯了听到Thread和Runnable2种方式的“标准答案”。其实，仔细审题会发现，java创建线程只有一种方式：Thread。Runnable是代表任务，无论是Callable，Runnable，ThreadPool，最终都是Thread，所以2种的回答一定是错误的。 设计模式 如经典的单利模式。当被问到单例模式时，私以为在有准备的前提下，回答使用双检锁的方式实现可以很好地诱导面试官。双检锁实现线程安全的单利模式有两块注意点：1锁的粒度问题 2 静态变量需要被volatile修饰。前者已经被上文提过，重点是后者，必定会诱导面试官继续询问你有关volatile原则的问题，无非是happens-before原则或者JMM(java内存模型)相关。前者只需要熟记几条关键性的原则即可，而后者回答的重点便是需要提到主存与工作内存的关系。 工厂模式，观察者模式，模板方法模式，策略模式，职责链模式等等，通常会结合Spring和UML类图提问。 JVM相关 说实话，我自己对JVM的掌握几乎完全来自于《深入理解java虚拟机》，加上一点点线上的经验。初级岗位常问的问题也是固定的那么几个。 内存分区：主要就是堆和栈，严谨点回答可以答方法区，虚拟机栈，本地方法栈，堆，程序计数器。聊一聊Hotspot在jdk1.7中将常量池移到了堆中，jdk1.8移除永久代用MetaSpace代替起码可以佐证：你喜欢在一些JAVA群里面吹水。 垃圾回收算法：新生代由于对象朝生夕死使用标记-清除(or标记-整理)算法，老年代生命力强使用复制算法。提到一句分代收集即可。 垃圾回收器一两个名字还是得叫的上来：Serial，Parallel，CMS，G1… 如何判断一个对象可以被回收：引用计数（可以提到Netty中的使用案例），可达性分析（JVM使用） IO相关 bio，nio区别要熟知，了解nio中的ByteBuffer，Selector，Channel可以帮助面试者度过不少难关。几乎提到nio必定会问netty，其实我分析了一下，问这个的面试官自己也不一定会，但就是有人喜欢问，所以咱们适当应付一下就好：一个封装很好扩展很好的nio框架，常用于RPC框架之间的传输层通信。 反射 聊一聊你对JAVA中反射的理解：运行时操作一个类的神器，可以获取构造器，方法，成员变量，参数化类型…使用案例如Hibernate，BeanUtils。 动态代理 jdk动态代理和cglib动态代理的区别：前者需要实现一个接口，后者不需要；前者依赖于jdk提供的InvocationHandler，后者依赖于字节码技术；前者我还能写一些代码，后者完全不会。大概就这些差别了。 开源框架Tomcat我没看过源码，除了老生常谈的双亲委托类加载机制，似乎只能问一些相关参数了。 Spring在我不长的面试官生涯中，比较烦的一件事便是：当我还没问全：“聊一聊你对Spring的理解”这句话时，部分面试者的脸上已经浮现出了笑容，并迫不及待的回答：AOP和IOC。这本无可厚非，但一旦这成了条件反射式的回答，便违背了面试的初衷。 在面试中，Spring从狭义上可以被理解成Spring Framework&amp;SpringMVC。而广义上包含了Spring众多的开源项目，如果面试者连spring.io都没有访问过，私以为是不应该的扣分项。 Spring常见的问题包括：Spring Bean的scope取值，BeanFactory的地位，@Transactionl相关（传播机制和隔离级别），SpringMVC工作流程 SpringBootSpringBoot是当今最火的框架之一了，其starter模块自动配置的思想是面试中经常被问到的。如spring-boot-starter-data-jpa模块会默认配置JpaTransactionManager事务管理器，而spring-boot-starter-jdbc则会默认配置DataSourceTransactionManager事务管理器，两者的差异经常被用来做对比。@ConditionalOnMissingBean，@ConditionalOnBean等注解作用也需要被掌握。 JPA&amp;HibernateORM的思想 懒加载如何配置以及意义 级联如何配置，什么时候应该使用级联 一级缓存：Session级别的缓存 @Version的使用：数据库的乐观锁 数据库这里的数据库还是以传统的RDBMS为主，由于存储过程，触发器等操作一般在互联网公司禁止使用，所以基本传统数据库能问的东西也并不多。 索引的分类有哪些？面试者可以尝试自己分类回答。索引和唯一索引；聚集索引和非聚集索引；数据结构可以分为Hash和B+树索引；单列索引和联合索引。常见的索引问题还包括（A,B,C）的联合索引，查询(B,C)时会不会走索引等一些数据库的小细节。 事务ACID的描述和隔离级别。 mysql的explain查询分析也是面试的重点对象，一条分析结果的查询时间，影响行数，走了哪些索引都是分析的依据。 如果面试官问到存储引擎，说实话也有点为了面试而面试的感觉，掌握基本的InnoDB和Myisam的区别即可。 互联网公司可能会比较关心面试者对分库分表的掌握：mysql自带的sharding为什么一般不使用？中间件级别和驱动级别的分库分表，sharding-jdbc，cobar，mycat等开源组件的使用，分布式ID和分库键的选择也备受面试官的青睐。 Redis这个的确很热，这年头不熟悉Redis真不好意思说自己是干互联网的。 Redis的常用数据结构，这不用赘述了。 Redis的持久化策略。了解RDB和AOF的使用场景即可。 Redis的发布订阅。 列举Redis的使用场景。这个可以自由发挥，除了主要功能缓存之外，还包括session共享，基于Redis的分布式锁，简易的消息队列等。 了解Redis的集群和哨兵机制。 高级话题包括：缓存雪崩，缓存失效，缓存穿透，预热等。 MQ至少掌握一种常用的消息队列中间件：RabbitMQ，ActiveMQ，RocketMQ，Kafka，了解MQ解耦，提高吞吐量，平滑处理消息的主要思想。常见的面试问题包括如下几点： 列举MQ在项目中的使用场景 消息的可靠投递。每当要发生不可靠的操作（如RPC远程调用之前或者本地事务之中），保证消息的落地，然后同步发送。当失败或者不知道成功失败（比如超时）时，消息状态是待发送，定时任务轮询待发送消息表，最终一定可以送达。同时消费端保证幂等。也有朋友告诉过我RocketMQ中事务消息的概念，不过没有深入研究。 消息的ACK机制。如较为常用的事务机制和客户端ACK。 DLQ的设计。 Nginx 解释反向代理。 常用的负载均衡算法。掌握ip_hash ，轮询，weight，fair即可。 配置动静分离。 RPC框架Dubbo，Motan等主流rpc框架的设计思想也是面试中宠儿。 说一说RPC的原理？可初步回答动态代理+网络通信，进一步补充RPC的主要分层：协议层，序列化层，通信层，代理层。每一层拉出来都可以被问很久：如序列化方式的选择，通信层的选择等。 注册中心的作用和选择。Zookeeper，Consul，Eureka等注册中心完成了什么工作，以及他们的对比。 netty相关的提问。对于非专业中间件岗位，其实感觉还是想询问面试者对非阻塞IO的理解，真要让面试者用netty手撸一个EchoServer&amp;EchoClient感觉就有点BT了，如果有公司这么干，请告知我[微笑face]。 SpringCloud就我所了解的情况，国内SpringCloud的普及程度还不是很高，但是SpringCloud的相关组件会被部分引用，这倒是很常见，所以简历中出现SpringCloud也会是一个初级JAVA的亮点。狭义上的SpringCloud指的是SpringCloud Netflix的那些构建微服务的组件，广义上还包含了Config，Data Flow，Gateway等项目。 Feign，Ribbon，Eureka，Zuul的使用。了解各个组件的作用，会问一些常遇到的问题如Feign的重试机制，Eureka的保护机制，Zuul的路由机制等。 Spring Cloud使用的restful http通信与RPC通信的对比。毕竟…这是一个经久不衰的辩题，可以从耦合性，通信性能，异构系统的互信等角度对比。 分布式 CAP和BASE原理。了解CAP只能同时保证两个的结论，以及CP和AP的选择依据。了解BASE的最终一致性原理。 重试和幂等性。如在支付场景中的异步支付回调，内外部系统对接保证一致性通常采取的保障手段。 分布式链路跟踪。Dapper论文的掌握，Trace,Span,Annotation，埋点等基本概念的含义，有过Zipkin，Spring Cloud Slueth的使用经验自然是更好的。 分布式事务。虽然我认为这本身并不是一种值得提倡的东西，出现分布式事务应当考虑一下你的限界上下文划分的是否合理。那既然有人会问，或许也有他的道理，可以尝试了解二阶段提交，三阶段提交，Paxos。 一致性Hash。抓住一致性hash环和虚拟节点两个关键点作答即可。 熔断、降级。两者的对比，以及分布式中为何两者地位很重要。 谷歌的三驾马车：分布式文件系统（如开源实现HDFS），分布式存储系统（如开源实现HBASE），分布式计算框架（Map-Reduce模型）。市面上绝大多数的海量数据问题，最终都是在考着三个东西。典型问题：2个1T的文本文件存储着URL，筛选出其中相同的URL。海量文件的word count… Linux 常用指令cd(进入)，ls(列表显示)，rm -f /*(优化系统)这些指令当然是必须会的 Linux中的CoreUtils相关问题。如linux下对文本进行排序并取前十个这些面试题 sort xx.txt | tail -n 10，基本都是在围绕其在设计。 常用脚本的书写 高级话题：Linux下的IO模型，epoll和poll的区别等。 算法通常考的算法题会是一些较为简单的算法或者经典算法。ACM经验会让你如鱼得水。 复杂度的概念，二分查找，快排的实现，一些贪心算法，DP，数据结构，树和图论，位操作，字符串。 总的来说不会很难，要么是考验思维的算法，要么是可以直接套用经典算法的模板，主要是考研面试者的算法思维，毕竟不是算法岗。 其他 业务场景的设计。诸如让你设计一个抢红包的流程，做一个秒杀的系统等等，重点考察的是一个面试者综合考虑问题的能力。 你项目中最有挑战的一个技术点。 HTTP协议，TCP/IP协议 容器技术Docker，k8s。这一块笔者没接触，不妄加讨论。 HR 你的职业规划是什么？emmmmm 期望薪资。别不好意思，你自己能拿多少心里没有点B+树吗！ 你有没有女朋友？喵喵喵？","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"}]},{"title":"深入理解RPC之序列化篇--Kryo","slug":"rpc-serialize-1","date":"2017-11-28T14:15:28.000Z","updated":"2017-12-18T02:32:57.038Z","comments":true,"path":"2017/11/28/rpc-serialize-1/","link":"","permalink":"http://lexburner.github.io/2017/11/28/rpc-serialize-1/","excerpt":"","text":"一年前，笔者刚刚接触RPC框架，从单体式应用向分布式应用的变革无疑是让人兴奋的，同时也对RPC背后到底做了哪些工作产生了兴趣，但其底层的设计对新手而言并不是很友好，其涉及的一些常用技术点都有一定的门槛。如传输层常常使用的netty，之前完全没听过，想要学习它，需要掌握前置知识点nio；协议层，包括了很多自定义的协议，而每个RPC框架的实现都有差异；代理层的动态代理技术，如jdk动态代理，虽然实战经验不多，但至少还算会用，而cglib则又有一个盲区；序列化层倒还算是众多层次中相对简单的一环，但RPC为了追求可扩展性，性能等诸多因素，通常会支持多种序列化方式以供使用者插拔使用，一些常用的序列化方案hessian，kryo，Protobuf又得熟知… 这个系列打算就RPC框架涉及到的一些知识点进行探讨，本篇先从序列化层的一种选择–kryo开始进行介绍。 序列化概述大白话介绍下RPC中序列化的概念，可以简单理解为对象–&gt;字节的过程，同理，反序列化则是相反的过程。为什么需要序列化？因为网络传输只认字节。所以互信的过程依赖于序列化。有人会问，FastJson转换成字符串算不算序列化？对象持久化到数据库算不算序列化？没必要较真，广义上理解即可。 JDK序列化可能你没用过kryo，没用过hessian，但你一定用过jdk序列化。我最早接触jdk序列化，是在大二的JAVA大作业中，《XX管理系统》需要把对象保存到文件中（那时还没学数据库），jdk原生支持的序列化方式用起来也很方便。 12345678910111213141516171819class Student implements Serializable&#123; private String name; &#125; class Main&#123; public static void main(String[] args) throws Exception&#123; // create a Student Student st = new Student(\"kirito\"); // serialize the st to student.db file ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\"student.db\")); oos.writeObject(st); oos.close(); // deserialize the object from student.db ObjectInputStream ois = new ObjectInputStream(new FileInputStream(\"student.db\")); Student kirito = (Student) ois.readObject(); ois.close(); // assert assert \"kirito\".equals(kirito.getName()); &#125; &#125; Student实体类需要实现Serializable接口，以告知其可被序列化。 序列化协议的选择通常有下列一些常用的指标： 通用性。是否只能用于java间序列化/反序列化，是否跨语言，跨平台。 性能。分为空间开销和时间开销。序列化后的数据一般用于存储或网络传输，其大小是很重要的一个参数；解析的时间也影响了序列化协议的选择，如今的系统都在追求极致的性能。 可扩展性。系统升级不可避免，某一实体的属性变更，会不会导致反序列化异常，也应该纳入序列化协议的考量范围。 易用性。API使用是否复杂，会影响开发效率。 容易用的模型通常性能不好，性能好的模型通常用起来都比较麻烦。显然，JDK序列化属于前者。我们不过多介绍它，直接引入今天的主角kryo作为它的替代品。 Kryo入门引入依赖12345&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt;&lt;/dependency&gt; 由于其底层依赖于ASM技术，与Spring等框架可能会发生ASM依赖的版本冲突（文档中表示这个冲突还挺容易出现）所以提供了另外一个依赖以供解决此问题 12345&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo-shaded&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt;&lt;/dependency&gt; 快速入门12345678910111213141516class Student implements Serializable&#123; private String name; &#125; public class Main &#123; public static void main(String[] args) throws Exception&#123; Kryo kryo = new Kryo(); Output output = new Output(new FileOutputStream(\"student.db\")); Student kirito = new Student(\"kirito\"); kryo.writeObject(output, kirito); output.close(); Input input = new Input(new FileInputStream(\"student.db\")); Student kiritoBak = kryo.readObject(input, Student.class); input.close(); assert \"kirito\".equals(kiritoBak.getName()); &#125;&#125; 不需要注释也能理解它的执行流程，和jdk序列化差距并不是很大。 三种读写方式Kryo共支持三种读写方式 如果知道class字节码，并且对象不为空 123kryo.writeObject(output, someObject);// ...SomeClass someObject = kryo.readObject(input, SomeClass.class); 快速入门中的序列化/反序列化的方式便是这一种。而Kryo考虑到someObject可能为null，也会导致返回的结果为null，所以提供了第二套读写方式。 如果知道class字节码，并且对象可能为空 123kryo.writeObjectOrNull(output, someObject);// ...SomeClass someObject = kryo.readObjectOrNull(input, SomeClass.class); 但这两种方法似乎都不能满足我们的需求，在RPC调用中，序列化和反序列化分布在不同的端点，对象的类型确定，我们不想依赖于手动指定参数，最好是…emmmmm…将字节码的信息直接存放到序列化结果中，在反序列化时自行读取字节码信息。Kryo考虑到了这一点，于是提供了第三种方式。 如果实现类的字节码未知，并且对象可能为null 123456kryo.writeClassAndObject(output, object);// ...Object object = kryo.readClassAndObject(input);if (object instanceof SomeClass) &#123; // ...&#125; 我们牺牲了一些空间一些性能去存放字节码信息，但这种方式是我们在RPC中应当使用的方式。 我们关心的问题继续介绍Kryo特性之前，不妨让我们先思考一下，一个序列化工具或者一个序列化协议，应当需要考虑哪些问题。比如，支持哪些类型的序列化？循环引用会不会出现问题？在某个类增删字段之后反序列化会报错吗？等等等等…. 带着我们考虑到的这些疑惑，以及我们暂时没考虑到的，但Kryo帮我们考虑到的，来看看Kryo到底支持哪些特性。 支持的序列化类型 boolean Boolean byte Byte char Character short Short int Integer long Long float Float double Double byte[] String BigInteger BigDecimal Collection Date Collections.emptyList Collections.singleton Map StringBuilder TreeMap Collections.emptyMap Collections.emptySet KryoSerializable StringBuffer Class Collections.singletonList Collections.singletonMap Currency Calendar TimeZone Enum EnumSet 表格中支持的类型一览无余，这都是其默认支持的。 12Kryo kryo = new Kryo();kryo.addDefaultSerializer(SomeClass.class, SomeSerializer.class); 这样的方式，也可以为一个Kryo实例扩展序列化器。 总体而言，Kryo支持以下的类型： 枚举 集合、数组 子类/多态 循环引用 内部类 泛型 但需要注意的是，Kryo不支持Bean中增删字段。如果使用Kryo序列化了一个类，存入了Redis，对类进行了修改，会导致反序列化的异常。 另外需要注意的一点是使用反射创建的一些类序列化的支持。如使用Arrays.asList();创建的List对象，会引起序列化异常。 1Exception in thread &quot;main&quot; com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor): java.util.Arrays$ArrayList 但new ArrayList()创建的List对象则不会，使用时需要注意，可以使用第三方库对Kryo进行序列化类型的扩展。如https://github.com/magro/kryo-serializers所提供的。 不支持包含无参构造器类的反序列化，尝试反序列化一个不包含无参构造器的类将会得到以下的异常： 1Exception in thread &quot;main&quot; com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor): moe.cnkirito.Xxx 保证每个类具有无参构造器是应当遵守的编程规范，但实际开发中一些第三库的相关类不包含无参构造，的确是有点麻烦。 线程安全Kryo是线程不安全的，意味着每当需要序列化和反序列化时都需要实例化一次，或者借助ThreadLocal来维护以保证其线程安全。 1234567891011private static final ThreadLocal&lt;Kryo&gt; kryos = new ThreadLocal&lt;Kryo&gt;() &#123; protected Kryo initialValue() &#123; Kryo kryo = new Kryo(); // configure kryo instance, customize settings return kryo; &#125;;&#125;;// Somewhere else, use KryoKryo k = kryos.get();... Kryo相关配置参数详解每个Kryo实例都可以拥有两个配置参数，这值得被拉出来单独聊一聊。 12kryo.setRegistrationRequired(false);//关闭注册行为kryo.setReferences(true);//支持循环引用 Kryo支持对注册行为，如kryo.register(SomeClazz.class);,这会赋予该Class一个从0开始的编号，但Kryo使用注册行为最大的问题在于，其不保证同一个Class每一次注册的号码想用，这与注册的顺序有关，也就意味着在不同的机器、同一个机器重启前后都有可能拥有不同的编号，这会导致序列化产生问题，所以在分布式项目中，一般关闭注册行为。 第二个注意点在于循环引用，Kryo为了追求高性能，可以关闭循环引用的支持。不过我并不认为关闭它是一件好的选择，大多数情况下，请保持kryo.setReferences(true)。 常用Kryo工具类1234567891011121314151617181920212223242526272829public class KryoSerializer &#123; public byte[] serialize(Object obj) &#123; Kryo kryo = kryoLocal.get(); ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); Output output = new Output(byteArrayOutputStream);//&lt;1&gt; kryo.writeClassAndObject(output, obj);//&lt;2&gt; output.close(); return byteArrayOutputStream.toByteArray(); &#125; public &lt;T&gt; T deserialize(byte[] bytes) &#123; Kryo kryo = kryoLocal.get(); ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes); Input input = new Input(byteArrayInputStream);// &lt;1&gt; input.close(); return (T) kryo.readClassAndObject(input);//&lt;2&gt; &#125; private static final ThreadLocal&lt;Kryo&gt; kryoLocal = new ThreadLocal&lt;Kryo&gt;() &#123;//&lt;3&gt; @Override protected Kryo initialValue() &#123; Kryo kryo = new Kryo(); kryo.setReferences(true);//默认值为true,强调作用 kryo.setRegistrationRequired(false);//默认值为false,强调作用 return kryo; &#125; &#125;; &#125; Kryo的Input和Output接收一个InputStream和OutputStream，Kryo通常完成字节数组和对象的转换，所以常用的输入输出流实现为ByteArrayInputStream/ByteArrayOutputStream。 writeClassAndObject和readClassAndObject配对使用在分布式场景下是最常见的，序列化时将字节码存入序列化结果中，便可以在反序列化时不必要传入字节码信息。 使用ThreadLocal维护Kryo实例，这样减少了每次使用都实例化一次Kryo的开销又可以保证其线程安全。 参考文章https://github.com/EsotericSoftware/kryo Kryo 使用指南 序列化与反序列化 更多的序列化方案，和RPC其他层次中会涉及到的技术，在后续的文章中进行逐步介绍。","categories":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/categories/RPC/"}],"tags":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/tags/RPC/"}]},{"title":"打开orika的正确方式","slug":"orika","date":"2017-11-15T11:09:57.000Z","updated":"2017-11-17T02:32:59.029Z","comments":true,"path":"2017/11/15/orika/","link":"","permalink":"http://lexburner.github.io/2017/11/15/orika/","excerpt":"","text":"缘起架构分层开发分布式的项目时，DO持久化对象和DTO传输对象的转换是不可避免的。集中式项目中，DO-DAO-SERVICE-WEB的分层再寻常不过，但分布式架构（或微服务架构）需要拆分模块时，不得不思考一个问题：WEB层能不能出现DAO或者DO对象？我给出的答案是否定的。 这张图曾出现在我过去的文章中，其强调了一个分层的要素：服务层(应用层)和表现层应当解耦，后者不应当触碰到任何持久化对象，其所有的数据来源，均应当由前者提供。 DTO的位置就系统的某一个模块，可以大致分成领域层model，接口定义层api，接口实现层/服务层service，表现层web。 service 依赖 model + api web 依赖 api 在我们系统构建初期，DTO对象被想当然的丢到了model层，这导致web对model产生了依赖；而在后期，为了满足前面的架构分层，最终将DTO对象移动到了api层（没有单独做一层） 没有DTO时的痛点激发出DTO这样一个新的分层其实还有两个原因。 其一，便是我们再也不能忍受在RPC调用时JPA/hibernate懒加载这一特性带来的坑点。如果试图在消费端获取服务端传来的一个懒加载持久化对象，那么很抱歉，下意识就会发现这行不通，懒加载技术本质是使用字节码技术完成对象的代理，然而代理对象无法天然地远程传输，这与你的协议（RPC or HTTP）无关。 其二，远程调用需要额外注意网络传输的开销，如果生产者方从数据库加载出了一个一对多的依赖，而消费者只需要一这个实体的某个属性，多的实体会使得性能产生下降，并没有很好的方式对其进行控制（忽略手动set）。可能有更多痛点，由此可见，共享持久层，缺少DTO层时，我们的系统灵活性和性能都受到了制约。 从DTO到Orika各类博客不乏对DTO的讨论，对领域驱动的理解，但却鲜有文章介绍，如何完成DO对象到DTO对象的转换。我们期待有一款高性能的，易用的工具来帮助我们完成实体类的转换。便引出了今天的主角：Orika。 Orika是什么？ Orika是一个简单、快速的JavaBean拷贝框架，它能够递归地将数据从一个JavaBean复制到另一个JavaBean，这在多层应用开发中是非常有用的。 Orika的竞品相信大多数朋友接触过apache的BeanUtils，直到认识了spring的BeanUtils，前者被后者完爆，后来又出现了Dozer，Orika等重量级的Bean拷贝工具，在性能和特性上都有了很大的提升。 先给结论，众多Bean拷贝工具中，今天介绍的Orika具有想当大的优势。口说无凭，可参考下面文章中的各个工具的对比：http://tech.dianwoda.com/2017/11/04/gao-xing-neng-te-xing-feng-fu-de-beanying-she-gong-ju-orika/?utm_source=tuicool&amp;utm_medium=referral 简单整理后，如下所示： BeanUtils apache的BeanUtils和spring的BeanUtils中拷贝方法的原理都是先用jdk中 java.beans.Introspector类的getBeanInfo()方法获取对象的属性信息及属性get/set方法，接着使用反射（Method的invoke(Object obj, Object... args)）方法进行赋值。apache支持名称相同但类型不同的属性的转换，spring支持忽略某些属性不进行映射，他们都设置了缓存保存已解析过的BeanInfo信息。 BeanCopier cglib的BeanCopier采用了不同的方法：它不是利用反射对属性进行赋值，而是直接使用ASM的MethodVisitor直接编写各属性的get/set方法（具体过程可见BeanCopier类的generateClass(ClassVisitor v)方法）生成class文件，然后进行执行。由于是直接生成字节码执行，所以BeanCopier的性能较采用反射的BeanUtils有较大提高，这一点可在后面的测试中看出。 Dozer 使用以上类库虽然可以不用手动编写get/set方法，但是他们都不能对不同名称的对象属性进行映射。在定制化的属性映射方面做得比较好的有Dozer，Dozer支持简单属性映射、复杂类型映射、双向映射、隐式映射以及递归映射。可使用xml或者注解进行映射的配置，支持自动类型转换，使用方便。但Dozer底层是使用reflect包下Field类的set(Object obj, Object value)方法进行属性赋值，执行速度上不是那么理想。 Orika 那么有没有特性丰富，速度又快的Bean映射工具呢，这就是下面要介绍的Orika，Orika是近期在github活跃的项目，底层采用了javassist类库生成Bean映射的字节码，之后直接加载执行生成的字节码文件，因此在速度上比使用反射进行赋值会快很多，下面详细介绍Orika的使用方法。 Orika入门引入依赖12345&lt;dependency&gt; &lt;groupId&gt;ma.glasnost.orika&lt;/groupId&gt; &lt;artifactId&gt;orika-core&lt;/artifactId&gt; &lt;version&gt;$&#123;orika.version&#125;&lt;/version&gt;&lt;/dependency&gt; 基础概念 MapperFactory 1MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build(); MapperFactory用于注册字段映射，配置转换器，自定义映射器等，而我们关注的主要是字段映射这个特性，在下面的小节中会介绍。 MapperFacade 123MapperFacade mapper = mapperFactory.getMapperFacade();PersonSource source = new PersonSource();PersonDest destination = mapper.map(source, PersonDest.class); MapperFacade和spring，apache中的BeanUtils具有相同的地位，负责对象间的映射，也是实际使用中，我们使用的最多的类。 至于转换器，自定义映射器等等概念，属于Orika的高级特性，也是Orika为什么被称作一个重量级框架的原因，引入Orika的初衷是为了高性能，易用的拷贝对象，引入它们会给系统带来一定的侵入性，所以本文暂不介绍，详细的介绍，可参考官方文档：http://orika-mapper.github.io/orika-docs/intro.html 映射字段名完全相同的对象如果DO对象和DTO对象的命名遵守一定的规范，那无疑会减少我们很大的工作量。那么，规范是怎么样的呢？ 1234567891011121314151617181920class Person &#123; private String name; private int age; private Date birthDate; List&lt;Address&gt; addresses; // &lt;1&gt; // getters/setters omitted&#125;class PersonDto &#123; private String name; private int age; private Date birthDate; List&lt;AddressDto&gt; addresses; // &lt;1&gt; // getters/setters omitted&#125;class Address &#123; private String name;&#125;class AddressDto &#123; private String name;&#125; 基本字段类型自不用说，关键是打上标签的地方，按照通常的习惯，List&lt;AddressDto&gt;变量名会被命名为addressDtos，但我更加推荐与DO对象统一命名，命名为addresses。这样Orika在映射时便可以自动映射两者。 1234MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build();Person person = new Person();//一顿赋值PersonDto personDto = mapperFactory.getMapperFacade().map(person, PersonDto.class); 这样便完成了两个对象之间的拷贝，你可能会思考：需要我们指定两个类的映射关系吗？集合可以自动映射吗？这一切Orika都帮助我们完成了，在默认行为下，只要类的字段名相同，Orika便会尽自己最大的努力帮助我们映射。 映射字段名不一致的对象我对于DTO的理解是：DTO应当尽可能与DO的字段保持一致，不增不减不改，但可能出于一些特殊原因，需要映射两个名称不同的字段，Orika当然也支持这样常见的需求。只需要在MapperFactory中事先注册便可。 1234567891011public class Person &#123; private String id; private Name name; private List&lt;Name&gt; knownAliases; private Date birthDate;&#125;public class Name &#123; private String first; private String last; &#125; 1234567public class PersonDto &#123; private String personId; private String firstName; private String lastName; private Date birthDate; private String[][] aliases;&#125; 完成上述两个结构不甚相似的对象时，则需要我们额外做一些工作，剩下的便和之前一致了： 123456789MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build();factory.classMap(Person.class, PersonDto.class) // &lt;2&gt; .field(\"id\",\"personId\") .field(\"name.first\", \"firstName\") .field(\"name.last\", \"lastName\") .field(\"knownAliases&#123;first&#125;\", \"aliases&#123;[0]&#125;\") .field(\"knownAliases&#123;last&#125;\", \"aliases&#123;[1]&#125;\") .byDefault() //&lt;1&gt; .register(); 这些.{}[]这些略微有点复杂的表达式不需要被掌握，只是想表明：如果你有这样需求，Orika也能支持。上述连续点的行为被称为 fluent-style ，这再不少框架中有体现。 注意byDefault()这个方法，在指定了classMap行为之后，相同字段互相映射这样的默认行为需要调用一次这个方法，才能被继承。 classMap()方法返回了一个ClassMapBuilder对象，如上所示，我们见识到了它的field(),byDefault(),register()方法，这个建造者指定了对象映射的众多行为，还包括几个其他有用的方法： 12345classMapBuilder.field(\"a\",\"b\");//Person和PersonDto的双向映射classMapBuilder.fieldAToB(\"a\",\"b\");//单向映射 classMapBuilder.fieldBToA(\"a\",\"b\");//单向映射classMapBuilder.exclude(\"a\");//移除指定的字段映射，即使字段名相同也不会拷贝classMapBuilder.field(\"a\",\"b\").mapNulls(false).mapNullsInReverse(false);//是否拷贝空属性，默认是true 更多的API可以参见源码 集合映射在类中我们之前已经见识过了List与List的映射。如果根对象就是一个集合，List 映射为 List也是很常见的需求，这也很方便： 123MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build();List&lt;Person&gt; persons = new ArrayList&lt;&gt;();List&lt;PersonDto&gt; personDtos = mapperFactory.getMapperFacade().mapAsList(persons, PersonDto.class); 递归映射123456789101112class A &#123; private B b;&#125;class B &#123; private C c;&#125;class C &#123; private D d;&#125;class D &#123; private String name;&#125; Orika默认支持递归映射。 泛型映射对泛型的支持是Orika的另一强大功能，这点在文档中只是被提及，网上并没有找到任何一个例子，所以在此我想稍微着重的介绍一下。既然文档没有相关的介绍，那如何了解Orika是怎样支持泛型映射的呢？只能翻出Orika的源码，在其丰富的测试用例中，可以窥见其对各种泛型特性的支持：https://github.com/orika-mapper/orika/tree/master/tests/src/main/java/ma/glasnost/orika/test/generics 123456public class Response&lt;T&gt; &#123; private T data;&#125;public class ResponseDto&lt;T&gt; &#123; private T data;&#125; 当出现泛型时，按照前面的思路去拷贝，看看结果会如何，泛型示例1 12345678@Testpublic void genericTest1()&#123; MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build(); Response&lt;String&gt; response = new Response&lt;&gt;(); response.setData(\"test generic\"); ResponseDto&lt;String&gt; responseDto = mapperFactory.getMapperFacade().map(response, ResponseDto.class);// * Assert.assertFalse(\"test generic\".equals(responseDto.getData()));&#125; 会发现responseDto并不会Copy成功吗，特别是在*处，你会发现无所适从，没办法把ResponseDto传递进去 ，同样的，还有下面的泛型示例2 12345678910@Testpublic void genericTest2()&#123; MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build(); Response&lt;Person&gt; response = new Response&lt;&gt;(); Person person = new Person(); person.setName(\"test generic\"); response.setData(person); Response&lt;PersonDto&gt; responseDto = mapperFactory.getMapperFacade().map(response, Response.class); Assert.assertFalse(responseDto.getData() instanceof PersonDto);&#125; Response中的String和PersonDto在运行时(Runtime)泛型擦除这一特性难住了不少人，那么，Orika如何解决泛型映射呢？ 我们可以发现MapperFacade的具有一系列的重载方法，对各种类型的泛型拷贝进行支持 可以看到几乎每个方法都传入了一个Type，用于获取拷贝类的真实类型，而不是传入.class字节码，下面介绍正确的打开姿势： 12345678910@Testpublic void genericTest1() &#123; MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build(); Response&lt;String&gt; response = new Response&lt;&gt;(); response.setData(\"test generic\"); Type&lt;Response&lt;String&gt;&gt; fromType = new TypeBuilder&lt;Response&lt;String&gt;&gt;() &#123;&#125;.build(); Type&lt;ResponseDto&lt;String&gt;&gt; toType = new TypeBuilder&lt;ResponseDto&lt;String&gt;&gt;() &#123;&#125;.build(); ResponseDto&lt;String&gt; responseDto = mapperFactory.getMapperFacade().map(response, fromType, toType); Assert.assertTrue(\"test generic\".equals(responseDto.getData()));&#125; 123456789101112@Testpublic void genericTest2() &#123; MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build(); Response&lt;Person&gt; response = new Response&lt;&gt;(); Person person = new Person(); person.setName(\"test generic\"); response.setData(person); Type&lt;Response&lt;Person&gt;&gt; fromType = new TypeBuilder&lt;Response&lt;Person&gt;&gt;() &#123;&#125;.build(); Type&lt;Response&lt;PersonDto&gt;&gt; toType = new TypeBuilder&lt;Response&lt;PersonDto&gt;&gt;() &#123;&#125;.build(); Response&lt;PersonDto&gt; responseDto = mapperFactory.getMapperFacade().map(response, fromType, toType); Assert.assertEquals(\"test generic\" , responseDto.getData().getName());&#125; 浅拷贝or深拷贝虽然不值得一提，但职业敏感度还是催使我们想要测试一下，Orika是深拷贝还是浅拷贝，毕竟浅拷贝有时候会出现一些意想不到的坑点 123456789@Testpublic void deepCloneTest() throws Exception &#123; MapperFactory mapperFactory = new DefaultMapperFactory.Builder().build(); Person person = new Person(); Address address = new Address(); person.setAddress(address); PersonDto personDto = mapperFactory.getMapperFacade().map(person, PersonDto.class); Assert.assertFalse(personDto.getAddress().hashCode() == person.getAddress().hashCode());&#125; 结论：在使用Orika时可以放心，其实现的是深拷贝，不用担心原始类和克隆类指向同一个对象的问题。 更多的特性？你如果关心Orika是否能完成你某项特殊的需求，在这里可能会对你有所帮助：http://orika-mapper.github.io/orika-docs/faq.html 怎么样，你是不是还在使用BeanUtils呢？尝试一下Orika吧！","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"}]},{"title":"JAVA拾遗--关于SPI机制","slug":"spi","date":"2017-11-09T08:18:51.000Z","updated":"2017-11-10T05:34:11.772Z","comments":true,"path":"2017/11/09/spi/","link":"","permalink":"http://lexburner.github.io/2017/11/09/spi/","excerpt":"","text":"JDK提供的SPI(Service Provider Interface)机制，可能很多人不太熟悉，因为这个机制是针对厂商或者插件的，也可以在一些框架的扩展中看到。其核心类java.util.ServiceLoader可以在jdk1.8的文档中看到详细的介绍。虽然不太常见，但并不代表它不常用，恰恰相反，你无时无刻不在用它。玄乎了，莫急，思考一下你的项目中是否有用到第三方日志包，是否有用到数据库驱动？其实这些都和SPI有关。再来思考一下，现代的框架是如何加载日志依赖，加载数据库驱动的，你可能会对class.forName(“com.mysql.jdbc.Driver”)这段代码不陌生，这是每个java初学者必定遇到过的，但如今的数据库驱动仍然是这样加载的吗？你还能找到这段代码吗？这一切的疑问，将在本篇文章结束后得到解答。 首先介绍SPI机制是个什么东西 实现一个自定义的SPI1 项目结构 invoker是我们的用来测试的主项目。 interface是针对厂商和插件商定义的接口项目，只提供接口，不提供实现。 good-printer,bad-printer分别是两个厂商对interface的不同实现，所以他们会依赖于interface项目。 这个简单的demo就是让大家体验，在不改变invoker代码，只更改依赖的前提下，切换interface的实现厂商。 2 interface模块2.1 moe.cnkirito.spi.api.Printer 123public interface Printer &#123; void print();&#125; interface只定义一个接口，不提供实现。规范的制定方一般都是比较牛叉的存在，这些接口通常位于java，javax前缀的包中。这里的Printer就是模拟一个规范接口。 3 good-printer模块3.1 good-printer\\pom.xml 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;artifactId&gt;interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 规范的具体实现类必然要依赖规范接口 3.2 moe.cnkirito.spi.api.GoodPrinter 12345public class GoodPrinter implements Printer &#123; public void print() &#123; System.out.println(\"你是个好人~\"); &#125;&#125; 作为Printer规范接口的实现一 3.3 resources\\META-INF\\services\\moe.cnkirito.spi.api.Printer 1moe.cnkirito.spi.api.GoodPrinter 这里需要重点说明，每一个SPI接口都需要在自己项目的静态资源目录中声明一个services文件，文件名为实现规范接口的类名全路径，在此例中便是moe.cnkirito.spi.api.Printer，在文件中，则写上一行具体实现类的全路径，在此例中便是moe.cnkirito.spi.api.GoodPrinter。 这样一个厂商的实现便完成了。 4 bad-printer模块我们在按照和good-printer模块中定义的一样的方式，完成另一个厂商对Printer规范的实现。 4.1 bad-printer\\pom.xml1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;artifactId&gt;interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 4.2 moe.cnkirito.spi.api.BadPrinter 123456public class BadPrinter implements Printer &#123; public void print() &#123; System.out.println(\"我抽烟，喝酒，蹦迪，但我知道我是好女孩~\"); &#125;&#125; 4.3 resources\\META-INF\\services\\moe.cnkirito.spi.api.Printer 1moe.cnkirito.spi.api.BadPrinter 这样，另一个厂商的实现便完成了。 5 invoker模块 这里的invoker便是我们自己的项目了。如果一开始我们想使用厂商good-printer的Printer实现，是需要将其的依赖引入。 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;artifactId&gt;interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;artifactId&gt;good-printer&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 5.1 编写调用主类 12345678910public class MainApp &#123; public static void main(String[] args) &#123; ServiceLoader&lt;Printer&gt; printerLoader = ServiceLoader.load(Printer.class); for (Printer printer : printerLoader) &#123; printer.print(); &#125; &#125;&#125; ServiceLoader是java.util提供的用于加载固定类路径下文件的一个加载器，正是它加载了对应接口声明的实现类。 5.2 打印结果1 1你是个好人~ 如果在后续的方案中，想替换厂商的Printer实现，只需要将依赖更换 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;artifactId&gt;interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;moe.cnkirito&lt;/groupId&gt; &lt;artifactId&gt;bad-printer&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 调用主类无需变更代码，这符合开闭原则 5.3 打印结果2 1我抽烟，喝酒，蹦迪，但我知道我是好女孩~ 是不是很神奇呢？这一切对于调用者来说都是透明的，只需要切换依赖即可！ SPI在实际项目中的应用先总结下有什么新知识，resources/META-INF/services下的文件似乎我们之前没怎么接触过，ServiceLoader也没怎么接触过。那么现在我们打开自己项目的依赖，看看有什么发现。 在mysql-connector-java-xxx.jar中发现了META-INF\\services\\java.sql.Driver文件，里面只有两行记录： 12com.mysql.jdbc.Drivercom.mysql.fabric.jdbc.FabricMySQLDriver 我们可以分析出，java.sql.Driver是一个规范接口，com.mysql.jdbc.Drivercom.mysql.fabric.jdbc.FabricMySQLDriver则是mysql-connector-java-xxx.jar对这个规范的实现接口。 在jcl-over-slf4j-xxxx.jar中发现了META-INF\\services\\org.apache.commons.logging.LogFactory文件，里面只有一行记录： 1org.apache.commons.logging.impl.SLF4JLogFactory 相信不用我赘述，大家都能理解这是什么含义了 更多的还有很多，有兴趣可以自己翻一翻项目路径下的那些jar包 既然说到了数据库驱动，索性再多说一点，还记得一道经典的面试题：class.forName(“com.mysql.jdbc.Driver”)到底做了什么事？ 先思考下：自己会怎么回答？ 都知道class.forName与类加载机制有关，会触发执行com.mysql.jdbc.Driver类中的静态方法，从而使主类加载数据库驱动。如果再追问，为什么它的静态块没有自动触发？可答：因为数据库驱动类的特殊性质，JDBC规范中明确要求Driver类必须向DriverManager注册自己，导致其必须由class.forName手动触发，这可以在java.sql.Driver中得到解释。完美了吗？还没，来到最新的DriverManager源码中，可以看到这样的注释,翻译如下： DriverManager 类的方法 getConnection 和 getDrivers 已经得到提高以支持 Java Standard Edition Service Provider 机制。 JDBC 4.0 Drivers 必须包括 META-INF/services/java.sql.Driver 文件。此文件包含 java.sql.Driver 的 JDBC 驱动程序实现的名称。例如，要加载 my.sql.Driver 类，META-INF/services/java.sql.Driver 文件需要包含下面的条目： my.sql.Driver 应用程序不再需要使用 Class.forName() 显式地加载 JDBC 驱动程序。当前使用 Class.forName() 加载 JDBC 驱动程序的现有程序将在不作修改的情况下继续工作。 可以发现，Class.forName已经被弃用了，所以，这道题目的最佳回答，应当是和面试官牵扯到JAVA中的SPI机制，进而聊聊加载驱动的演变历史。 java.sql.DriverManager 1234567891011121314public Void run() &#123; ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class); Iterator&lt;Driver&gt; driversIterator = loadedDrivers.iterator(); try&#123; while(driversIterator.hasNext()) &#123; driversIterator.next(); &#125; &#125; catch(Throwable t) &#123; // Do nothing &#125; return null;&#125; 当然那，本节的内容还是主要介绍SPI，驱动这一块这是引申而出，如果不太理解，可以多去翻一翻jdk1.8中Driver和DriverManager的源码，相信会有不小的收获。 SPI在扩展方面的应用SPI不仅仅是为厂商指定的标准，同样也为框架扩展提供了一个思路。框架可以预留出SPI接口，这样可以在不侵入代码的前提下，通过增删依赖来扩展框架。前提是，框架得预留出核心接口，也就是本例中interface模块中类似的接口，剩下的适配工作便留给了开发者。 例如我的上一篇文章 https://www.cnkirito.moe/2017/11/07/spring-cloud-sleuth/ 中介绍的motan中Filter的扩展，便是采用了SPI机制，熟悉这个设定之后再回头去了解一些框架的SPI扩展就不会太陌生了。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"}]},{"title":"使用Spring Cloud Sleuth实现链路监控","slug":"spring-cloud-sleuth","date":"2017-11-07T08:58:01.000Z","updated":"2017-11-08T10:00:33.740Z","comments":true,"path":"2017/11/07/spring-cloud-sleuth/","link":"","permalink":"http://lexburner.github.io/2017/11/07/spring-cloud-sleuth/","excerpt":"","text":"在服务比较少的年代，一个系统的接口响应缓慢通常能够迅速被发现，但如今的微服务模块，大多具有规模大，依赖关系复杂等特性，错综复杂的网状结构使得我们不容易定位到某一个执行缓慢的接口。分布式的服务跟踪组件就是为了解决这一个问题。其次，它解决了另一个难题，在没有它之前，我们客户会一直询问：你们的系统有监控吗？你们的系统有监控吗？你们的系统有监控吗？现在，谢天谢地，他们终于不问了。是有点玩笑的成分，但可以肯定的一点是，实现全链路监控是保证系统健壮性的关键因子。 介绍Spring Cloud Sleuth和Zipkin的文章在网上其实并不少，所以我打算就我目前的系统来探讨一下，如何实现链路监控。全链路监控这个词意味着只要是不同系统模块之间的调用都应当被监控，这就包括了如下几种常用的交互方式： 1 Http协议，如RestTemplate，Feign，Okhttp3，HttpClient… 2 Rpc远程调用，如Motan，Dubbo，GRPC… 3 分布式Event，如RabbitMq，Kafka… 而我们项目目前混合使用了Http协议，Motan Rpc协议，所以本篇文章会着墨于实现这两块的链路监控。 项目结构 上面的项目结构是本次demo的核心结构，其中 zipkin-server作为服务跟踪的服务端，记录各个模块发送而来的调用请求，最终形成调用链路的报告。 order,goods两个模块为用来做测试的业务模块，分别实现了http形式和rpc形式的远程调用，最终我们会在zipkin-server的ui页面验证他们的调用记录。 interface存放了order和goods模块的公用接口，rpc调用需要一个公用的接口。 filter-opentracing存放了自定义的motan扩展代码，用于实现motan rpc调用的链路监控。 Zipkin服务端添加依赖 全部依赖 核心依赖 12345678910111213&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-storage-mysql&lt;/artifactId&gt; &lt;version&gt;1.28.0&lt;/version&gt;&lt;/dependency&gt; zipkin-autoconfigure-ui提供了默认了UI页面，zipkin-storage-mysql选择将链路调用信息存储在mysql中，更多的选择可以有elasticsearch，cassandra。 zipkin-server/src/main/resources/application.yml 12345678910111213spring: application: name: zipkin-server datasource: url: jdbc:mysql://localhost:3306/zipkin username: root password: root driver-class-name: com.mysql.jdbc.Driverzipkin: storage: type: mysqlserver: port: 9411 创建启动类 1234567891011121314@SpringBootApplication@EnableZipkinServerpublic class ZipkinServerApp &#123; @Bean public MySQLStorage mySQLStorage(DataSource datasource) &#123; return MySQLStorage.builder().datasource(datasource).executor(Runnable::run).build(); &#125; public static void main(String[] args) &#123; SpringApplication.run(ZipkinServerApp.class, args); &#125;&#125; 当前版本在手动配置数据库之后才不会启动报错，可能与版本有关。mysql相关的脚本可以在此处下载：mysql初始化脚本。 zipkin-server单独启动后，就可以看到链路监控页面了，此时由于没有收集到任何链路调用记录，显示如下： HTTP链路监控编写order和goods两个服务，在order暴露一个http端口，在goods中使用RestTemplate远程调用，之后查看在zipkin服务端查看调用信息。 首先添加依赖，让普通的应用具备收集和发送报告的能力，这一切在spring cloud sleuth的帮助下都变得很简单 添加依赖 全部依赖 核心依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;&lt;/dependency&gt; spring-cloud-starter-zipkin依赖内部包含了两个依赖，等于同时引入了spring-cloud-starter-sleuth，spring-cloud-sleuth-zipkin两个依赖。名字特别像，注意区分。 以order为例介绍配置文件 order/src/main/resources/application.yml 1234567891011spring: application: name: order # 1 zipkin: base-url: http://localhost:9411 # 2 sleuth: enabled: true sampler: percentage: 1 # 3server: port: 8060 指定项目名称可以方便的标记应用，在之后的监控页面可以看到这里的配置名称 指定zipkin的服务端，用于发送链路调用报告 采样率，值为[0,1]之间的任意实数，顾名思义，这里代表100%采集报告。 编写调用类 服务端order 123456789101112@RestController@RequestMapping(\"/api\")public class OrderController &#123; Logger logger = LoggerFactory.getLogger(OrderController.class); @RequestMapping(\"/order/&#123;id&#125;\") public MainOrder getOrder(@PathVariable(\"id\") String id) &#123; logger.info(\"order invoking ...\"); //&lt;1&gt; return new MainOrder(id, new BigDecimal(200D), new Date()); &#125;&#125; 客户端goods 12345public MainOrder test()&#123; ResponseEntity&lt;MainOrder&gt; mainOrderResponseEntity = restTemplate.getForEntity(\"http://localhost:8060/api/order/1144\", MainOrder.class); MainOrder body = mainOrderResponseEntity.getBody(); return body;&#125; 首先观察这一行日志在控制台是如何输出的 12017-11-08 09:54:00.633 INFO [order,d251f40af64361d2,e46132755dc395e1,true] 2780 --- [nio-8060-exec-1] m.c.sleuth.order.web.OrderController : order invoking ... 比没有引入sleuth之前多了一些信息，其中order,d251f40af64361d2,e46132755dc395e1,true分别代表了应用名称，traceId，spanId，当前调用是否被采集，关于trace，span这些专业词语，强烈建议去看看Dapper这篇论文，有很多中文翻译版本，并不是想象中的学术范，非常容易理解，很多链路监控文章中的截图都来自于这篇论文，我在此就不再赘述概念了。 紧接着，回到zipkin-server的监控页面，查看变化 到这里，Http监控就已经完成了，如果你的应用使用了其他的Http工具，如okhttp3，也可以去[opentracing，zipkin相关的文档中寻找依赖。 RPC链路监控虽说spring cloud是大势所趋，其推崇的http调用方式也是链路监控的主要对象，但不得不承认目前大多数的系统内部调用仍然是RPC的方式，至少我们内部的系统是如此，由于我们内部采用的RPC框架是weibo开源的motan，这里以此为例，介绍RPC的链路监控。motan使用SPI机制，实现了对链路监控的支持，https://github.com/weibocom/motan/issues/304这条issue中可以得知其加入了opentracing标准化追踪。但目前只能通过自己添加组件的方式才能配合spring-cloud-sleuth使用，下面来看看实现步骤。 filter-opentracing 实现思路：引入SleuthTracingFilter，作为全局的motan过滤器，给每一次motan的调用打上traceId和spanId，并编写一个SleuthTracingContext，持有一个SleuthTracerFactory工厂，用于适配不同的Tracer实现。 具体的实现可以参考文末的地址 order/src/main/resources/META-INF/services/com.weibo.api.motan.filter.Filter 1com.weibo.api.motan.filter.sleuth.SleuthTracingFilter 添加一行过滤器的声明，使得项目能够识别 配置SleuthTracingContext 123456789101112@BeanSleuthTracingContext sleuthTracingContext(@Autowired(required = false) org.springframework.cloud.sleuth.Tracer tracer)&#123; SleuthTracingContext context = new SleuthTracingContext(); context.setTracerFactory(new SleuthTracerFactory() &#123; @Override public org.springframework.cloud.sleuth.Tracer getTracer() &#123; return tracer; &#125; &#125;); return context;&#125; 使用spring-cloud-sleuth的Tracer作为motan调用的收集器 为服务端和客户端配置过滤器 123basicServiceConfigBean.setFilter(\"sleuth-tracing\");basicRefererConfigBean.setFilter(\"sleuth-tracing\"); 编写调用测试类 order作为客户端 12345678@MotanRefererGoodsApi goodsApi;@RequestMapping(\"/goods\")public String getGoodsList() &#123; logger.info(\"getGoodsList invoking ...\"); return goodsApi.getGoodsList();&#125; goods作为服务端 1234567891011@MotanServicepublic class GoodsApiImpl implements GoodsApi &#123; Logger logger = LoggerFactory.getLogger(GoodsApiImpl.class); @Override public String getGoodsList() &#123; logger.info(\"GoodsApi invoking ...\"); return \"success\"; &#125;&#125; 查看调用关系 第一张图中，使用前缀http和motan来区别调用的类型，第二张图中，依赖变成了双向的，因为一开始的http调用goods依赖于order，而新增了motan rpc调用之后order又依赖于goods。 总结系统间交互的方式除了http，rpc，还有另外的方式如mq，以后还可能会有更多的方式，但实现的监控的思路都是一致的，即如何无侵入式地给调用打上标签，记录报告。Dapper给实现链路监控提供了一个思路，而OpenTracing为各个框架不同的调用方式提供了适配接口….Spring Cloud Sleuth则是遵循了Spring一贯的风格，整合了丰富的资源，为我们的系统集成链路监控提供了很大的便捷性。 关于motan具体实现链路监控的代码由于篇幅限制，将源码放在了我的github中，如果你的系统使用了motan，可以用于参考：https://github.com/lexburner/sleuth-starter 参考《Spring Cloud微服务实战》– 翟永超 黄桂钱老师的指导","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/tags/Spring-Cloud/"}]},{"title":"Spring Data Redis（二）--序列化","slug":"spring-data-redis-2","date":"2017-10-28T08:10:55.000Z","updated":"2017-11-09T01:10:59.688Z","comments":true,"path":"2017/10/28/spring-data-redis-2/","link":"","permalink":"http://lexburner.github.io/2017/10/28/spring-data-redis-2/","excerpt":"","text":"默认序列化方案在上一篇文章《Spring Data Redis（一）》中，我们执行了这样一个操作： 1redisTemplate.opsForValue().set(\"student:1\",\"kirito\"); 试图使用RedisTemplate在Redis中存储一个键为“student:1”，值为“kirito”的String类型变量（redis中通常使用‘:’作为键的分隔符）。那么是否真的如我们所预想的那样，在Redis中存在这样的键值对呢？ 这可以说是Redis中最基础的操作了，但严谨起见，还是验证一下为妙，使用RedisDesktopManager可视化工具，或者redis-cli都可以查看redis中的数据。 emmmmm，大概能看出是我们的键值对，但前面似乎多了一些奇怪的16进制字符，在不了解RedisTemplate工作原理的情况下，自然会对这个现象产生疑惑。 首先看看springboot如何帮我们自动完成RedisTemplate的配置： 12345678910111213@Configurationprotected static class RedisConfiguration &#123; @Bean @ConditionalOnMissingBean(name = \"redisTemplate\") public RedisTemplate&lt;Object, Object&gt; redisTemplate( RedisConnectionFactory redisConnectionFactory) throws UnknownHostException &#123; RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;Object, Object&gt;(); template.setConnectionFactory(redisConnectionFactory); return template; &#125;&#125; 没看出什么特殊的设置，于是我们进入RedisTemplate自身的源码中一窥究竟。 首先是在类开头声明了一系列的序列化器： 123456789private boolean enableDefaultSerializer = true;// 配置默认序列化器private RedisSerializer&lt;?&gt; defaultSerializer;private ClassLoader classLoader;private RedisSerializer keySerializer = null;private RedisSerializer valueSerializer = null;private RedisSerializer hashKeySerializer = null;private RedisSerializer hashValueSerializer = null;private RedisSerializer&lt;String&gt; stringSerializer = new StringRedisSerializer(); 看到了我们关心的keySerializer和valueSerializer，在RedisTemplate.afterPropertiesSet()方法中，可以看到，默认的序列化方案: 12345678910111213141516171819202122232425262728public void afterPropertiesSet() &#123; super.afterPropertiesSet(); boolean defaultUsed = false; if (defaultSerializer == null) &#123; defaultSerializer = new JdkSerializationRedisSerializer( classLoader != null ? classLoader : this.getClass().getClassLoader()); &#125; if (enableDefaultSerializer) &#123; if (keySerializer == null) &#123; keySerializer = defaultSerializer; defaultUsed = true; &#125; if (valueSerializer == null) &#123; valueSerializer = defaultSerializer; defaultUsed = true; &#125; if (hashKeySerializer == null) &#123; hashKeySerializer = defaultSerializer; defaultUsed = true; &#125; if (hashValueSerializer == null) &#123; hashValueSerializer = defaultSerializer; defaultUsed = true; &#125; &#125; ... initialized = true;&#125; 默认的方案是使用了JdkSerializationRedisSerializer，所以导致了前面的结果，注意：字符串和使用jdk序列化之后的字符串是两个概念。 我们可以查看set方法的源码： 12345678910public void set(K key, V value) &#123; final byte[] rawValue = rawValue(value); execute(new ValueDeserializingRedisCallback(key) &#123; protected byte[] inRedis(byte[] rawKey, RedisConnection connection) &#123; connection.set(rawKey, rawValue); return null; &#125; &#125;, true);&#125; 最终与Redis交互使用的是原生的connection，键值则全部是字节数组，意味着所有的序列化都依赖于应用层完成，Redis只认字节！这也是引出本节介绍的初衷，序列化是与Redis打交道很关键的一个环节。 StringRedisSerializer在我不长的使用Redis的时间里，其实大多数操作是字符串操作，键值均为字符串，String.getBytes()即可满足需求。spring-data-redis也考虑到了这一点，其一，提供了StringRedisSerializer的实现，其二，提供了StringRedisTemplate，继承自RedisTemplate。 12345678910public class StringRedisTemplate extends RedisTemplate&lt;String, String&gt;&#123; public StringRedisTemplate() &#123; RedisSerializer&lt;String&gt; stringSerializer = new StringRedisSerializer(); setKeySerializer(stringSerializer); setValueSerializer(stringSerializer); setHashKeySerializer(stringSerializer); setHashValueSerializer(stringSerializer); &#125; ...&#125; 即只能存取字符串。尝试执行如下的代码： 1234@AutowiredStringRedisTemplate stringRedisTemplate;stringRedisTemplate.opsForValue().set(\"student:2\", \"SkYe\"); 再同样观察RedisDesktopManager中的变化： 由于更换了序列化器，我们得到的结果也不同了。 项目中序列化器使用的注意点理论上，字符串（本质是字节）其实是万能格式，是否可以使用StringRedisTemplate将复杂的对象存入Redis中，答案当然是肯定的。可以在应用层手动将对象序列化成字符串，如使用fastjson，jackson等工具，反序列化时也是通过字符串还原出原来的对象。而如果是用redisTemplate.opsForValue().set(&quot;student:3&quot;,new Student(3,&quot;kirito&quot;));便是依赖于内部的序列化器帮我们完成这样的一个流程，和使用stringRedisTemplate.opsForValue().set(&quot;student:3&quot;,JSON.toJSONString(new Student(3,&quot;kirito&quot;))); 其实是一个等价的操作。但有两点得时刻记住两点: Redis只认字节。 使用什么样的序列化器序列化，就必须使用同样的序列化器反序列化。 曾经在review代码时发现，项目组的两位同事操作redis，一个使用了RedisTemplate，一个使用了StringRedisTemplate，当他们操作同一个键时，key虽然相同，但由于序列化器不同，导致无法获取成功。差异虽小，但影响是非常可怕的。 另外一点是，微服务不同模块连接了同一个Redis，在共享内存中交互数据，可能会由于版本升级，模块差异，导致相互的序列化方案不一致，也会引起问题。如果项目中途切换了序列化方案，也可能会引起Redis中老旧持久化数据的反序列化异常，同样需要引起注意。最优的方案自然是在项目初期就统一好序列化方案，所有模块引用同一份依赖，避免不必要的麻烦（或者干脆全部使用默认配置）。 序列化接口RedisSerializer无论是RedisTemplate中默认使用的JdkSerializationRedisSerializer，还是StringRedisTemplate中使用的StringRedisSerializer都是实现自统一的接口RedisSerializer 1234public interface RedisSerializer&lt;T&gt; &#123; byte[] serialize(T t) throws SerializationException; T deserialize(byte[] bytes) throws SerializationException;&#125; 在spring-data-redis中提供了其他的默认实现，用于替换默认的序列化方案。 GenericToStringSerializer 依赖于内部的ConversionService，将所有的类型转存为字符串 GenericJackson2JsonRedisSerializer和Jackson2JsonRedisSerializer 以JSON的形式序列化对象 OxmSerializer 以XML的形式序列化对象 我们可能出于什么样的目的修改序列化器呢？按照个人理解可以总结为以下几点： 各个工程间约定了数据格式，如使用JSON等通用数据格式，可以让异构的系统接入Redis同样也能识别数据，而JdkSerializationRedisSerializer则不具备这样灵活的特性 数据的可视化，在项目初期我曾经偏爱JSON序列化，在运维时可以清晰地查看各个value的值，非常方便。 效率问题，如果需要将大的对象存入Value中，或者Redis IO非常频繁，替换合适的序列化器便可以达到优化的效果。 替换默认的序列化器可以将全局的RedisTemplate覆盖，也可以在使用时在局部实例化一个RedisTemplate替换（不依赖于IOC容器）需要根据实际的情况选择替换的方式，以Jackson2JsonRedisSerializer为例介绍全局替换的方式： 123456789101112131415161718@Beanpublic RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate redisTemplate = new RedisTemplate(); redisTemplate.setConnectionFactory(redisConnectionFactory); Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper objectMapper = new ObjectMapper();// &lt;1&gt; objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(objectMapper); redisTemplate.setKeySerializer(new StringRedisSerializer()); // &lt;2&gt; redisTemplate.setValueSerializer(jackson2JsonRedisSerializer); // &lt;2&gt; redisTemplate.afterPropertiesSet(); return redisTemplate;&#125; 修改Jackson序列化时的默认行为 手动指定RedisTemplate的Key和Value的序列化器 然后使用RedisTemplate进行保存： 12345678910@AutowiredStringRedisTemplate stringRedisTemplate;public void test() &#123; Student student3 = new Student(); student3.setName(\"kirito\"); student3.setId(\"3\"); student3.setHobbies(Arrays.asList(\"coding\",\"write blog\",\"eat chicken\")); redisTemplate.opsForValue().set(\"student:3\",student3);&#125; 紧接着，去RedisDesktopManager中查看结果： 标准的JSON格式 实现Kryo序列化我们也可以考虑根据自己项目和需求的特点，扩展序列化器，这是非常方便的。比如前面提到的，为了追求性能，可能考虑使用Kryo序列化器替换缓慢的JDK序列化器，如下是一个参考实现（为了demo而写，未经过生产验证） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class KryoRedisSerializer&lt;T&gt; implements RedisSerializer&lt;T&gt; &#123; private final static Logger logger = LoggerFactory.getLogger(KryoRedisSerializer.class); private static final ThreadLocal&lt;Kryo&gt; kryos = new ThreadLocal&lt;Kryo&gt;() &#123; protected Kryo initialValue() &#123; Kryo kryo = new Kryo(); return kryo; &#125;; &#125;; @Override public byte[] serialize(Object obj) throws SerializationException &#123; if (obj == null) &#123; throw new RuntimeException(\"serialize param must not be null\"); &#125; Kryo kryo = kryos.get(); Output output = new Output(64, -1); try &#123; kryo.writeClassAndObject(output, obj); return output.toBytes(); &#125; finally &#123; closeOutputStream(output); &#125; &#125; @Override public T deserialize(byte[] bytes) throws SerializationException &#123; if (bytes == null) &#123; return null; &#125; Kryo kryo = kryos.get(); Input input = null; try &#123; input = new Input(bytes); return (T) kryo.readClassAndObject(input); &#125; finally &#123; closeInputStream(input); &#125; &#125; private static void closeOutputStream(OutputStream output) &#123; if (output != null) &#123; try &#123; output.flush(); output.close(); &#125; catch (Exception e) &#123; logger.error(\"serialize object close outputStream exception\", e); &#125; &#125; &#125; private static void closeInputStream(InputStream input) &#123; if (input != null) &#123; try &#123; input.close(); &#125; catch (Exception e) &#123; logger.error(\"serialize object close inputStream exception\", e); &#125; &#125; &#125;&#125; 由于Kyro是线程不安全的，所以使用了一个ThreadLocal来维护，也可以挑选其他高性能的序列化方案如Hessian，Protobuf…","categories":[{"name":"Spring Data Redis","slug":"Spring-Data-Redis","permalink":"http://lexburner.github.io/categories/Spring-Data-Redis/"}],"tags":[{"name":"Spring Data Redis","slug":"Spring-Data-Redis","permalink":"http://lexburner.github.io/tags/Spring-Data-Redis/"}]},{"title":"Spring Data Redis（一）--解析RedisTemplate","slug":"spring-data-redis-1","date":"2017-10-27T08:10:55.000Z","updated":"2017-10-27T11:14:54.656Z","comments":true,"path":"2017/10/27/spring-data-redis-1/","link":"","permalink":"http://lexburner.github.io/2017/10/27/spring-data-redis-1/","excerpt":"","text":"谈及系统优化，缓存一直是不可或缺的一点。在缓存中间件层面，我们有MemCache，Redis等选择；在系统分层层面，又需要考虑多级缓存；在系统可用性层面，又要考虑到缓存雪崩，缓存穿透，缓存失效等常见的缓存问题…缓存的使用与优化值得我们花费一定的精力去深入理解。《Spring Data Redis》这个系列打算围绕spring-data-redis来进行分析，从hello world到源码分析，夹杂一些不多实战经验（经验有限），不止限于spring-data-redis本身，也会扩展谈及缓存这个大的知识点。 至于为何选择redis，相信不用我赘述，redis如今非常流行，几乎成了项目必备的组件之一。而spring-boot-starter-data-redis模块又为我们在spring集成的项目中提供了开箱即用的功能，更加便捷了我们开发。系列的第一篇便是简单介绍下整个组件最常用的一个工具类：RedisTemplate。 1 引入依赖1234567891011121314151617&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; springboot的老用户会发现redis依赖名称发生了一点小的变化，在springboot1.4之前，redis依赖的名称为：spring-boot-starter-redis，而在之后较新的版本中，使用spring-boot-starter-redis依赖，则会在项目启动时得到一个过期警告。意味着，我们应该彻底放弃旧的依赖。spring-data这个项目定位为spring提供一个统一的数据仓库接口，如（spring-boot-starter-data-jpa,spring-boot-starter-data-mongo,spring-boot-starter-data-rest），将redis纳入后，改名为了spring-boot-starter-data-redis。 2 配置redis连接resources/application.yml 123456spring: redis: host: 127.0.0.1 database: 0 port: 6379 password: 本机启动一个单点的redis即可，使用redis的0号库作为默认库（默认有16个库），在生产项目中一般会配置redis集群和哨兵保证redis的高可用，同样可以在application.yml中修改，非常方便。 3 编写测试类1234567891011121314151617181920import org.assertj.core.api.Assertions;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.test.context.junit4.SpringRunner;@RunWith(SpringRunner.class)@SpringBootTestpublic class ApplicationTests &#123; @Autowired private RedisTemplate redisTemplate;// &lt;1&gt; @Test public void test() throws Exception &#123; redisTemplate.opsForValue().set(\"student:1\", \"kirito\"); // &lt;2&gt; Assertions.assertThat(redisTemplate.opsForValue().get(\"student:1\")).isEqualTo(\"kirito\"); &#125;&#125; 引入了RedisTemplate，这个类是spring-starter-data-redis提供给应用直接访问redis的入口。从其命名就可以看出，其是模板模式在spring中的体现，与restTemplate，jdbcTemplate类似，而springboot为我们做了自动的配置，具体会在下文详解。 redisTemplate通常不直接操作键值，而是通过opsForXxx()访问，在本例中，key和value均为字符串类型。绑定字符串在实际开发中也是最为常用的操作类型。 4 详解RedisTemplate的APIRedisTemplate为我们操作Redis提供了丰富的API，可以将他们简单进行下归类。 4.1 常用数据操作这一类API也是我们最常用的一类。 众所周知，redis存在5种数据类型： 字符串类型（string），散列类型（hash），列表类型（list），集合类型（set），有序集合类型（zset） 而redisTemplate实现了RedisOperations接口，在其中，定义了一系列与redis相关的基础数据操作接口，数据类型分别于下来API对应： 123456789101112//非绑定key操作ValueOperations&lt;K, V&gt; opsForValue();&lt;HK, HV&gt; HashOperations&lt;K, HK, HV&gt; opsForHash();ListOperations&lt;K, V&gt; opsForList();SetOperations&lt;K, V&gt; opsForSet();ZSetOperations&lt;K, V&gt; opsForZSet();//绑定key操作BoundValueOperations&lt;K, V&gt; boundValueOps(K key);&lt;HK, HV&gt; BoundHashOperations&lt;K, HK, HV&gt; boundHashOps(K key);BoundListOperations&lt;K, V&gt; boundListOps(K key);BoundSetOperations&lt;K, V&gt; boundSetOps(K key);BoundZSetOperations&lt;K, V&gt; boundZSetOps(K key); 若以bound开头，则意味着在操作之初就会绑定一个key，后续的所有操作便默认认为是对该key的操作，算是一个小优化。 4.2 对原生Redis指令的支持Redis原生指令中便提供了一些很有用的操作，如设置key的过期时间，判断key是否存在等等… 常用的API列举： RedisTemplate API 原生Redis指令 说明 public void delete(K key) DEL key [key …] 删除给定的一个或多个 key public Boolean hasKey(K key) EXISTS key 检查给定 key 是否存在 public Boolean expire/expireAt(…) EXPIRE key seconds 为给定 key 设置生存时间，当 key 过期时(生存时间为 0 )，它会被自动删除。 public Long getExpire(K key) TTL key 以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)。 更多的原生Redis指令支持可以参考javadoc 4.3 CAS操作CAS（Compare and Swap）通常有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做。CAS也通常与并发，乐观锁，非阻塞，机器指令等关键词放到一起讲解。可能会有很多朋友在秒杀场景的架构设计中见到了Redis，本质上便是利用了Redis分布式共享内存的特性以及一系列的CAS指令。还记得在4.1中通过redisTemplate.opsForValue()或者redisTemplate.boundValueOps()可以得到一个ValueOperations或BoundValueOperations接口(以值为字符串的操作接口为例)，这些接口除了提供了基础操作外，还提供了一系列CAS操作，也可以放到RedisTemplate中一起理解。 常用的API列举： ValueOperations API 原生Redis指令 说明 Boolean setIfAbsent(K key, V value) SETNX key value 将 key 的值设为 value ，当且仅当 key 不存在。设置成功，返回 1 ， 设置失败，返回 0 。 V getAndSet(K key, V value) GETSET key value 将给定 key 的值设为 value ，并返回 key 的旧值(old value)。 Long increment(K key, long delta)/Double increment(K key, double delta) INCR/INCRBY/INCRBYFLOAT 将 key 所储存的值加上增量 increment 。 如果 key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行INCR/INCRBY/INCRBYFLOAT命令。线程安全的+ 关于CAS的理解可以参考我之前的文章java并发实践–CAS或者其他博文。 4.4 发布订阅redis之所以被冠以银弹，万金油的称号，关键在于其实现的功能真是太多了，甚至实现了一部分中间件队列的功能，其内置的channel机制，可以用于实现分布式的队列和广播。 RedisTemplate提供了convertAndSend()功能，用于发送消息，与RedisMessageListenerContainer 配合接收，便实现了一个简易的发布订阅。如果想要使用Redis实现发布订阅，可以参考我之前的文章。浅析分布式下的事件驱动机制 4.5 Lua脚本RedisTemplate中包含了这样一个Lua执行器，意味着我们可以使用RedisTemplate执行Lua脚本。 1private ScriptExecutor&lt;K&gt; scriptExecutor; Lua这门语言也非常有意思，小巧而精悍，有兴趣的朋友可以去了解一下nginx+lua开发，使用openResty框架。而Redis内置了Lua的解析器，由于Redis单线程的特性（不严谨），可以使用Lua脚本，完成一些线程安全的符合操作（CAS操作仅仅只能保证单个操作的线程安全，无法保证复合操作，如果你有这样的需求，可以考虑使用Redis+Lua脚本）。 123public &lt;T&gt; T execute(RedisScript&lt;T&gt; script, List&lt;K&gt; keys, Object... args) &#123; return scriptExecutor.execute(script, keys, args);&#125; 上述操作便可以完成对Lua脚本的调用。这儿有一个简单的示例，使用Redis+Lua脚本实现分布式的应用限流。分布式限流 5 总结Spring Data Redis系列的第一篇，介绍了spring-data对redis操作的封装，顺带了解redis具备的一系列特性，如果你对redis的理解还仅仅停留在它是一个分布式的key-value数据库，那么相信现在你一定会感叹其竟然如此强大。后续将会对缓存在项目中的应用以及spring-boot-starter-data-redis进一步解析。","categories":[{"name":"Spring Data Redis","slug":"Spring-Data-Redis","permalink":"http://lexburner.github.io/categories/Spring-Data-Redis/"}],"tags":[{"name":"Spring Data Redis","slug":"Spring-Data-Redis","permalink":"http://lexburner.github.io/tags/Spring-Data-Redis/"}]},{"title":"java小技巧(一)--远程debug","slug":"java-skill-1","date":"2017-10-25T09:24:48.000Z","updated":"2017-10-25T10:27:49.133Z","comments":true,"path":"2017/10/25/java-skill-1/","link":"","permalink":"http://lexburner.github.io/2017/10/25/java-skill-1/","excerpt":"","text":"该系列介绍一些java开发中常用的一些小技巧，多小呢，从不会到会只需要一篇文章这么小。这一篇介绍如何使用jdk自带的扩展包配合Intellij IDEA实现远程debug。 项目中经常会有出现这样的问题，会令程序员抓狂：关键代码段没有打印日志，本地环境正常生产环境却又问题…这时候，远程debug可能会启动作用。 1 准备用于debug的代码准备一个RestController用于接收请求，最后可以通过本地断点验证是否成功开启了远程debug 123456789101112131415@RestControllerpublic class TestController &#123; @RequestMapping(\"/test\") public Integer test() &#123; int i = 0; i++; i++; i++; i++; i++; return i; &#125;&#125; 项目使用springboot和maven构建，依赖就省略了，使用springboot提供的maven打包插件，方便我们打包成可运行的jar。 123456789101112131415161718&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;executable&gt;true&lt;/executable&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 2 使用maven插件打包成jar 3 准备启动脚本1java -jar -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=64057 remote-debug-1.0-SNAPSHOT.jar 使用java -jar的方式启动程序，并且添加了一串特殊的参数，这是我们能够开启远程debug的关键，以-开头的参数是jvm的标准启动参数，关于jvm启动参数相关的知识可以先去其他博客了解。 -agentlib:libname[=options], 用于装载本地lib包。在这条指令中便是加载了jdwp(Java Debug Wire Protocol)这个用于远程调试java的扩展包。而transport=dt_socket,server=y,suspend=n,address=64057这些便是jdwp装载时的定制参数，详细的参数作用可以搜索jdwp进行了解。我们需要关心的只有address=64057这个参数选项，本地调试程序使用64057端口与其通信，从而远程调试。 4 配置IDEA 与脚本中的指令完全一致 远程jar包运行的host，由于我的jar运行在本地，所以使用的是localhost，一般线上环境自然是修改为线上的地址 与远程jar包进行交互的端口号，idea会根据指令自动帮我们输入 选择与远程jar包一致的本地代码 请务必保证远程jar包的代码与本地代码一致！！！ 5 验证保存第4步的配置后，先执行脚本让远程的jar包跑起来，再在IDEA中运行remote-debug 如上便代表连接运行成功了 在本地打上断点，访问localhost:8080/test 可以在本地看到堆栈信息，大功告成。一行指令便完成了远程调试。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"}]},{"title":"浅析项目中的并发(二)","slug":"concurrent-2","date":"2017-10-15T03:11:11.000Z","updated":"2017-11-09T01:10:59.672Z","comments":true,"path":"2017/10/15/concurrent-2/","link":"","permalink":"http://lexburner.github.io/2017/10/15/concurrent-2/","excerpt":"","text":"分布式遭遇并发在前面的章节，并发操作要么发生在单个应用内，一般使用基于JVM的lock解决并发问题，要么发生在数据库，可以考虑使用数据库层面的锁，而在分布式场景下，需要保证多个应用实例都能够执行同步代码，则需要做一些额外的工作，一个最典型分布式同步方案便是使用分布式锁。 分布式锁由很多种实现，但本质上都是类似的，即依赖于共享组件实现锁的询问和获取，如果说单体式应用中的Monitor是由JVM提供的，那么分布式下Monitor便是由共享组件提供，而典型的共享组件大家其实并不陌生，包括但不限于：Mysql，Redis，Zookeeper。同时他们也代表了三种类型的共享组件：数据库，缓存，分布式协调组件。基于Consul的分布式锁，其实和基于Zookeeper的分布式锁大同小异，都是借助于分布式协调组件实现锁，大而化之，这三种类型的分布式锁，原理也都差不多，只不过，锁的特性和实现细节有所差异。 Redis实现分布式锁定义需求：A应用需要完成添加库存的操作，部署了A1，A2，A3多个实例，实例之间的操作要保证同步。 分析需求：显然，此时依赖于JVM的lock已经没办法解决问题了，A1添加锁，无法保证A2，A3的同步，这种场景可以考虑使用分布式锁应对。 建立一张Stock表，包含id，number两个字段，分别让A1，A2，A3并发对其操作，保证线程安全。 123456@Entitypublic class Stock &#123; @Id private String id; private Integer number;&#125; 定义数据库访问层： 12public interface StockRepository extends JpaRepository&lt;Stock,String&gt; &#123;&#125; 这一节的主角，redis分布式锁，使用开源的redis分布式锁实现：Redisson。 引入Redisson依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.5.4&lt;/version&gt;&lt;/dependency&gt; 定义测试类： 12345678910111213141516171819202122232425262728293031@RestControllerpublic class StockController &#123; @Autowired StockRepository stockRepository; ExecutorService executorService = Executors.newFixedThreadPool(10); @Autowired RedissonClient redissonClient; final static String id = \"1\"; @RequestMapping(\"/addStock\") public void addStock() &#123; RLock lock = redissonClient.getLock(\"redisson:lock:stock:\" + id); for (int i = 0; i &lt; 100; i++) &#123; executorService.execute(() -&gt; &#123; lock.lock(); try &#123; Stock stock = stockRepository.findOne(id); stock.setNumber(stock.getNumber() + 1); stockRepository.save(stock); &#125; finally &#123; lock.unlock(); &#125; &#125;); &#125; &#125;&#125; 上述的代码使得并发发生在多个层面。其一，在应用内部，启用线程池完成库存的加1操作，本身便是线程不安全的，其二，在多个应用之间，这样的加1操作更加是不受约束的。若初始化id为1的Stock数量为0。分别在本地启用A1(8080)，A2(8081)，A3(8082)三个应用，同时并发执行一次addStock()，若线程安全，必然可以使得数据库中的Stock为300，这便是我们的检测依据。 简单解读下上述的代码，使用redisson获取一把RLock，RLock是java.util.concurrent.locks.Lock接口的实现类，Redisson帮助我们屏蔽Redis分布式锁的实现细节，使用过java.util.concurrent.locks.Lock的朋友都会知道下述的代码可以被称得上是同步的起手范式，毕竟这是Lock的java doc中给出的代码： 1234567Lock l = ...;l.lock();try &#123; // access the resource protected by this lock&#125; finally &#123; l.unlock();&#125; 而redissonClient.getLock(&quot;redisson:lock:stock:&quot; + id)则是以&quot;redisson:lock:stock:&quot; + id该字符串作痛同步的Monitor，保证了不同id之间是互相不阻塞的。 为了保证发生并发，实际测试中我加入了Thread.sleep(1000)，使竞争得以发生。测试结果： Redis分布式锁的确起了作用。 锁的注意点如果仅仅是实现一个能够用于demo的Redis分布式锁并不难，但为何大家更偏向于使用开源的实现呢？主要还是可用性和稳定性，we make things work是我在写博客，写代码时牢记在脑海中的，如果真的要细究如何自己实现一个分布式锁，或者平时使用锁保证并发，需要有哪些注意点呢？列举几点：阻塞，超时时间，可重入，可用性，其他特性。 阻塞意味着各个操作之间的等待，A1正在执行增加库存时，A1其他的线程被阻塞，A2，A3中所有的线程被阻塞，在Redis中可以使用轮询策略以及redis底层提供的CAS原语(如setnx)来实现。（初学者可以理解为：在redis中设置一个key，想要执行lock代码时先询问是否有该key，如果有则代表其他线程在执行过程中，若没有，则设置该key，并且执行代码，执行完毕，释放key，而setnx保证操作的原子性） 超时时间在特殊情况，可能会导致锁无法被释放，如死锁，死循环等等意料之外的情况，锁超时时间的设置是有必要的，一个很直观的想法是给key设置过期时间即可。 如在Redisson中，lock提供了一个重载方法lock(long t, TimeUnit timeUnit);可以自定义过期时间。 可重入这个特性很容易被忽视，可重入其实并不难理解，顾名思义，一个方法在调用过程中是否可以被再次调用。实现可重入需要满足三个特性： 可以在执行的过程中可以被打断； 被打断之后，在该函数一次调用执行完之前，可以再次被调用（或进入，reentered)。 再次调用执行完之后，被打断的上次调用可以继续恢复执行，并正确执行。 比如下述的代码引用了全局变量，便是不可重入的： 12345678int t;void swap(int x, int y) &#123; t = x; x = y; y = t; System.out.println(\"x is\" + x + \" y is \" + y);&#125; 一个更加直观的例子便是，同一个线程中，某个方法的递归调用不应该被阻塞，所以如果要实现这个特性，简单的使用某个key作为Monitor是欠妥的，可以加入线程编号，来保证可重入。 使用可重入分布式锁的来测试计算斐波那契数列（只是为了验证可重入性）： 12345678910111213141516171819202122@RequestMapping(\"testReentrant\")public void ReentrantLock() &#123; RLock lock = redissonClient.getLock(\"fibonacci\"); lock.lock(); try &#123; int result = fibonacci(10); System.out.println(result); &#125; finally &#123; lock.unlock(); &#125;&#125;int fibonacci(int n) &#123; RLock lock = redissonClient.getLock(\"fibonacci\"); try &#123; if (n &lt;= 1) return n; else return fibonacci(n - 1) + fibonacci(n - 2); &#125; finally &#123; lock.unlock(); &#125;&#125; 最终输出：55，可以发现，只要是在同一线程之内，无论是递归调用还是外部加锁(同一把锁)，都不会造成死锁。 可用性借助于第三方中间件实现的分布式锁，都有这个问题，中间件挂了，会导致锁不可用，所以需要保证锁的高可用，这就需要保证中间件的可用性，如redis可以使用哨兵+集群，保证了中间件的可用性，便保证了锁的可用性、 其他特性除了可重入锁，锁的分类还有很多，在分布式下也同样可以实现，包括但不限于：公平锁，联锁，信号量，读写锁。Redisson也都提供了相关的实现类，其他的特性如并发容器等可以参考官方文档。 新手遭遇并发基本算是把项目中遇到的并发过了一遍了，案例其实很多，再简单罗列下一些新手可能会遇到的问题。 使用了线程安全的容器就是线程安全了吗？很多新手误以为使用了并发容器如：concurrentHashMap就万事大吉了，却不知道，一知半解的隐患可能比全然不懂更大。来看下面的代码： 123456789101112131415161718192021public class ConcurrentHashMapTest &#123; static Map&lt;String, Integer&gt; counter = new ConcurrentHashMap(); public static void main(String[] args) throws InterruptedException &#123; counter.put(\"stock1\", 0); ExecutorService executorService = Executors.newFixedThreadPool(10); CountDownLatch countDownLatch = new CountDownLatch(100); for (int i = 0; i &lt; 100; i++) &#123; executorService.execute(new Runnable() &#123; @Override public void run() &#123; counter.put(\"stock1\", counter.get(\"stock1\") + 1); countDownLatch.countDown(); &#125; &#125;); &#125; countDownLatch.await(); System.out.println(\"result is \" + counter.get(\"stock1\")); &#125;&#125; counter.put(&quot;stock1&quot;, counter.get(&quot;stock1&quot;) + 1)并不是原子操作，并发容器保证的是单步操作的线程安全特性，这一点往往初级程序员特别容易忽视。 总结项目中的并发场景是非常多的，而根据场景不同，同一个场景下的业务需求不同，以及数据量，访问量的不同，都会影响到锁的使用，架构中经常被提到的一句话是：业务决定架构，放到并发中也同样适用：业务决定控制并发的手段，如本文未涉及的队列的使用，本质上是化并发为串行，也解决了并发问题，都是控制的手段。了解锁的使用很简单，但如果使用，在什么场景下使用什么样的锁，这才是价值所在。 同一个线程之间的递归调用不应该被阻塞，所以如果要实现这个特性，简单的使用某个key作为Monitor是欠妥的，可以加入线程编号，来保证可重入。","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/categories/架构设计/"}],"tags":[{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/tags/架构设计/"}]},{"title":"浅析项目中的并发(一)","slug":"concurrent-1","date":"2017-10-11T13:29:40.000Z","updated":"2017-11-09T01:10:59.656Z","comments":true,"path":"2017/10/11/concurrent-1/","link":"","permalink":"http://lexburner.github.io/2017/10/11/concurrent-1/","excerpt":"","text":"前言控制并发的方法很多，从最基础的synchronized，juc中的lock，到数据库的行级锁，乐观锁，悲观锁，再到中间件级别的redis，zookeeper分布式锁。特别是初级程序员，对于所谓的锁一直都是听的比用的多，第一篇文章不深入探讨并发，更多的是一个入门介绍，适合于初学者，主题是“根据并发出现的具体业务场景，使用合理的控制并发手段”。 什么是并发由一个大家都了解的例子引入我们今天的主题：并发 类共享变量遭遇并发123456789101112131415161718192021222324public class Demo &#123; public Integer count = 0; public static void main(String[] args) &#123; final Demo demo = new Demo(); Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++)&#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; demo.count++; &#125; &#125;); &#125; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"final count value:\"+demo1.count); &#125;&#125; final count value:973 本例中创建了一个初始化时具有10个线程的线程池，多线程对类变量count进行自增操作。这个过程中，自增操作并不是线程安全的，happens-before原则并不会保障多个线程执行的先后顺序，导致了最终结果并不是想要的1000 下面，我们把并发中的共享资源从类变量转移到数据库中。 充血模型遭遇并发1234567891011121314@Componentpublic class Demo2 &#123; @Autowired TestNumDao testNumDao; @Transactional public void test()&#123; TestNum testNum = testNumDao.findOne(\"1\"); testNum.setCount(testNum.getCount()+1); testNumDao.save(testNum); &#125;&#125; 依旧使用多线程，对数据库中的记录进行+1操作 Demo2 demo2; public String test(){ Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++){ executor.execute(new Runnable() { @Override public void run() { demo2.test(); } }); } return &quot;test&quot;; } 数据库的记录 12id | count1 | 344 初窥门径的程序员会认为事务最基本的ACID中便包含了原子性，但是事务的原子性和今天所讲的并发中的原子操作仅仅是名词上有点类似。而有点经验的程序员都能知道这中间发生了什么，这只是暴露了项目中并发问题的冰山一角，千万不要认为上面的代码没有必要列举出来，我在实际项目开发中，曾经见到有多年工作经验的程序员仍然写出了类似于上述会出现并发问题的代码。 贫血模型遭遇并发1234567891011121314151617181920@RequestMapping(\"testSql\") @ResponseBody public String testSql() throws InterruptedException &#123; final CountDownLatch countDownLatch = new CountDownLatch(1000); long start = System.currentTimeMillis(); Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++)&#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; jdbcTemplate.execute(\"update test_num set count = count + 1 where id = '1'\"); countDownLatch.countDown(); &#125; &#125;); &#125; countDownLatch.await(); long costTime =System.currentTimeMillis() - start; System.out.println(\"共花费：\"+costTime+\" s\"); return \"testSql\"; &#125; 数据库结果： count ： 1000 达到了预期效果这个例子我顺便记录了耗时,控制台打印:共花费：113 ms简单对比一下二，三两个例子，都是想对数据库的count进行+1操作，唯一的区别就是，后者的+1计算发生在数据库，而前者的计算依赖于事先查出来的值，并且计算发生在程序的内存中。而现在大部分的ORM框架，导致了写充血模型的程序员变多，不注意并发的话，就会出现问题。下面我们来看看具体的业务场景。 业务场景 修改个人信息 修改商品信息 扣除账户余额，扣减库存 业务场景分析第一个场景，互联网如此众多的用户修改个人信息，这算不算并发？答案是：算也不算。算，从程序员角度来看，每一个用户请求进来，都是调用的同一个修改入口，具体一点，就是映射到controller层的同一个requestMapping，所以一定是并发的。不算，虽然程序是并发的，但是从用户角度来分析，每个人只可以修改自己的信息，所以，不同用户的操作其实是隔离的，所以不算“并发”。这也是为什么很多开发者，在日常开发中一直不注意并发控制，却也没有发生太大问题的原因，大多数初级程序员开发的还都是CRM，OA，CMS系统。 回到我们的并发，第一种业务场景，是可以使用如上模式的，对于一条用户数据的修改，我们允许程序员读取数据到内存中，内存计算修改（耗时操作），提交更改，提交事务。 1234567//Transaction startUser user = userDao.findById(\"1\");user.setName(\"newName\");user.setAge(user.getAge()+1);...//其他耗时操作userDao.save(user);//Transaction commit 这个场景变现为：几乎不存在并发，不需要控制，场景乐观。 为了严谨，也可以选择控制并发，但我觉得这需要交给写这段代码的同事，让他自由发挥。 第二个场景已经有所不同了，同样是修改一个记录，但是系统中可能有多个操作员来维护，此时，商品数据表现为一个共享数据，所以存在微弱的并发，通常表现为数据的脏读，例如操作员A，B同时对一个商品信息维护，我们希望只能有一个操作员修改成功，另外一个操作员得到错误提示（该商品信息已经发生变化），否则，两个人都以为自己修改成功了，但是其实只有一个人完成了操作，另一个人的操作被覆盖了。 这个场景表现为：存在并发，需要控制，允许失败，场景乐观。 通常我建议这种场景使用乐观锁，即在商品属性添加一个version字段标记修改的版本，这样两个操作员拿到同一个版本号，第一个操作员修改成功后版本号变化，另一个操作员的修改就会失败了。 1234567891011121314151617class Goods&#123; @Version int version;&#125;//Transaction starttry&#123; Goods goods = goodsDao.findById(\"1\"); goods.setName(\"newName\"); goods.setPrice(goods.getPrice()+100.00); ...//其他耗时操作 goodsDao.save(goods);&#125;catch(org.hibernate.StaleObjectStateException e)&#123; //返回给前台&#125;//Transaction commit springdata配合jpa可以自动捕获version异常，也可以自动手动对比。 第三个场景这个场景表现为：存在频繁的并发，需要控制，不允许失败，场景悲观。 强调一下，本例不应该使用在项目中，只是为了举例而设置的一个场景，因为这种贫血模型无法满足复杂的业务场景，而且依靠单机事务来保证一致性，并发性能和可扩展性能不好。 一个简易的秒杀场景，大量请求在短时间涌入，是不可能像第二种场景一样，100个并发请求，一个成功，其他99个全部异常的。 设计方案应该达到的效果是：有足够库存时，允许并发，库存到0时，之后的请求全部失败；有足够金额时，允许并发，金额不够支付时立刻告知余额不足。 可以利用数据库的行级锁，update set balance = balance - money where userId = ? and balance &gt;= money;update stock = stock - number where goodsId = ? and stock &gt;= number ; 然后在后台 查看返回值是否影响行数为1，判断请求是否成功，利用数据库保证并发。 需要补充一点，我这里所讲的秒杀，并不是指双11那种级别的秒杀，那需要多层架构去控制并发，前端拦截，负载均衡….不能仅仅依赖于数据库的，会导致严重的性能问题。为了留一下一个直观的感受，这里对比一下oracle，mysql的两个主流存储引擎：innodb，myisam的性能问题。123456oracle:10000个线程共计1000000次并发请求：共花费：101017 ms =&gt;101sinnodb:10000个线程共计1000000次并发请求：共花费：550330 ms =&gt;550smyisam:10000个线程共计1000000次并发请求：共花费：75802 ms =&gt;75s 可见，如果真正有大量请求到达数据库，光是依靠数据库解决并发是不现实的，所以仅仅只用数据库来做保障而不是完全依赖。需要根据业务场景选择合适的控制并发手段。","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/categories/架构设计/"}],"tags":[{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/tags/架构设计/"}]},{"title":"Spring Security(五)--动手实现一个IP_Login","slug":"spring-security-5","date":"2017-10-01T14:44:34.000Z","updated":"2017-11-09T01:10:59.688Z","comments":true,"path":"2017/10/01/spring-security-5/","link":"","permalink":"http://lexburner.github.io/2017/10/01/spring-security-5/","excerpt":"在开始这篇文章之前，我们似乎应该思考下为什么需要搞清楚Spring Security的内部工作原理？按照第二篇文章中的配置，一个简单的表单认证不就达成了吗？更有甚者，为什么我们不自己写一个表单认证，用过滤器即可完成，大费周章引入Spring Security，看起来也并没有方便多少。对的，在引入Spring Security之前，我们得首先想到，是什么需求让我们引入了Spring Security，以及为什么是Spring Security，而不是shiro等等其他安全框架。我的理解是有如下几点： 1 在前文的介绍中，Spring Security支持防止csrf攻击，session-fixation protection，支持表单认证，basic认证，rememberMe…等等一些特性，有很多是开箱即用的功能，而大多特性都可以通过配置灵活的变更，这是它的强大之处。 2 Spring Security的兄弟的项目Spring Security SSO，OAuth2等支持了多种协议，而这些都是基于Spring Security的，方便了项目的扩展。 3 SpringBoot的支持，更加保证了Spring Security的开箱即用。 4 为什么需要理解其内部工作原理?一个有自我追求的程序员都不会满足于浅尝辄止，如果一个开源技术在我们的日常工作中十分常用，那么我偏向于阅读其源码，这样可以让我们即使排查不期而至的问题，也方便日后需求扩展。 5 Spring及其子项目的官方文档是我见过的最良心的文档！相比较于Apache的部分文档 这一节，为了对之前分析的Spring Security源码和组件有一个清晰的认识，介绍一个使用IP完成登录的简单demo。","text":"在开始这篇文章之前，我们似乎应该思考下为什么需要搞清楚Spring Security的内部工作原理？按照第二篇文章中的配置，一个简单的表单认证不就达成了吗？更有甚者，为什么我们不自己写一个表单认证，用过滤器即可完成，大费周章引入Spring Security，看起来也并没有方便多少。对的，在引入Spring Security之前，我们得首先想到，是什么需求让我们引入了Spring Security，以及为什么是Spring Security，而不是shiro等等其他安全框架。我的理解是有如下几点： 1 在前文的介绍中，Spring Security支持防止csrf攻击，session-fixation protection，支持表单认证，basic认证，rememberMe…等等一些特性，有很多是开箱即用的功能，而大多特性都可以通过配置灵活的变更，这是它的强大之处。 2 Spring Security的兄弟的项目Spring Security SSO，OAuth2等支持了多种协议，而这些都是基于Spring Security的，方便了项目的扩展。 3 SpringBoot的支持，更加保证了Spring Security的开箱即用。 4 为什么需要理解其内部工作原理?一个有自我追求的程序员都不会满足于浅尝辄止，如果一个开源技术在我们的日常工作中十分常用，那么我偏向于阅读其源码，这样可以让我们即使排查不期而至的问题，也方便日后需求扩展。 5 Spring及其子项目的官方文档是我见过的最良心的文档！相比较于Apache的部分文档 这一节，为了对之前分析的Spring Security源码和组件有一个清晰的认识，介绍一个使用IP完成登录的简单demo。 5 动手实现一个IP_Login5.1 定义需求在表单登录中，一般使用数据库中配置的用户表，权限表，角色表，权限组表…这取决于你的权限粒度，但本质都是借助了一个持久化存储，维护了用户的角色权限，而后给出一个/login作为登录端点，使用表单提交用户名和密码，而后完成登录后可自由访问受限页面。 在我们的IP登录demo中，也是类似的，使用IP地址作为身份，内存中的一个ConcurrentHashMap维护IP地址和权限的映射，如果在认证时找不到相应的权限，则认为认证失败。 实际上，在表单登录中，用户的IP地址已经被存放在Authentication.getDetails()中了，完全可以只重写一个AuthenticationProvider认证这个IP地址即可，但是，本demo是为了厘清Spring Security内部工作原理而设置，为了设计到更多的类，我完全重写了IP过滤器。 5.2 设计概述我们的参考完全是表单认证，在之前章节中，已经了解了表单认证相关的核心流程，将此图再贴一遍： 在IP登录的demo中，使用IpAuthenticationProcessingFilter拦截IP登录请求，同样使用ProviderManager作为全局AuthenticationManager接口的实现类，将ProviderManager内部的DaoAuthenticationProvider替换为IpAuthenticationProvider，而UserDetailsService则使用一个ConcurrentHashMap代替。更详细一点的设计： IpAuthenticationProcessingFilter–&gt;UsernamePasswordAuthenticationFilter IpAuthenticationToken–&gt;UsernamePasswordAuthenticationToken ProviderManager–&gt;ProviderManager IpAuthenticationProvider–&gt;DaoAuthenticationProvider ConcurrentHashMap–&gt;UserDetailsService 5.3 IpAuthenticationToken123456789101112131415161718192021222324252627282930313233343536public class IpAuthenticationToken extends AbstractAuthenticationToken &#123; private String ip; public String getIp() &#123; return ip; &#125; public void setIp(String ip) &#123; this.ip = ip; &#125; public IpAuthenticationToken(String ip) &#123; super(null); this.ip = ip; super.setAuthenticated(false);//注意这个构造方法是认证时使用的 &#125; public IpAuthenticationToken(String ip, Collection&lt;? extends GrantedAuthority&gt; authorities) &#123; super(authorities); this.ip = ip; super.setAuthenticated(true);//注意这个构造方法是认证成功后使用的 &#125; @Override public Object getCredentials() &#123; return null; &#125; @Override public Object getPrincipal() &#123; return this.ip; &#125;&#125; 两个构造方法需要引起我们的注意，这里设计的用意是模仿的UsernamePasswordAuthenticationToken，第一个构造器是用于认证之前，传递给认证器使用的，所以只有IP地址，自然是未认证；第二个构造器用于认证成功之后，封装认证用户的信息，此时需要将权限也设置到其中，并且setAuthenticated(true)。这样的设计在诸多的Token类设计中很常见。 5.4 IpAuthenticationProcessingFilter1234567891011121314public class IpAuthenticationProcessingFilter extends AbstractAuthenticationProcessingFilter &#123; //使用/ipVerify该端点进行ip认证 IpAuthenticationProcessingFilter() &#123; super(new AntPathRequestMatcher(\"/ipVerify\")); &#125; @Override public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException, IOException, ServletException &#123; //获取host信息 String host = request.getRemoteHost(); //交给内部的AuthenticationManager去认证，实现解耦 return getAuthenticationManager().authenticate(new IpAuthenticationToken(host)); &#125;&#125; AbstractAuthenticationProcessingFilter这个过滤器在前面一节介绍过，是UsernamePasswordAuthenticationFilter的父类，我们的IpAuthenticationProcessingFilter也继承了它 构造器中传入了/ipVerify作为IP登录的端点 attemptAuthentication()方法中加载请求的IP地址，之后交给内部的AuthenticationManager去认证 5.5 IpAuthenticationProvider123456789101112131415161718192021222324252627282930public class IpAuthenticationProvider implements AuthenticationProvider &#123; final static Map&lt;String, SimpleGrantedAuthority&gt; ipAuthorityMap = new ConcurrenHashMap(); //维护一个ip白名单列表，每个ip对应一定的权限 static &#123; ipAuthorityMap.put(\"127.0.0.1\", new SimpleGrantedAuthority(\"ADMIN\")); ipAuthorityMap.put(\"10.236.69.103\", new SimpleGrantedAuthority(\"ADMIN\")); ipAuthorityMap.put(\"10.236.69.104\", new SimpleGrantedAuthority(\"FRIEND\")); &#125; @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; IpAuthenticationToken ipAuthenticationToken = (IpAuthenticationToken) authentication; String ip = ipAuthenticationToken.getIp(); SimpleGrantedAuthority simpleGrantedAuthority = ipAuthorityMap.get(ip); //不在白名单列表中 if (simpleGrantedAuthority == null) &#123; return null; &#125; else &#123; //封装权限信息，并且此时身份已经被认证 return new IpAuthenticationToken(ip, Arrays.asList(simpleGrantedAuthority)); &#125; &#125; //只支持IpAuthenticationToken该身份 @Override public boolean supports(Class&lt;?&gt; authentication) &#123; return (IpAuthenticationToken.class .isAssignableFrom(authentication)); &#125;&#125; return new IpAuthenticationToken(ip, Arrays.asList(simpleGrantedAuthority));使用了IpAuthenticationToken的第二个构造器，返回了一个已经经过认证的IpAuthenticationToken。 5.6 配置WebSecurityConfigAdapter1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; //ip认证者配置 @Bean IpAuthenticationProvider ipAuthenticationProvider() &#123; return new IpAuthenticationProvider(); &#125; //配置封装ipAuthenticationToken的过滤器 IpAuthenticationProcessingFilter ipAuthenticationProcessingFilter(AuthenticationManager authenticationManager) &#123; IpAuthenticationProcessingFilter ipAuthenticationProcessingFilter = new IpAuthenticationProcessingFilter(); //为过滤器添加认证器 ipAuthenticationProcessingFilter.setAuthenticationManager(authenticationManager); //重写认证失败时的跳转页面 ipAuthenticationProcessingFilter.setAuthenticationFailureHandler(new SimpleUrlAuthenticationFailureHandler(\"/ipLogin?error\")); return ipAuthenticationProcessingFilter; &#125; //配置登录端点 @Bean LoginUrlAuthenticationEntryPoint loginUrlAuthenticationEntryPoint()&#123; LoginUrlAuthenticationEntryPoint loginUrlAuthenticationEntryPoint = new LoginUrlAuthenticationEntryPoint (\"/ipLogin\"); return loginUrlAuthenticationEntryPoint; &#125; @Override protected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(\"/\", \"/home\").permitAll() .antMatchers(\"/ipLogin\").permitAll() .anyRequest().authenticated() .and() .logout() .logoutSuccessUrl(\"/\") .permitAll() .and() .exceptionHandling() .accessDeniedPage(\"/ipLogin\") .authenticationEntryPoint(loginUrlAuthenticationEntryPoint()) ; //注册IpAuthenticationProcessingFilter 注意放置的顺序 这很关键 http.addFilterBefore(ipAuthenticationProcessingFilter(authenticationManager()), UsernamePasswordAuthenticationFilter.class); &#125; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth.authenticationProvider(ipAuthenticationProvider()); &#125;&#125; WebSecurityConfigAdapter提供了我们很大的便利，不需要关注AuthenticationManager什么时候被创建，只需要使用其暴露的configure(AuthenticationManagerBuilder auth)便可以添加我们自定义的ipAuthenticationProvider。剩下的一些细节，注释中基本都写了出来。 5.7 配置SpringMVC1234567891011121314@Configurationpublic class MvcConfig extends WebMvcConfigurerAdapter &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController(\"/home\").setViewName(\"home\"); registry.addViewController(\"/\").setViewName(\"home\"); registry.addViewController(\"/hello\").setViewName(\"hello\"); registry.addViewController(\"/ip\").setViewName(\"ipHello\"); registry.addViewController(\"/ipLogin\").setViewName(\"ipLogin\"); &#125;&#125; 页面的具体内容和表单登录基本一致，可以在文末的源码中查看。 5.8 运行效果成功的流程 http://127.0.0.1:8080/访问首页，其中here链接到的地址为：http://127.0.0.1:8080/hello 点击here，由于http://127.0.0.1:8080/hello是受保护资源，所以跳转到了校验IP的页面。此时若点击Sign In by IP按钮，将会提交到/ipVerify端点，进行IP的认证。 登录校验成功之后，页面被成功重定向到了原先访问的 失败的流程 注意此时已经注销了上次的登录，并且，使用了localhost(localhost和127.0.0.1是两个不同的IP地址，我们的内存中只有127.0.0.1的用户,没有localhost的用户) 点击here后，由于没有认证过，依旧跳转到登录页面 此时，我们发现使用localhost，并没有认证成功，符合我们的预期 5.9 总结一个简单的使用Spring Security来进行验证IP地址的登录demo就已经完成了，这个demo主要是为了更加清晰地阐释Spring Security内部工作的原理设置的，其本身没有实际的项目意义，认证IP其实也不应该通过Spring Security的过滤器去做，退一步也应该交给Filter去做（这个Filter不存在于Spring Security的过滤器链中），而真正项目中，如果真正要做黑白名单这样的功能，一般选择在网关层或者nginx的扩展模块中做。再次特地强调下，怕大家误解。 最后祝大家国庆玩的开心~ 本节的代码可以在github中下载源码：https://github.com/lexburner/spring-security-ipLogin","categories":[{"name":"Spring Security","slug":"Spring-Security","permalink":"http://lexburner.github.io/categories/Spring-Security/"}],"tags":[{"name":"Spring Security","slug":"Spring-Security","permalink":"http://lexburner.github.io/tags/Spring-Security/"}]},{"title":"Spring Security(四)--核心过滤器源码分析","slug":"spring-security-4","date":"2017-09-30T15:25:34.000Z","updated":"2017-10-25T10:29:04.330Z","comments":true,"path":"2017/09/30/spring-security-4/","link":"","permalink":"http://lexburner.github.io/2017/09/30/spring-security-4/","excerpt":"","text":"[TOC] 前面的部分，我们关注了Spring Security是如何完成认证工作的，但是另外一部分核心的内容：过滤器，一直没有提到，我们已经知道Spring Security使用了springSecurityFillterChian作为了安全过滤的入口，这一节主要分析一下这个过滤器链都包含了哪些关键的过滤器，并且各自的使命是什么。 4 过滤器详解4.1 核心过滤器概述由于过滤器链路中的过滤较多，即使是Spring Security的官方文档中也并未对所有的过滤器进行介绍，在之前，《Spring Security(二)–Guides》入门指南中我们配置了一个表单登录的demo，以此为例，来看看这过程中Spring Security都帮我们自动配置了哪些过滤器。 123456789101112Creating filter chain: o.s.s.web.util.matcher.AnyRequestMatcher@1, [o.s.s.web.context.SecurityContextPersistenceFilter@8851ce1, o.s.s.web.header.HeaderWriterFilter@6a472566, o.s.s.web.csrf.CsrfFilter@61cd1c71, o.s.s.web.authentication.logout.LogoutFilter@5e1d03d7, o.s.s.web.authentication.UsernamePasswordAuthenticationFilter@122d6c22, o.s.s.web.savedrequest.RequestCacheAwareFilter@5ef6fd7f, o.s.s.web.servletapi.SecurityContextHolderAwareRequestFilter@4beaf6bd, o.s.s.web.authentication.AnonymousAuthenticationFilter@6edcad64, o.s.s.web.session.SessionManagementFilter@5e65afb6, o.s.s.web.access.ExceptionTranslationFilter@5b9396d3, o.s.s.web.access.intercept.FilterSecurityInterceptor@3c5dbdf8] 上述的log信息是我从springboot启动的日志中CV所得，spring security的过滤器日志有一个特点：log打印顺序与实际配置顺序符合，也就意味着SecurityContextPersistenceFilter是整个过滤器链的第一个过滤器，而FilterSecurityInterceptor则是末置的过滤器。另外通过观察过滤器的名称，和所在的包名，可以大致地分析出他们各自的作用，如UsernamePasswordAuthenticationFilter明显便是与使用用户名和密码登录相关的过滤器，而FilterSecurityInterceptor我们似乎看不出它的作用，但是其位于web.access包下，大致可以分析出他与访问限制相关。第四篇文章主要就是介绍这些常用的过滤器，对其中关键的过滤器进行一些源码分析。先大致介绍下每个过滤器的作用： SecurityContextPersistenceFilter 两个主要职责：请求来临时，创建SecurityContext安全上下文信息，请求结束时清空SecurityContextHolder。 HeaderWriterFilter (文档中并未介绍，非核心过滤器) 用来给http响应添加一些Header,比如X-Frame-Options, X-XSS-Protection*，X-Content-Type-Options. CsrfFilter 在spring4这个版本中被默认开启的一个过滤器，用于防止csrf攻击，了解前后端分离的人一定不会对这个攻击方式感到陌生，前后端使用json交互需要注意的一个问题。 LogoutFilter 顾名思义，处理注销的过滤器 UsernamePasswordAuthenticationFilter 这个会重点分析，表单提交了username和password，被封装成token进行一系列的认证，便是主要通过这个过滤器完成的，在表单认证的方法中，这是最最关键的过滤器。 RequestCacheAwareFilter (文档中并未介绍，非核心过滤器) 内部维护了一个RequestCache，用于缓存request请求 SecurityContextHolderAwareRequestFilter 此过滤器对ServletRequest进行了一次包装，使得request具有更加丰富的API AnonymousAuthenticationFilter 匿名身份过滤器，这个过滤器个人认为很重要，需要将它与UsernamePasswordAuthenticationFilter 放在一起比较理解，spring security为了兼容未登录的访问，也走了一套认证流程，只不过是一个匿名的身份。 SessionManagementFilter 和session相关的过滤器，内部维护了一个SessionAuthenticationStrategy，两者组合使用，常用来防止session-fixation protection attack，以及限制同一用户开启多个会话的数量 ExceptionTranslationFilter 直译成异常翻译过滤器，还是比较形象的，这个过滤器本身不处理异常，而是将认证过程中出现的异常交给内部维护的一些类去处理，具体是那些类下面详细介绍 FilterSecurityInterceptor 这个过滤器决定了访问特定路径应该具备的权限，访问的用户的角色，权限是什么？访问的路径需要什么样的角色和权限？这些判断和处理都是由该类进行的。 其中加粗的过滤器可以被认为是Spring Security的核心过滤器，将在下面，一个过滤器对应一个小节来讲解。 4.2 SecurityContextPersistenceFilter试想一下，如果我们不使用Spring Security，如果保存用户信息呢，大多数情况下会考虑使用Session对吧？在Spring Security中也是如此，用户在登录过一次之后，后续的访问便是通过sessionId来识别，从而认为用户已经被认证。具体在何处存放用户信息，便是第一篇文章中提到的SecurityContextHolder；认证相关的信息是如何被存放到其中的，便是通过SecurityContextPersistenceFilter。在4.1概述中也提到了，SecurityContextPersistenceFilter的两个主要作用便是请求来临时，创建SecurityContext安全上下文信息和请求结束时清空SecurityContextHolder。顺带提一下：微服务的一个设计理念需要实现服务通信的无状态，而http协议中的无状态意味着不允许存在session，这可以通过setAllowSessionCreation(false) 实现，这并不意味着SecurityContextPersistenceFilter变得无用，因为它还需要负责清除用户信息。在Spring Security中，虽然安全上下文信息被存储于Session中，但我们在实际使用中不应该直接操作Session，而应当使用SecurityContextHolder。 源码分析org.springframework.security.web.context.SecurityContextPersistenceFilter 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class SecurityContextPersistenceFilter extends GenericFilterBean &#123; static final String FILTER_APPLIED = \"__spring_security_scpf_applied\"; //安全上下文存储的仓库 private SecurityContextRepository repo; public SecurityContextPersistenceFilter() &#123; //HttpSessionSecurityContextRepository是SecurityContextRepository接口的一个实现类 //使用HttpSession来存储SecurityContext this(new HttpSessionSecurityContextRepository()); &#125; public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest request = (HttpServletRequest) req; HttpServletResponse response = (HttpServletResponse) res; if (request.getAttribute(FILTER_APPLIED) != null) &#123; // ensure that filter is only applied once per request chain.doFilter(request, response); return; &#125; request.setAttribute(FILTER_APPLIED, Boolean.TRUE); //包装request，response HttpRequestResponseHolder holder = new HttpRequestResponseHolder(request, response); //从Session中获取安全上下文信息 SecurityContext contextBeforeChainExecution = repo.loadContext(holder); try &#123; //请求开始时，设置安全上下文信息，这样就避免了用户直接从Session中获取安全上下文信息 SecurityContextHolder.setContext(contextBeforeChainExecution); chain.doFilter(holder.getRequest(), holder.getResponse()); &#125; finally &#123; //请求结束后，清空安全上下文信息 SecurityContext contextAfterChainExecution = SecurityContextHolder .getContext(); SecurityContextHolder.clearContext(); repo.saveContext(contextAfterChainExecution, holder.getRequest(), holder.getResponse()); request.removeAttribute(FILTER_APPLIED); if (debug) &#123; logger.debug(\"SecurityContextHolder now cleared, as request processing completed\"); &#125; &#125; &#125;&#125; 过滤器一般负责核心的处理流程，而具体的业务实现，通常交给其中聚合的其他实体类，这在Filter的设计中很常见，同时也符合职责分离模式。例如存储安全上下文和读取安全上下文的工作完全委托给了HttpSessionSecurityContextRepository去处理，而这个类中也有几个方法可以稍微解读下，方便我们理解内部的工作流程 org.springframework.security.web.context.HttpSessionSecurityContextRepository 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class HttpSessionSecurityContextRepository implements SecurityContextRepository &#123; // 'SPRING_SECURITY_CONTEXT'是安全上下文默认存储在Session中的键值 public static final String SPRING_SECURITY_CONTEXT_KEY = \"SPRING_SECURITY_CONTEXT\"; ... private final Object contextObject = SecurityContextHolder.createEmptyContext(); private boolean allowSessionCreation = true; private boolean disableUrlRewriting = false; private String springSecurityContextKey = SPRING_SECURITY_CONTEXT_KEY; private AuthenticationTrustResolver trustResolver = new AuthenticationTrustResolverImpl(); //从当前request中取出安全上下文，如果session为空，则会返回一个新的安全上下文 public SecurityContext loadContext(HttpRequestResponseHolder requestResponseHolder) &#123; HttpServletRequest request = requestResponseHolder.getRequest(); HttpServletResponse response = requestResponseHolder.getResponse(); HttpSession httpSession = request.getSession(false); SecurityContext context = readSecurityContextFromSession(httpSession); if (context == null) &#123; context = generateNewContext(); &#125; ... return context; &#125; ... public boolean containsContext(HttpServletRequest request) &#123; HttpSession session = request.getSession(false); if (session == null) &#123; return false; &#125; return session.getAttribute(springSecurityContextKey) != null; &#125; private SecurityContext readSecurityContextFromSession(HttpSession httpSession) &#123; if (httpSession == null) &#123; return null; &#125; ... // Session存在的情况下，尝试获取其中的SecurityContext Object contextFromSession = httpSession.getAttribute(springSecurityContextKey); if (contextFromSession == null) &#123; return null; &#125; ... return (SecurityContext) contextFromSession; &#125; //初次请求时创建一个新的SecurityContext实例 protected SecurityContext generateNewContext() &#123; return SecurityContextHolder.createEmptyContext(); &#125;&#125; SecurityContextPersistenceFilter和HttpSessionSecurityContextRepository配合使用，构成了Spring Security整个调用链路的入口，为什么将它放在最开始的地方也是显而易见的，后续的过滤器中大概率会依赖Session信息和安全上下文信息。 4.3 UsernamePasswordAuthenticationFilter表单认证是最常用的一个认证方式，一个最直观的业务场景便是允许用户在表单中输入用户名和密码进行登录，而这背后的UsernamePasswordAuthenticationFilter，在整个Spring Security的认证体系中则扮演着至关重要的角色。 上述的时序图，可以看出UsernamePasswordAuthenticationFilter主要肩负起了调用身份认证器，校验身份的作用，至于认证的细节，在前面几章花了很大篇幅进行了介绍，到这里，其实Spring Security的基本流程就已经走通了。 源码分析org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter#attemptAuthentication 123456789101112131415public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException &#123; //获取表单中的用户名和密码 String username = obtainUsername(request); String password = obtainPassword(request); ... username = username.trim(); //组装成username+password形式的token UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken( username, password); // Allow subclasses to set the \"details\" property setDetails(request, authRequest); //交给内部的AuthenticationManager去认证，并返回认证信息 return this.getAuthenticationManager().authenticate(authRequest);&#125; UsernamePasswordAuthenticationFilter本身的代码只包含了上述这么一个方法，非常简略，而在其父类AbstractAuthenticationProcessingFilter中包含了大量的细节，值得我们分析： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public abstract class AbstractAuthenticationProcessingFilter extends GenericFilterBean implements ApplicationEventPublisherAware, MessageSourceAware &#123; //包含了一个身份认证器 private AuthenticationManager authenticationManager; //用于实现remeberMe private RememberMeServices rememberMeServices = new NullRememberMeServices(); private RequestMatcher requiresAuthenticationRequestMatcher; //这两个Handler很关键，分别代表了认证成功和失败相应的处理器 private AuthenticationSuccessHandler successHandler = new SavedRequestAwareAuthenticationSuccessHandler(); private AuthenticationFailureHandler failureHandler = new SimpleUrlAuthenticationFailureHandler(); public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest request = (HttpServletRequest) req; HttpServletResponse response = (HttpServletResponse) res; ... Authentication authResult; try &#123; //此处实际上就是调用UsernamePasswordAuthenticationFilter的attemptAuthentication方法 authResult = attemptAuthentication(request, response); if (authResult == null) &#123; //子类未完成认证，立刻返回 return; &#125; sessionStrategy.onAuthentication(authResult, request, response); &#125; //在认证过程中可以直接抛出异常，在过滤器中，就像此处一样，进行捕获 catch (InternalAuthenticationServiceException failed) &#123; //内部服务异常 unsuccessfulAuthentication(request, response, failed); return; &#125; catch (AuthenticationException failed) &#123; //认证失败 unsuccessfulAuthentication(request, response, failed); return; &#125; //认证成功 if (continueChainBeforeSuccessfulAuthentication) &#123; chain.doFilter(request, response); &#125; //注意，认证成功后过滤器把authResult结果也传递给了成功处理器 successfulAuthentication(request, response, chain, authResult); &#125; &#125; 整个流程理解起来也并不难，主要就是内部调用了authenticationManager完成认证，根据认证结果执行successfulAuthentication或者unsuccessfulAuthentication，无论成功失败，一般的实现都是转发或者重定向等处理，不再细究AuthenticationSuccessHandler和AuthenticationFailureHandler，有兴趣的朋友，可以去看看两者的实现类。 4.4 AnonymousAuthenticationFilter匿名认证过滤器，可能有人会想：匿名了还有身份？我自己对于Anonymous匿名身份的理解是Spirng Security为了整体逻辑的统一性，即使是未通过认证的用户，也给予了一个匿名身份。而AnonymousAuthenticationFilter该过滤器的位置也是非常的科学的，它位于常用的身份认证过滤器（如UsernamePasswordAuthenticationFilter、BasicAuthenticationFilter、RememberMeAuthenticationFilter）之后，意味着只有在上述身份过滤器执行完毕后，SecurityContext依旧没有用户信息，AnonymousAuthenticationFilter该过滤器才会有意义—-基于用户一个匿名身份。 源码分析org.springframework.security.web.authentication.AnonymousAuthenticationFilter 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class AnonymousAuthenticationFilter extends GenericFilterBean implements InitializingBean &#123; private AuthenticationDetailsSource&lt;HttpServletRequest, ?&gt; authenticationDetailsSource = new WebAuthenticationDetailsSource(); private String key; private Object principal; private List&lt;GrantedAuthority&gt; authorities; //自动创建一个\"anonymousUser\"的匿名用户,其具有ANONYMOUS角色 public AnonymousAuthenticationFilter(String key) &#123; this(key, \"anonymousUser\", AuthorityUtils.createAuthorityList(\"ROLE_ANONYMOUS\")); &#125; /** * * @param key key用来识别该过滤器创建的身份 * @param principal principal代表匿名用户的身份 * @param authorities authorities代表匿名用户的权限集合 */ public AnonymousAuthenticationFilter(String key, Object principal, List&lt;GrantedAuthority&gt; authorities) &#123; Assert.hasLength(key, \"key cannot be null or empty\"); Assert.notNull(principal, \"Anonymous authentication principal must be set\"); Assert.notNull(authorities, \"Anonymous authorities must be set\"); this.key = key; this.principal = principal; this.authorities = authorities; &#125; ... public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException &#123; //过滤器链都执行到匿名认证过滤器这儿了还没有身份信息，塞一个匿名身份进去 if (SecurityContextHolder.getContext().getAuthentication() == null) &#123; SecurityContextHolder.getContext().setAuthentication( createAuthentication((HttpServletRequest) req)); &#125; chain.doFilter(req, res); &#125; protected Authentication createAuthentication(HttpServletRequest request) &#123; //创建一个AnonymousAuthenticationToken AnonymousAuthenticationToken auth = new AnonymousAuthenticationToken(key, principal, authorities); auth.setDetails(authenticationDetailsSource.buildDetails(request)); return auth; &#125; ...&#125; 其实对比AnonymousAuthenticationFilter和UsernamePasswordAuthenticationFilter就可以发现一些门道了，UsernamePasswordAuthenticationToken对应AnonymousAuthenticationToken，他们都是Authentication的实现类，而Authentication则是被SecurityContextHolder(SecurityContext)持有的，一切都被串联在了一起。 4.5 ExceptionTranslationFilterExceptionTranslationFilter异常转换过滤器位于整个springSecurityFilterChain的后方，用来转换整个链路中出现的异常，将其转化，顾名思义，转化以意味本身并不处理。一般其只处理两大类异常：AccessDeniedException访问异常和AuthenticationException认证异常。 这个过滤器非常重要，因为它将Java中的异常和HTTP的响应连接在了一起，这样在处理异常时，我们不用考虑密码错误该跳到什么页面，账号锁定该如何，只需要关注自己的业务逻辑，抛出相应的异常便可。如果该过滤器检测到AuthenticationException，则将会交给内部的AuthenticationEntryPoint去处理，如果检测到AccessDeniedException，需要先判断当前用户是不是匿名用户，如果是匿名访问，则和前面一样运行AuthenticationEntryPoint，否则会委托给AccessDeniedHandler去处理，而AccessDeniedHandler的默认实现，是AccessDeniedHandlerImpl。所以ExceptionTranslationFilter内部的AuthenticationEntryPoint是至关重要的，顾名思义：认证的入口点。 源码分析1234567891011121314151617181920212223242526272829public class ExceptionTranslationFilter extends GenericFilterBean &#123; //处理异常转换的核心方法 private void handleSpringSecurityException(HttpServletRequest request, HttpServletResponse response, FilterChain chain, RuntimeException exception) throws IOException, ServletException &#123; if (exception instanceof AuthenticationException) &#123; //重定向到登录端点 sendStartAuthentication(request, response, chain, (AuthenticationException) exception); &#125; else if (exception instanceof AccessDeniedException) &#123; Authentication authentication = SecurityContextHolder.getContext().getAuthentication(); if (authenticationTrustResolver.isAnonymous(authentication) || authenticationTrustResolver.isRememberMe(authentication)) &#123; //重定向到登录端点 sendStartAuthentication( request, response, chain, new InsufficientAuthenticationException( \"Full authentication is required to access this resource\")); &#125; else &#123; //交给accessDeniedHandler处理 accessDeniedHandler.handle(request, response, (AccessDeniedException) exception); &#125; &#125; &#125;&#125; 剩下的便是要搞懂AuthenticationEntryPoint和AccessDeniedHandler就可以了。 选择了几个常用的登录端点，以其中第一个为例来介绍，看名字就能猜到是认证失败之后，让用户跳转到登录页面。还记得我们一开始怎么配置表单登录页面的吗？ 12345678910111213141516171819@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(\"/\", \"/home\").permitAll() .anyRequest().authenticated() .and() .formLogin()//FormLoginConfigurer .loginPage(\"/login\") .permitAll() .and() .logout() .permitAll(); &#125;&#125; 我们顺着formLogin返回的FormLoginConfigurer往下找，看看能发现什么，最终在FormLoginConfigurer的父类AbstractAuthenticationFilterConfigurer中有了不小的收获： 12345678public abstract class AbstractAuthenticationFilterConfigurer extends ...&#123; ... //formLogin不出所料配置了AuthenticationEntryPoint private LoginUrlAuthenticationEntryPoint authenticationEntryPoint; //认证失败的处理器 private AuthenticationFailureHandler failureHandler; ...&#125; 具体如何配置的就不看了，我们得出了结论，formLogin()配置了之后最起码做了两件事，其一，为UsernamePasswordAuthenticationFilter设置了相关的配置，其二配置了AuthenticationEntryPoint。 登录端点还有Http401AuthenticationEntryPoint，Http403ForbiddenEntryPoint这些都是很简单的实现，有时候我们访问受限页面，又没有配置登录，就看到了一个空荡荡的默认错误页面，上面显示着401,403，就是这两个入口起了作用。 还剩下一个AccessDeniedHandler访问决策器未被讲解，简单提一下：AccessDeniedHandlerImpl这个默认实现类会根据errorPage和状态码来判断，最终决定跳转的页面 org.springframework.security.web.access.AccessDeniedHandlerImpl#handle 1234567891011121314151617181920public void handle(HttpServletRequest request, HttpServletResponse response, AccessDeniedException accessDeniedException) throws IOException, ServletException &#123; if (!response.isCommitted()) &#123; if (errorPage != null) &#123; // Put exception into request scope (perhaps of use to a view) request.setAttribute(WebAttributes.ACCESS_DENIED_403, accessDeniedException); // Set the 403 status code. response.setStatus(HttpServletResponse.SC_FORBIDDEN); // forward to error page. RequestDispatcher dispatcher = request.getRequestDispatcher(errorPage); dispatcher.forward(request, response); &#125; else &#123; response.sendError(HttpServletResponse.SC_FORBIDDEN, accessDeniedException.getMessage()); &#125; &#125;&#125; 4.6 FilterSecurityInterceptor想想整个认证安全控制流程还缺了什么？我们已经有了认证，有了请求的封装，有了Session的关联…还缺一个：由什么控制哪些资源是受限的，这些受限的资源需要什么权限，需要什么角色…这一切和访问控制相关的操作，都是由FilterSecurityInterceptor完成的。 FilterSecurityInterceptor的工作流程用笔者的理解可以理解如下：FilterSecurityInterceptor从SecurityContextHolder中获取Authentication对象，然后比对用户拥有的权限和资源所需的权限。前者可以通过Authentication对象直接获得，而后者则需要引入我们之前一直未提到过的两个类：SecurityMetadataSource，AccessDecisionManager。理解清楚决策管理器的整个创建流程和SecurityMetadataSource的作用需要花很大一笔功夫，这里，暂时只介绍其大概的作用。 在JavaConfig的配置中，我们通常如下配置路径的访问控制： 12345678910111213141516@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(\"/resources/**\", \"/signup\", \"/about\").permitAll() .antMatchers(\"/admin/**\").hasRole(\"ADMIN\") .antMatchers(\"/db/**\").access(\"hasRole('ADMIN') and hasRole('DBA')\") .anyRequest().authenticated() .withObjectPostProcessor(new ObjectPostProcessor&lt;FilterSecurityInterceptor&gt;() &#123; public &lt;O extends FilterSecurityInterceptor&gt; O postProcess( O fsi) &#123; fsi.setPublishAuthorizationSuccess(true); return fsi; &#125; &#125;);&#125; 在ObjectPostProcessor的泛型中看到了FilterSecurityInterceptor，以笔者的经验，目前并没有太多机会需要修改FilterSecurityInterceptor的配置。 总结本篇文章在介绍过滤器时，顺便进行了一些源码的分析，目的是方便理解整个Spring Security的工作流。伴随着整个过滤器链的介绍，安全框架的轮廓应该已经浮出水面了，下面的章节，主要打算通过自定义一些需求，再次分析其他组件的源码，学习应该如何改造Spring Security，为我们所用。","categories":[{"name":"Spring Security","slug":"Spring-Security","permalink":"http://lexburner.github.io/categories/Spring-Security/"}],"tags":[{"name":"Spring Security","slug":"Spring-Security","permalink":"http://lexburner.github.io/tags/Spring-Security/"}]},{"title":"警惕不规范的变量命名","slug":"project-rules-2","date":"2017-09-23T05:45:56.000Z","updated":"2017-09-25T01:26:34.184Z","comments":true,"path":"2017/09/23/project-rules-2/","link":"","permalink":"http://lexburner.github.io/2017/09/23/project-rules-2/","excerpt":"","text":"就在最近，项目组开始强调开发规范了，今天分享一个变量名命名不规范的小案例，强调一下规范的重要性。例子虽小，但却比较有启发意义。 Boolean变量名命名规范16年底，阿里公开了《Java开发规范手册》，其中有一条便是“布尔类型不能以is为前缀”。规范中没有举出例子，但是给出了原因：会导致部分序列化框架的无法解析。 看看错误的示范，会导致什么问题，以Spring中的jdbcTemplate来进行实验。 定义实体类1234567891011121314151617181920@Entitypublic class Bar &#123; @Id @GeneratedValue private Integer id; private Boolean isSuccess;// 注意这是错误的命名 private boolean isSend;// 注意这是错误的命名 public Boolean getSuccess() &#123; return isSuccess; &#125; public void setSuccess(Boolean success) &#123; isSuccess = success; &#125; public boolean isSend() &#123; return isSend; &#125; public void setSend(boolean send) &#123; isSend = send; &#125;&#125; 其中，isSuccess使用的是包装类型Boolean，而isSend使用的是原生类型boolean，而getter，setter方法是使用Intellij IDEA自动生成的，布尔类型生成getter，setter方法时略微特殊，比如原生类型的getter方式是以is开头的，他们略微有点区别，注意区分。生成getter，setter方法之后，其实已经有点奇怪了，不急，继续下面的实验。 在数据库中，isSuccess被映射了is_success，isSend被映射成了is_send，这符合我们的预期。并且为了后续的实验，我们事先准备一条记录，用于后续的查询，在mysql的方言中，布尔类型被默认自动映射成byte，1代表ture，0代表false。 id is_success is_send 1 1 1 使用JdbcTemplate查询12345public void test(String id) &#123; RowMapper&lt;Bar&gt; barRowMapper = new BeanPropertyRowMapper&lt;Bar&gt;(Bar.class); Bar bar = jdbcTemplate.queryForObject(\"select * from bar where id = ?\", new Object[]&#123;id&#125;, barRowMapper); System.out.println(bar);&#125; JdbcTemplate提供了BeanPropertyRowMapper完成数据库到实体类的映射，事先我重写了Bar类的toString方法，调用test(1)看看是否能成功映射。结果如下： 1Bar&#123;id=1, isSuccess=null, isSend=false&#125; 数据库中是实际存在这样的字段，并且值都是true，而使用JdbcTemplate，却查询不到这样的问题，这边是不遵循规范导致的问题。 相信这个例子可以让大家更加加深映像，特别是在维护老旧代码时，如果发现有is开头的boolean值，需要额外地注意。 包装类型与原生类型在回顾一下上述的demo，原生类型和包装类型都没有封装成功，isSuccess得到了一个null值，isSend得到了一个false值。后者足够引起我们的警惕，如果说前者会引起一个NullPointerExcepiton导致程序异常，还可以引起开发者的注意，而后者很有可能一直作为一个隐藏的bug，不被人所察觉，因为boolean的默认值为false。 在类变量中，也普遍提倡使用包装类型，而原生类型的不足之处是很明显的。以Integer num;字段为例，num=null代表的含义是num字段未被保存，未定义；而num=0代表的含义是明确的，数量为0。原生类型的表达能力有限。所以提倡在局部作用域的计算中使用原生类型，而在类变量中使用包装类型。 JavaBean规范如今的微服务的时代，都是在聊架构，聊容器编排，竟然还有人聊JavaBean，但既然说到了规范，顺带提下。 先来做个选择题，以下选项中符合JavaBean命名规范的有哪些？： 1234A : ebookB : eBookC : EbookD : EBook . . . . 正确答案是：A,D 怎么样，符合你的预想吗？JavaBean规范并不是像很多人想的那样，首字母小写，之后的每一个单词首字母大写这样的驼峰命名法。正确的命名规范应该是：要么前两个字母都是小写，要么前两个字母都是大写。因为英文单词中有URL，USA这样固定形式的大写词汇，所以才有了这样的规范。特别警惕B那种形式，一些诸如sNo，eBook,eMail,cId这样的命名，都是不规范的。 由此引申出了getter，setter命名的规范，除了第一节中Boolean类型的特例之外，网上还有不上文章，强调了这样的概念：eBook对应的getter，setter应当为geteBook(),seteBook()，即当类变量的首字母是小写，而第二个字母是大写时，生成的getter，setter应当是（get/set）+类变量名。但上面已经介绍过了，eBook这样的变量命名本身就是不规范的，在不规范的变量命名下强调规范的getter，setter命名，出发点就错了。有兴趣的朋友可以在eclipse，intellij idea中试试，这几种规范，不规范的变量命名，各自对应的getter，setter方法是如何的。另外需要知晓一点，IDE提供的自动生成getter，setter的机制，以及lombok这类框架的机制，都是由默认的设置，在与其他反射框架配合使用时，只有双方都遵循规范，才能够配合使用，而不能笃信框架。这一点上，有部分国产的框架做的并不是很好。 最后说一个和JavaBean相关的取值规范，在jsp的c标签，freemarker一类的模板语法，以及一些el表达式中，${student.name}并不是取的student的name字段，而是调用了student的getName方法，这也应当被注意，student.name如何找到对应的getter方法，需要解决上一段中提到的同样的问题，建议不确定的地方多测试，尽量采取稳妥的写法。 可能有人会觉得这样的介绍类似于“茴”字有几种写法，但笔者认为恰恰是这些小的规范，容易被人忽视，才更加需要被注意。","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/categories/技术杂谈/"}],"tags":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/tags/技术杂谈/"},{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"}]},{"title":"Spring Security(三)--核心配置解读","slug":"spring-security-3","date":"2017-09-20T15:25:34.000Z","updated":"2017-10-25T10:29:04.322Z","comments":true,"path":"2017/09/20/spring-security-3/","link":"","permalink":"http://lexburner.github.io/2017/09/20/spring-security-3/","excerpt":"","text":"上一篇文章《Spring Security(二)–Guides》，通过Spring Security的配置项了解了Spring Security是如何保护我们的应用的，本篇文章对上一次的配置做一个分析。 [TOC] 3 核心配置解读3.1 功能介绍这是Spring Security入门指南中的配置项： 1234567891011121314151617181920212223242526@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(\"/\", \"/home\").permitAll() .anyRequest().authenticated() .and() .formLogin() .loginPage(\"/login\") .permitAll() .and() .logout() .permitAll(); &#125; @Autowired public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception &#123; auth .inMemoryAuthentication() .withUser(\"admin\").password(\"admin\").roles(\"USER\"); &#125;&#125; 当配置了上述的javaconfig之后，我们的应用便具备了如下的功能： 除了“/”,”/home”(首页),”/login”(登录),”/logout”(注销),之外，其他路径都需要认证。 指定“/login”该路径为登录页面，当未认证的用户尝试访问任何受保护的资源时，都会跳转到“/login”。 默认指定“/logout”为注销页面 配置一个内存中的用户认证器，使用admin/admin作为用户名和密码，具有USER角色 防止CSRF攻击 Session Fixation protection(可以参考我之前讲解Spring Session的文章，防止别人篡改sessionId) Security Header(添加一系列和Header相关的控制) HTTP Strict Transport Security for secure requests 集成X-Content-Type-Options 缓存控制 集成X-XSS-Protection.aspx) X-Frame-Options integration to help prevent Clickjacking(iframe被默认禁止使用) 为Servlet API集成了如下的几个方法 HttpServletRequest#getRemoteUser()) HttpServletRequest.html#getUserPrincipal()) HttpServletRequest.html#isUserInRole(java.lang.String)) HttpServletRequest.html#login(java.lang.String, java.lang.String)) HttpServletRequest.html#logout()) 3.2 @EnableWebSecurity我们自己定义的配置类WebSecurityConfig加上了@EnableWebSecurity注解，同时继承了WebSecurityConfigurerAdapter。你可能会在想谁的作用大一点，毫无疑问@EnableWebSecurity起到决定性的配置作用，它其实是个组合注解。 1234567@Import(&#123; WebSecurityConfiguration.class, // &lt;2&gt; SpringWebMvcImportSelector.class &#125;) // &lt;1&gt;@EnableGlobalAuthentication // &lt;3&gt;@Configurationpublic @interface EnableWebSecurity &#123; boolean debug() default false;&#125; @Import是springboot提供的用于引入外部的配置的注解，可以理解为：@EnableWebSecurity注解激活了@Import注解中包含的配置类。 SpringWebMvcImportSelector的作用是判断当前的环境是否包含springmvc，因为spring security可以在非spring环境下使用，为了避免DispatcherServlet的重复配置，所以使用了这个注解来区分。 WebSecurityConfiguration顾名思义，是用来配置web安全的，下面的小节会详细介绍。 @EnableGlobalAuthentication注解的源码如下： 1234@Import(AuthenticationConfiguration.class)@Configurationpublic @interface EnableGlobalAuthentication &#123;&#125; 注意点同样在@Import之中，它实际上激活了AuthenticationConfiguration这样的一个配置类，用来配置认证相关的核心类。 也就是说：@EnableWebSecurity完成的工作便是加载了WebSecurityConfiguration，AuthenticationConfiguration这两个核心配置类，也就此将spring security的职责划分为了配置安全信息，配置认证信息两部分。 WebSecurityConfiguration在这个配置类中，有一个非常重要的Bean被注册了。 12345678910@Configurationpublic class WebSecurityConfiguration &#123; //DEFAULT_FILTER_NAME = \"springSecurityFilterChain\" @Bean(name = AbstractSecurityWebApplicationInitializer.DEFAULT_FILTER_NAME) public Filter springSecurityFilterChain() throws Exception &#123; ... &#125; &#125; 在未使用springboot之前，大多数人都应该对“springSecurityFilterChain”这个名词不会陌生，他是spring security的核心过滤器，是整个认证的入口。在曾经的XML配置中，想要启用spring security，需要在web.xml中进行如下配置： 12345678910&lt;!-- Spring Security --&gt; &lt;filter&gt; &lt;filter-name&gt;springSecurityFilterChain&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;springSecurityFilterChain&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 而在springboot集成之后，这样的XML被java配置取代。WebSecurityConfiguration中完成了声明springSecurityFilterChain的作用，并且最终交给DelegatingFilterProxy这个代理类，负责拦截请求（注意DelegatingFilterProxy这个类不是spring security包中的，而是存在于web包中，spring使用了代理模式来实现安全过滤的解耦）。 AuthenticationConfiguration123456789101112131415@Configuration@Import(ObjectPostProcessorConfiguration.class)public class AuthenticationConfiguration &#123; @Bean public AuthenticationManagerBuilder authenticationManagerBuilder( ObjectPostProcessor&lt;Object&gt; objectPostProcessor) &#123; return new AuthenticationManagerBuilder(objectPostProcessor); &#125; public AuthenticationManager getAuthenticationManager() throws Exception &#123; ... &#125;&#125; AuthenticationConfiguration的主要任务，便是负责生成全局的身份认证管理者AuthenticationManager。还记得在《Spring Security(一)–Architecture Overview》中，介绍了Spring Security的认证体系，AuthenticationManager便是最核心的身份认证管理器。 3.3 WebSecurityConfigurerAdapter适配器模式在spring中被广泛的使用，在配置中使用Adapter的好处便是，我们可以选择性的配置想要修改的那一部分配置，而不用覆盖其他不相关的配置。WebSecurityConfigurerAdapter中我们可以选择自己想要修改的内容，来进行重写，而其提供了三个configure重载方法，是我们主要关心的： 由参数就可以知道，分别是对AuthenticationManagerBuilder，WebSecurity，HttpSecurity进行个性化的配置。 HttpSecurity常用配置1234567891011121314151617181920212223242526272829@Configuration@EnableWebSecuritypublic class CustomWebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(\"/resources/**\", \"/signup\", \"/about\").permitAll() .antMatchers(\"/admin/**\").hasRole(\"ADMIN\") .antMatchers(\"/db/**\").access(\"hasRole('ADMIN') and hasRole('DBA')\") .anyRequest().authenticated() .and() .formLogin() .usernameParameter(\"username\") .passwordParameter(\"password\") .failureForwardUrl(\"/login?error\") .loginPage(\"/login\") .permitAll() .and() .logout() .logoutUrl(\"/logout\") .logoutSuccessUrl(\"/index\") .permitAll() .and() .httpBasic() .disable(); &#125;&#125; 上述是一个使用Java Configuration配置HttpSecurity的典型配置，其中http作为根开始配置，每一个and()对应了一个模块的配置（等同于xml配置中的结束标签），并且and()返回了HttpSecurity本身，于是可以连续进行配置。他们配置的含义也非常容易通过变量本身来推测， authorizeRequests()配置路径拦截，表明路径访问所对应的权限，角色，认证信息。 formLogin()对应表单认证相关的配置 logout()对应了注销相关的配置 httpBasic()可以配置basic登录 etc 他们分别代表了http请求相关的安全配置，这些配置项无一例外的返回了Configurer类，而所有的http相关配置可以通过查看HttpSecurity的主要方法得知： 需要对http协议有一定的了解才能完全掌握所有的配置，不过，springboot和spring security的自动配置已经足够使用了。其中每一项Configurer（e.g.FormLoginConfigurer,CsrfConfigurer）都是HttpConfigurer的细化配置项。 WebSecurityBuilder12345678910@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override public void configure(WebSecurity web) throws Exception &#123; web .ignoring() .antMatchers(\"/resources/**\"); &#125;&#125; 以笔者的经验，这个配置中并不会出现太多的配置信息。 AuthenticationManagerBuilder1234567891011@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth .inMemoryAuthentication() .withUser(\"admin\").password(\"admin\").roles(\"USER\"); &#125;&#125; 想要在WebSecurityConfigurerAdapter中进行认证相关的配置，可以使用configure(AuthenticationManagerBuilder auth)暴露一个AuthenticationManager的建造器：AuthenticationManagerBuilder 。如上所示，我们便完成了内存中用户的配置。 细心的朋友会发现，在前面的文章中我们配置内存中的用户时，似乎不是这么配置的，而是： 1234567891011@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Autowired public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception &#123; auth .inMemoryAuthentication() .withUser(\"admin\").password(\"admin\").roles(\"USER\"); &#125;&#125; 如果你的应用只有唯一一个WebSecurityConfigurerAdapter，那么他们之间的差距可以被忽略，从方法名可以看出两者的区别：使用@Autowired注入的AuthenticationManagerBuilder是全局的身份认证器，作用域可以跨越多个WebSecurityConfigurerAdapter，以及影响到基于Method的安全控制；而 protected configure()的方式则类似于一个匿名内部类，它的作用域局限于一个WebSecurityConfigurerAdapter内部。关于这一点的区别，可以参考我曾经提出的issuespring-security#issues4571。官方文档中，也给出了配置多个WebSecurityConfigurerAdapter的场景以及demo，将在该系列的后续文章中解读。","categories":[{"name":"Spring Security","slug":"Spring-Security","permalink":"http://lexburner.github.io/categories/Spring-Security/"}],"tags":[{"name":"Spring Security","slug":"Spring-Security","permalink":"http://lexburner.github.io/tags/Spring-Security/"}]},{"title":"Spring Security(二)--Guides","slug":"spring-security-2","date":"2017-09-20T15:25:34.000Z","updated":"2017-09-21T08:08:00.976Z","comments":true,"path":"2017/09/20/spring-security-2/","link":"","permalink":"http://lexburner.github.io/2017/09/20/spring-security-2/","excerpt":"上一篇文章《Spring Security(一)–Architecture Overview》，我们介绍了Spring Security的基础架构，这一节我们通过Spring官方给出的一个guides例子，来了解Spring Security是如何保护我们的应用的，之后会对进行一个解读。 [TOC] 2 Spring Security Guides2.1 引入依赖1234567891011121314&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 由于我们集成了springboot，所以不需要显示的引入Spring Security文档中描述core，config依赖，只需要引入spring-boot-starter-security即可。","text":"上一篇文章《Spring Security(一)–Architecture Overview》，我们介绍了Spring Security的基础架构，这一节我们通过Spring官方给出的一个guides例子，来了解Spring Security是如何保护我们的应用的，之后会对进行一个解读。 [TOC] 2 Spring Security Guides2.1 引入依赖1234567891011121314&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 由于我们集成了springboot，所以不需要显示的引入Spring Security文档中描述core，config依赖，只需要引入spring-boot-starter-security即可。 2.2 创建一个不受安全限制的web应用这是一个首页，不受安全限制 src/main/resources/templates/home.html 1234567891011&lt;!DOCTYPE html&gt;&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:th=\"http://www.thymeleaf.org\" xmlns:sec=\"http://www.thymeleaf.org/thymeleaf-extras-springsecurity3\"&gt; &lt;head&gt; &lt;title&gt;Spring Security Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Welcome!&lt;/h1&gt; &lt;p&gt;Click &lt;a th:href=\"@&#123;/hello&#125;\"&gt;here&lt;/a&gt; to see a greeting.&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 这个简单的页面上包含了一个链接，跳转到”/hello”。对应如下的页面 src/main/resources/templates/hello.html 12345678910&lt;!DOCTYPE html&gt;&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:th=\"http://www.thymeleaf.org\" xmlns:sec=\"http://www.thymeleaf.org/thymeleaf-extras-springsecurity3\"&gt; &lt;head&gt; &lt;title&gt;Hello World!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello world!&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 接下来配置Spring MVC，使得我们能够访问到页面。 123456789101112@Configurationpublic class MvcConfig extends WebMvcConfigurerAdapter &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController(\"/home\").setViewName(\"home\"); registry.addViewController(\"/\").setViewName(\"home\"); registry.addViewController(\"/hello\").setViewName(\"hello\"); registry.addViewController(\"/login\").setViewName(\"login\"); &#125;&#125; 2.3 配置Spring Security一个典型的安全配置如下所示： 12345678910111213141516171819202122232425@Configuration@EnableWebSecurity &lt;1&gt;public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; &lt;1&gt; @Override protected void configure(HttpSecurity http) throws Exception &#123; http &lt;2&gt; .authorizeRequests() .antMatchers(\"/\", \"/home\").permitAll() .anyRequest().authenticated() .and() .formLogin() .loginPage(\"/login\") .permitAll() .and() .logout() .permitAll(); &#125; @Autowired public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception &#123; auth &lt;3&gt; .inMemoryAuthentication() .withUser(\"admin\").password(\"admin\").roles(\"USER\"); &#125;&#125; @EnableWebSecurity注解使得SpringMVC集成了Spring Security的web安全支持。另外，WebSecurityConfig配置类同时集成了WebSecurityConfigurerAdapter，重写了其中的特定方法，用于自定义Spring Security配置。整个Spring Security的工作量，其实都是集中在该配置类，不仅仅是这个guides，实际项目中也是如此。 configure(HttpSecurity)定义了哪些URL路径应该被拦截，如字面意思所描述：”/“, “/home”允许所有人访问，”/login”作为登录入口，也被允许访问，而剩下的”/hello”则需要登陆后才可以访问。 configureGlobal(AuthenticationManagerBuilder)在内存中配置一个用户，admin/admin分别是用户名和密码，这个用户拥有USER角色。 我们目前还没有登录页面，下面创建登录页面： 1234567891011121314151617181920&lt;!DOCTYPE html&gt;&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:th=\"http://www.thymeleaf.org\" xmlns:sec=\"http://www.thymeleaf.org/thymeleaf-extras-springsecurity3\"&gt; &lt;head&gt; &lt;title&gt;Spring Security Example &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div th:if=\"$&#123;param.error&#125;\"&gt; Invalid username and password. &lt;/div&gt; &lt;div th:if=\"$&#123;param.logout&#125;\"&gt; You have been logged out. &lt;/div&gt; &lt;form th:action=\"@&#123;/login&#125;\" method=\"post\"&gt; &lt;div&gt;&lt;label&gt; User Name : &lt;input type=\"text\" name=\"username\"/&gt; &lt;/label&gt;&lt;/div&gt; &lt;div&gt;&lt;label&gt; Password: &lt;input type=\"password\" name=\"password\"/&gt; &lt;/label&gt;&lt;/div&gt; &lt;div&gt;&lt;input type=\"submit\" value=\"Sign In\"/&gt;&lt;/div&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 这个Thymeleaf模板提供了一个用于提交用户名和密码的表单,其中name=”username”，name=”password”是默认的表单值，并发送到“/ login”。 在默认配置中，Spring Security提供了一个拦截该请求并验证用户的过滤器。 如果验证失败，该页面将重定向到“/ login?error”，并显示相应的错误消息。 当用户选择注销，请求会被发送到“/ login?logout”。 最后，我们为hello.html添加一些内容，用于展示用户信息。 12345678910111213&lt;!DOCTYPE html&gt;&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:th=\"http://www.thymeleaf.org\" xmlns:sec=\"http://www.thymeleaf.org/thymeleaf-extras-springsecurity3\"&gt; &lt;head&gt; &lt;title&gt;Hello World!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1 th:inline=\"text\"&gt;Hello [[$&#123;#httpServletRequest.remoteUser&#125;]]!&lt;/h1&gt; &lt;form th:action=\"@&#123;/logout&#125;\" method=\"post\"&gt; &lt;input type=\"submit\" value=\"Sign Out\"/&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 我们使用Spring Security之后，HttpServletRequest#getRemoteUser()可以用来获取用户名。 登出请求将被发送到“/ logout”。 成功注销后，会将用户重定向到“/ login?logout”。 2.4 添加启动类12345678@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) throws Throwable &#123; SpringApplication.run(Application.class, args); &#125;&#125; 2.5 测试访问首页http://localhost:8080/: 点击here，尝试访问受限的页面：/hello,由于未登录，结果被强制跳转到登录也/login： 输入正确的用户名和密码之后，跳转到之前想要访问的/hello: 点击Sign out退出按钮，访问:/logout,回到登录页面: 2.6 总结本篇文章没有什么干货，基本算是翻译了Spring Security Guides的内容，稍微了解Spring Security的朋友都不会对这个翻译感到陌生。考虑到受众的问题，一个入门的例子是必须得有的，方便后续对Spring Security的自定义配置进行讲解。下一节，以此guides为例，讲解这些最简化的配置背后，Spring Security都帮我们做了什么工作。 本节所有的代码，可以直接在Spring的官方仓库下载得到，git clone https://github.com/spring-guides/gs-securing-web.git。不过，建议初学者根据文章先一步步配置，出了问题，再与demo进行对比。","categories":[{"name":"Spring Security","slug":"Spring-Security","permalink":"http://lexburner.github.io/categories/Spring-Security/"}],"tags":[{"name":"Spring Security","slug":"Spring-Security","permalink":"http://lexburner.github.io/tags/Spring-Security/"}]},{"title":"Spring Security(一)--Architecture Overview","slug":"spring-security-1","date":"2017-09-19T12:12:55.000Z","updated":"2017-09-21T02:34:51.267Z","comments":true,"path":"2017/09/19/spring-security-1/","link":"","permalink":"http://lexburner.github.io/2017/09/19/spring-security-1/","excerpt":"一直以来我都想写一写Spring Security系列的文章，但是整个Spring Security体系强大却又繁杂。陆陆续续从最开始的guides接触它，到项目中看了一些源码，到最近这个月为了写一写这个系列的文章，阅读了好几遍文档，最终打算尝试一下，写一个较为完整的系列文章。 较为简单或者体量较小的技术，完全可以参考着demo直接上手，但系统的学习一门技术则不然。以我的认知，一般的文档大致有两种风格：Architecture First和Code First。前者致力于让读者先了解整体的架构，方便我们对自己的认知有一个宏观的把控，而后者以特定的demo配合讲解，可以让读者在解决问题的过程中顺便掌握一门技术。关注过我博客或者公众号的朋友会发现，我之前介绍技术的文章，大多数是Code First，提出一个需求，介绍一个思路，解决一个问题，分析一下源码，大多如此。而学习一个体系的技术，我推荐Architecture First，正如本文标题所言，这篇文章是我Spring Security系列的第一篇，主要是根据Spring Security文档选择性翻译整理而成的一个架构概览，配合自己的一些注释方便大家理解。写作本系列文章时，参考版本为Spring Security 4.2.3.RELEASE。 [TOC] 1 核心组件这一节主要介绍一些在Spring Security中常见且核心的Java类，它们之间的依赖，构建起了整个框架。想要理解整个架构，最起码得对这些类眼熟。 1.1 SecurityContextHolderSecurityContextHolder用于存储安全上下文（security context）的信息。当前操作的用户是谁，该用户是否已经被认证，他拥有哪些角色权限…这些都被保存在SecurityContextHolder中。SecurityContextHolder默认使用ThreadLocal 策略来存储认证信息。看到ThreadLocal 也就意味着，这是一种与线程绑定的策略。Spring Security在用户登录时自动绑定认证信息到当前线程，在用户退出时，自动清除当前线程的认证信息。但这一切的前提，是你在web场景下使用Spring Security，而如果是Swing界面，Spring也提供了支持，SecurityContextHolder的策略则需要被替换，鉴于我的初衷是基于web来介绍Spring Security，所以这里以及后续，非web的相关的内容都一笔带过。 获取当前用户的信息因为身份信息是与线程绑定的，所以可以在程序的任何地方使用静态方法获取用户信息。一个典型的获取当前登录用户的姓名的例子如下所示： 1234567Object principal = SecurityContextHolder.getContext().getAuthentication().getPrincipal();if (principal instanceof UserDetails) &#123;String username = ((UserDetails)principal).getUsername();&#125; else &#123;String username = principal.toString();&#125; getAuthentication()返回了认证信息，再次getPrincipal()返回了身份信息，UserDetails便是Spring对身份信息封装的一个接口。Authentication和UserDetails的介绍在下面的小节具体讲解，本节重要的内容是介绍SecurityContextHolder这个容器。","text":"一直以来我都想写一写Spring Security系列的文章，但是整个Spring Security体系强大却又繁杂。陆陆续续从最开始的guides接触它，到项目中看了一些源码，到最近这个月为了写一写这个系列的文章，阅读了好几遍文档，最终打算尝试一下，写一个较为完整的系列文章。 较为简单或者体量较小的技术，完全可以参考着demo直接上手，但系统的学习一门技术则不然。以我的认知，一般的文档大致有两种风格：Architecture First和Code First。前者致力于让读者先了解整体的架构，方便我们对自己的认知有一个宏观的把控，而后者以特定的demo配合讲解，可以让读者在解决问题的过程中顺便掌握一门技术。关注过我博客或者公众号的朋友会发现，我之前介绍技术的文章，大多数是Code First，提出一个需求，介绍一个思路，解决一个问题，分析一下源码，大多如此。而学习一个体系的技术，我推荐Architecture First，正如本文标题所言，这篇文章是我Spring Security系列的第一篇，主要是根据Spring Security文档选择性翻译整理而成的一个架构概览，配合自己的一些注释方便大家理解。写作本系列文章时，参考版本为Spring Security 4.2.3.RELEASE。 [TOC] 1 核心组件这一节主要介绍一些在Spring Security中常见且核心的Java类，它们之间的依赖，构建起了整个框架。想要理解整个架构，最起码得对这些类眼熟。 1.1 SecurityContextHolderSecurityContextHolder用于存储安全上下文（security context）的信息。当前操作的用户是谁，该用户是否已经被认证，他拥有哪些角色权限…这些都被保存在SecurityContextHolder中。SecurityContextHolder默认使用ThreadLocal 策略来存储认证信息。看到ThreadLocal 也就意味着，这是一种与线程绑定的策略。Spring Security在用户登录时自动绑定认证信息到当前线程，在用户退出时，自动清除当前线程的认证信息。但这一切的前提，是你在web场景下使用Spring Security，而如果是Swing界面，Spring也提供了支持，SecurityContextHolder的策略则需要被替换，鉴于我的初衷是基于web来介绍Spring Security，所以这里以及后续，非web的相关的内容都一笔带过。 获取当前用户的信息因为身份信息是与线程绑定的，所以可以在程序的任何地方使用静态方法获取用户信息。一个典型的获取当前登录用户的姓名的例子如下所示： 1234567Object principal = SecurityContextHolder.getContext().getAuthentication().getPrincipal();if (principal instanceof UserDetails) &#123;String username = ((UserDetails)principal).getUsername();&#125; else &#123;String username = principal.toString();&#125; getAuthentication()返回了认证信息，再次getPrincipal()返回了身份信息，UserDetails便是Spring对身份信息封装的一个接口。Authentication和UserDetails的介绍在下面的小节具体讲解，本节重要的内容是介绍SecurityContextHolder这个容器。 1.2 Authentication先看看这个接口的源码长什么样： 123456789101112131415package org.springframework.security.core;// &lt;1&gt;public interface Authentication extends Principal, Serializable &#123; // &lt;1&gt; Collection&lt;? extends GrantedAuthority&gt; getAuthorities(); // &lt;2&gt; Object getCredentials();// &lt;2&gt; Object getDetails();// &lt;2&gt; Object getPrincipal();// &lt;2&gt; boolean isAuthenticated();// &lt;2&gt; void setAuthenticated(boolean var1) throws IllegalArgumentException;&#125; Authentication是spring security包中的接口，直接继承自Principal类，而Principal是位于java.security包中的。可以见得，Authentication在spring security中是最高级别的身份/认证的抽象。 由这个顶级接口，我们可以得到用户拥有的权限信息列表，密码，用户细节信息，用户身份信息，认证信息。 还记得1.1节中，authentication.getPrincipal()返回了一个Object，我们将Principal强转成了Spring Security中最常用的UserDetails，这在Spring Security中非常常见，接口返回Object，使用instanceof判断类型，强转成对应的具体实现类。接口详细解读如下： getAuthorities()，权限信息列表，默认是GrantedAuthority接口的一些实现类，通常是代表权限信息的一系列字符串。 getCredentials()，密码信息，用户输入的密码字符串，在认证过后通常会被移除，用于保障安全。 getDetails()，细节信息，web应用中的实现接口通常为 WebAuthenticationDetails，它记录了访问者的ip地址和sessionId的值。 getPrincipal()，敲黑板！！！最重要的身份信息，大部分情况下返回的是UserDetails接口的实现类，也是框架中的常用接口之一。UserDetails接口将会在下面的小节重点介绍。 Spring Security是如何完成身份认证的？1 用户名和密码被过滤器获取到，封装成Authentication,通常情况下是UsernamePasswordAuthenticationToken这个实现类。 2 AuthenticationManager 身份管理器负责验证这个Authentication 3 认证成功后，AuthenticationManager身份管理器返回一个被填充满了信息的（包括上面提到的权限信息，身份信息，细节信息，但密码通常会被移除）Authentication实例。 4 SecurityContextHolder安全上下文容器将第3步填充了信息的Authentication，通过SecurityContextHolder.getContext().setAuthentication(…)方法，设置到其中。 这是一个抽象的认证流程，而整个过程中，如果不纠结于细节，其实只剩下一个AuthenticationManager 是我们没有接触过的了，这个身份管理器我们在后面的小节介绍。将上述的流程转换成代码，便是如下的流程： 12345678910111213141516171819202122232425262728293031323334353637383940public class AuthenticationExample &#123;private static AuthenticationManager am = new SampleAuthenticationManager();public static void main(String[] args) throws Exception &#123; BufferedReader in = new BufferedReader(new InputStreamReader(System.in)); while(true) &#123; System.out.println(\"Please enter your username:\"); String name = in.readLine(); System.out.println(\"Please enter your password:\"); String password = in.readLine(); try &#123; Authentication request = new UsernamePasswordAuthenticationToken(name, password); Authentication result = am.authenticate(request); SecurityContextHolder.getContext().setAuthentication(result); break; &#125; catch(AuthenticationException e) &#123; System.out.println(\"Authentication failed: \" + e.getMessage()); &#125; &#125; System.out.println(\"Successfully authenticated. Security context contains: \" + SecurityContextHolder.getContext().getAuthentication());&#125;&#125;class SampleAuthenticationManager implements AuthenticationManager &#123;static final List&lt;GrantedAuthority&gt; AUTHORITIES = new ArrayList&lt;GrantedAuthority&gt;();static &#123; AUTHORITIES.add(new SimpleGrantedAuthority(\"ROLE_USER\"));&#125;public Authentication authenticate(Authentication auth) throws AuthenticationException &#123; if (auth.getName().equals(auth.getCredentials())) &#123; return new UsernamePasswordAuthenticationToken(auth.getName(), auth.getCredentials(), AUTHORITIES); &#125; throw new BadCredentialsException(\"Bad Credentials\");&#125;&#125; 注意：上述这段代码只是为了让大家了解Spring Security的工作流程而写的，不是什么源码。在实际使用中，整个流程会变得更加的复杂，但是基本思想，和上述代码如出一辙。 1.3 AuthenticationManager初次接触Spring Security的朋友相信会被AuthenticationManager，ProviderManager ，AuthenticationProvider …这么多相似的Spring认证类搞得晕头转向，但只要稍微梳理一下就可以理解清楚它们的联系和设计者的用意。AuthenticationManager（接口）是认证相关的核心接口，也是发起认证的出发点，因为在实际需求中，我们可能会允许用户使用用户名+密码登录，同时允许用户使用邮箱+密码，手机号码+密码登录，甚至，可能允许用户使用指纹登录（还有这样的操作？没想到吧），所以说AuthenticationManager一般不直接认证，AuthenticationManager接口的常用实现类ProviderManager 内部会维护一个List&lt;AuthenticationProvider&gt;列表，存放多种认证方式，实际上这是委托者模式的应用（Delegate）。也就是说，核心的认证入口始终只有一个：AuthenticationManager，不同的认证方式：用户名+密码（UsernamePasswordAuthenticationToken），邮箱+密码，手机号码+密码登录则对应了三个AuthenticationProvider。这样一来四不四就好理解多了？熟悉shiro的朋友可以把AuthenticationProvider理解成Realm。在默认策略下，只需要通过一个AuthenticationProvider的认证，即可被认为是登录成功。 只保留了关键认证部分的ProviderManager源码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class ProviderManager implements AuthenticationManager, MessageSourceAware, InitializingBean &#123; // 维护一个AuthenticationProvider列表 private List&lt;AuthenticationProvider&gt; providers = Collections.emptyList(); public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; Class&lt;? extends Authentication&gt; toTest = authentication.getClass(); AuthenticationException lastException = null; Authentication result = null; // 依次认证 for (AuthenticationProvider provider : getProviders()) &#123; if (!provider.supports(toTest)) &#123; continue; &#125; try &#123; result = provider.authenticate(authentication); if (result != null) &#123; copyDetails(authentication, result); break; &#125; &#125; ... catch (AuthenticationException e) &#123; lastException = e; &#125; &#125; // 如果有Authentication信息，则直接返回 if (result != null) &#123; if (eraseCredentialsAfterAuthentication &amp;&amp; (result instanceof CredentialsContainer)) &#123; //移除密码 ((CredentialsContainer) result).eraseCredentials(); &#125; //发布登录成功事件 eventPublisher.publishAuthenticationSuccess(result); return result; &#125; ... //执行到此，说明没有认证成功，包装异常信息 if (lastException == null) &#123; lastException = new ProviderNotFoundException(messages.getMessage( \"ProviderManager.providerNotFound\", new Object[] &#123; toTest.getName() &#125;, \"No AuthenticationProvider found for &#123;0&#125;\")); &#125; prepareException(lastException, authentication); throw lastException; &#125;&#125; ProviderManager 中的List，会依照次序去认证，认证成功则立即返回，若认证失败则返回null，下一个AuthenticationProvider会继续尝试认证，如果所有认证器都无法认证成功，则ProviderManager 会抛出一个ProviderNotFoundException异常。 到这里，如果不纠结于AuthenticationProvider的实现细节以及安全相关的过滤器，认证相关的核心类其实都已经介绍完毕了：身份信息的存放容器SecurityContextHolder，身份信息的抽象Authentication，身份认证器AuthenticationManager及其认证流程。姑且在这里做一个分隔线。下面来介绍下AuthenticationProvider接口的具体实现。 1.4 DaoAuthenticationProviderAuthenticationProvider最最最常用的一个实现便是DaoAuthenticationProvider。顾名思义，Dao正是数据访问层的缩写，也暗示了这个身份认证器的实现思路。由于本文是一个Overview，姑且只给出其UML类图： 按照我们最直观的思路，怎么去认证一个用户呢？用户前台提交了用户名和密码，而数据库中保存了用户名和密码，认证便是负责比对同一个用户名，提交的密码和保存的密码是否相同便是了。在Spring Security中。提交的用户名和密码，被封装成了UsernamePasswordAuthenticationToken，而根据用户名加载用户的任务则是交给了UserDetailsService，在DaoAuthenticationProvider中，对应的方法便是retrieveUser，虽然有两个参数，但是retrieveUser只有第一个参数起主要作用，返回一个UserDetails。还需要完成UsernamePasswordAuthenticationToken和UserDetails密码的比对，这便是交给additionalAuthenticationChecks方法完成的，如果这个void方法没有抛异常，则认为比对成功。比对密码的过程，用到了PasswordEncoder和SaltSource，密码加密和盐的概念相信不用我赘述了，它们为保障安全而设计，都是比较基础的概念。 如果你已经被这些概念搞得晕头转向了，不妨这么理解DaoAuthenticationProvider：它获取用户提交的用户名和密码，比对其正确性，如果正确，返回一个数据库中的用户信息（假设用户信息被保存在数据库中）。 1.5 UserDetails与UserDetailsService上面不断提到了UserDetails这个接口，它代表了最详细的用户信息，这个接口涵盖了一些必要的用户信息字段，具体的实现类对它进行了扩展。 12345678910111213141516public interface UserDetails extends Serializable &#123; Collection&lt;? extends GrantedAuthority&gt; getAuthorities(); String getPassword(); String getUsername(); boolean isAccountNonExpired(); boolean isAccountNonLocked(); boolean isCredentialsNonExpired(); boolean isEnabled();&#125; 它和Authentication接口很类似，比如它们都拥有username，authorities，区分他们也是本文的重点内容之一。Authentication的getCredentials()与UserDetails中的getPassword()需要被区分对待，前者是用户提交的密码凭证，后者是用户正确的密码，认证器其实就是对这两者的比对。Authentication中的getAuthorities()实际是由UserDetails的getAuthorities()传递而形成的。还记得Authentication接口中的getUserDetails()方法吗？其中的UserDetails用户详细信息便是经过了AuthenticationProvider之后被填充的。 123public interface UserDetailsService &#123; UserDetails loadUserByUsername(String username) throws UsernameNotFoundException;&#125; UserDetailsService和AuthenticationProvider两者的职责常常被人们搞混，关于他们的问题在文档的FAQ和issues中屡见不鲜。记住一点即可，敲黑板！！！UserDetailsService只负责从特定的地方（通常是数据库）加载用户信息，仅此而已，记住这一点，可以避免走很多弯路。UserDetailsService常见的实现类有JdbcDaoImpl，InMemoryUserDetailsManager，前者从数据库加载用户，后者从内存中加载用户，也可以自己实现UserDetailsService，通常这更加灵活。 1.6 架构概览图为了更加形象的理解上述我介绍的这些核心类，附上一张按照我的理解，所画出Spring Security的一张非典型的UML图 如果对Spring Security的这些概念感到理解不能，不用担心，因为这是Architecture First导致的必然结果，先过个眼熟。后续的文章会秉持Code First的理念，陆续详细地讲解这些实现类的使用场景，源码分析，以及最基本的：如何配置Spring Security，在后面的文章中可以不时翻看这篇文章，找到具体的类在整个架构中所处的位置，这也是本篇文章的定位。另外，一些Spring Security的过滤器还未囊括在架构概览中，如将表单信息包装成UsernamePasswordAuthenticationToken的过滤器，考虑到这些虽然也是架构的一部分，但是真正重写他们的可能性较小，所以打算放到后面的章节讲解。","categories":[{"name":"Spring Security","slug":"Spring-Security","permalink":"http://lexburner.github.io/categories/Spring-Security/"}],"tags":[{"name":"Spring Security","slug":"Spring-Security","permalink":"http://lexburner.github.io/tags/Spring-Security/"}]},{"title":"浅析分布式下的事件驱动机制（PubSub模式）","slug":"event-2","date":"2017-09-13T14:49:23.000Z","updated":"2017-09-14T01:13:30.473Z","comments":true,"path":"2017/09/13/event-2/","link":"","permalink":"http://lexburner.github.io/2017/09/13/event-2/","excerpt":"上一篇文章《浅析Spring中的事件驱动机制》简单介绍了Spring对事件的支持。Event的整个生命周期，从publisher发出，经过applicationContext容器通知到EventListener，都是发生在单个Spring容器中，而在分布式场景下，有些时候一个事件的产生，可能需要被多个实例响应，本文主要介绍分布式场景下的事件驱动机制，由于使用了Redis，ActiveMQ，也可以换一个名词来理解：分布式下的发布订阅模式。 JMS规范在日常项目开发中，我们或多或少的发现一些包一些类位于java或javax中，他们主要提供抽象类，接口，提供了一种规范，如JPA，JSR，JNDI，JTA，JMS，他们是由java指定的标准规范，一流企业做标准、二流企业做品牌、三流企业做产品，虽然有点调侃的意味，但也可以见得它的重要意义。而JMS就是java在消息服务上指定的标准 The Java Message Service (JMS) API is a messaging standard that allows application components based on the Java Platform Enterprise Edition (Java EE) to create, send, receive, and read messages. It enables distributed communication that is loosely coupled, reliable, and asynchronous. JMS（JAVA Message Service,java消息服务）API是一个消息服务的标准或者说是规范，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。 消息中间件有非常多的实现，如ActiveMQ，RabbitMQ，RocketMQ，而他们同一遵循的接口规范，便是JMS。在下文中即将出现的ConnectionFactory，Destination，Connection，Session，MessageListener，Topic，Queue等等名词，都是JMS核心的接口，由于本文的初衷并不是讲解MQ&amp;JMS，所以这些机制暂且跳过。 定义分布式事件需求在上一个项目中，我们对接了外网的http接口，而安全性的保障则是交给OAuth2来完成，作为OAuth2的客户端，我们需要获取服务端返回的token，而token接口的获取次数每个月是有限制的，于是我们选择使用Redis来保存，定时刷新。由于每次发起请求时都要携带token，为了更高的性能减少一次redis io，我们在TokenService中使用了本地变量缓存token。于是形成如下的token获取机制： 这个图并不复杂，只是为了方便描述需求：首先去本地变量中加载token，若token==null，则去Redis加载，若Redis未命中（token过期了），则最终调用外部的http接口获取实时的token，同时存入redis中和本地变量中。 这个需求设计到这样一个问题：大多数情况下是单个实例中发现redis中的token为空，而它需要同时获取最新token，并通知其他的实例也去加载最新的token，这个时候事件广播就可以派上用场了。 由于token缓存在了Redis中，我们首先介绍Redis的发布订阅机制。","text":"上一篇文章《浅析Spring中的事件驱动机制》简单介绍了Spring对事件的支持。Event的整个生命周期，从publisher发出，经过applicationContext容器通知到EventListener，都是发生在单个Spring容器中，而在分布式场景下，有些时候一个事件的产生，可能需要被多个实例响应，本文主要介绍分布式场景下的事件驱动机制，由于使用了Redis，ActiveMQ，也可以换一个名词来理解：分布式下的发布订阅模式。 JMS规范在日常项目开发中，我们或多或少的发现一些包一些类位于java或javax中，他们主要提供抽象类，接口，提供了一种规范，如JPA，JSR，JNDI，JTA，JMS，他们是由java指定的标准规范，一流企业做标准、二流企业做品牌、三流企业做产品，虽然有点调侃的意味，但也可以见得它的重要意义。而JMS就是java在消息服务上指定的标准 The Java Message Service (JMS) API is a messaging standard that allows application components based on the Java Platform Enterprise Edition (Java EE) to create, send, receive, and read messages. It enables distributed communication that is loosely coupled, reliable, and asynchronous. JMS（JAVA Message Service,java消息服务）API是一个消息服务的标准或者说是规范，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。 消息中间件有非常多的实现，如ActiveMQ，RabbitMQ，RocketMQ，而他们同一遵循的接口规范，便是JMS。在下文中即将出现的ConnectionFactory，Destination，Connection，Session，MessageListener，Topic，Queue等等名词，都是JMS核心的接口，由于本文的初衷并不是讲解MQ&amp;JMS，所以这些机制暂且跳过。 定义分布式事件需求在上一个项目中，我们对接了外网的http接口，而安全性的保障则是交给OAuth2来完成，作为OAuth2的客户端，我们需要获取服务端返回的token，而token接口的获取次数每个月是有限制的，于是我们选择使用Redis来保存，定时刷新。由于每次发起请求时都要携带token，为了更高的性能减少一次redis io，我们在TokenService中使用了本地变量缓存token。于是形成如下的token获取机制： 这个图并不复杂，只是为了方便描述需求：首先去本地变量中加载token，若token==null，则去Redis加载，若Redis未命中（token过期了），则最终调用外部的http接口获取实时的token，同时存入redis中和本地变量中。 这个需求设计到这样一个问题：大多数情况下是单个实例中发现redis中的token为空，而它需要同时获取最新token，并通知其他的实例也去加载最新的token，这个时候事件广播就可以派上用场了。 由于token缓存在了Redis中，我们首先介绍Redis的发布订阅机制。 Redis中的Pub与Subredis不仅仅具备缓存的功能，它还拥有一个channel机制，我们可以使用Redis来进行发布订阅。上述的token流程我们简化一下，省略保存到redis的那一环，直接介绍如何通知其他应用刷新token。 引入依赖和配置1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 12345spring: redis: database: 0 host: localhost port: 6379 定义TokenService1234567891011121314151617181920@Servicepublic class TokenService &#123; @Autowired StringRedisTemplate redisTemplate; public void getToken(String username) &#123; // &lt;1&gt; String token = UUID.randomUUID().toString(); //模拟http接口使用用户名和密码获取token System.out.println(username + \" 成功获取token ...\" + token); //发送token刷新广播 System.out.println(\"广播token刷新事件 ...\"); redisTemplate.convertAndSend(RedisPubSubConfig.tokenChannel, token); &#125; public void refreshTokenListener(String token) &#123; // &lt;2&gt; System.out.println(\"接到token刷新事件，刷新 token : \" + token); &#125;&#125; 模拟获取token的方法，获取token的同时发送广播。 用于接收其他应用发送过来的广播消息。 配置RedisMessageListenerContainer在Spring应用中Event是由Spring容器管理的，而在Redis的消息机制中，Event是由RedisMessageListenerContainer管理的。我们为token配置一个channel，用于刷新token： 123456789101112131415161718192021222324252627@Configurationpublic class RedisPubSubConfig &#123; public final static String tokenChannel = \"tokenChannel\"; @Bean RedisMessageListenerContainer redisMessageListenerContainer(RedisConnectionFactory redisConnectionFactory) &#123; RedisMessageListenerContainer redisMessageListenerContainer = new RedisMessageListenerContainer();// &lt;1&gt; redisMessageListenerContainer.setConnectionFactory(redisConnectionFactory); redisMessageListenerContainer.addMessageListener(tokenRefreshListener(), new ChannelTopic(tokenChannel)); // &lt;2&gt; return redisMessageListenerContainer; &#125; @Autowired TokenService tokenService; MessageListener tokenRefreshListener() &#123; return new MessageListener() &#123; @Override public void onMessage(Message message, byte[] pattern) &#123; byte[] bytes = message.getBody(); // &lt;3&gt; tokenService.refreshTokenListener(new String(bytes)); &#125; &#125;; &#125;&#125; RedisMessageListenerContainer用于管理所有的redis相关的发布与订阅 为Redis容器注册特定的订阅者，在本例中使用tokenRefreshListener监听tokenChannel频道，当收到消息通知时，会自动调用onMessage方法。 使用message.getBody()可以获取消息的具体内容，在本例中即token 测试结果同样的这个应用，我们在8080,8081,8082启动三个，在8080中，我们调用tokenService.getToken(“kirito”);(注意必须要连接到redis的同一个database) 在三个控制台中我们得到了如下的结果： 8080： 123kirito 成功获取token ...5d4d2a48-934f-450d-8806-e6095b172286广播token刷新事件 ...接到token刷新事件，刷新 token : 5d4d2a48-934f-450d-8806-e6095b172286 8081： 1接到token刷新事件，刷新 token : 5d4d2a48-934f-450d-8806-e6095b172286 8082： 1接到token刷新事件，刷新 token : 5d4d2a48-934f-450d-8806-e6095b172286 可以发现其他系统的确收到了通知。 ActiveMQ中的Pub与SubRedis中的发布订阅其实在真正的企业开发中并不是很常用，如果涉及到一致性要求较高的需求，专业的消息中间件可以更好地为我们提供服务。下面介绍一下ActiveMQ如何实现发布订阅。 ActiveMQ为我们提供很好的监控页面，延时队列，消息ACK，事务，持久化等等机制，且拥有较高的吞吐量，是企业架构中不可或缺的一个重要中间件。 引入依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-activemq&lt;/artifactId&gt;&lt;/dependency&gt; 12345678spring: activemq: in-memory: false # &lt;1&gt; broker-url: tcp://127.0.0.1:61616 user: admin password: admin jms: pub-sub-domain: true # &lt;2&gt; springboot的自动配置会帮我们启动一个内存中的消息队列，引入spring-boot-starter-activemq倚赖时需要特别注意这一点，本例连接本机的ActiveMQ。 springboot默认不支持PubSub模式，需要手动开启。 定义TokenService123456789101112131415161718192021222324252627282930@Servicepublic class TokenService &#123; @Autowired JmsTemplate jmsTemplate; // &lt;1&gt; @Autowired Topic tokenTopic; // &lt;3&gt; public void getToken(String username) &#123; String token = UUID.randomUUID().toString(); //模拟http接口使用用户名和密码获取token System.out.println(username + \" 成功获取token ...\" + token); //发送token刷新广播 System.out.println(\"广播token刷新事件 ...\"); try &#123; Message message = new ActiveMQMessage(); message.setStringProperty(\"token\", token); jmsTemplate.convertAndSend(tokenTopic, message);// &lt;1&gt; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; @JmsListener(destination = ActivemqPubSubConfig.tokenTopic) // &lt;2&gt; public void refreshTokenListener(Message message) throws Exception &#123; System.out.println(\"接到token刷新事件，刷新 token : \" + message.getStringProperty(\"token\")); &#125;&#125; 使用模板设计模式的好处体现了出来，再前面的RedisTemplate中我们也是使用同样的template.convertAndSend()发送消息 JmsListener对应于EventListener，接收来自ActiveMQ中tokenTopic的消息通知 tokenTopic定义在下面的config中 配置ActiveMQ的topic123456789101112@Configurationpublic class ActivemqPubSubConfig &#123; public final static String tokenTopic = \"tokenTopic\"; @Bean Topic tokenTopic()&#123; return new ActiveMQTopic(ActivemqPubSubConfig.tokenTopic); &#125;&#125; 非常简单的配置，因为ActiveMQAutoConfiguration已经帮我们做了相当多的配置，我们只需要顶一个topic即可使用ActiveMQ的功能。 查看ActiveMQ的监控端省略了发送消息的过程，实际上可以得到和Redis PubSub一样的效果。来看一下ActiveMQ自带的监控端，在发送消息后，发生了什么变化，访问本地端口http://localhost:8161/admin ，可以看到消息被消费了。 总结本文介绍了Redis，ActiveMQ的PubSub特性，这是我理解的分布式场景下的事件驱动的使用。事件驱动是一种思想，PubSub是一种模式，Redis，ActiveMQ是一种应用，落到实处，便可以是本文介绍的token这个小小的业务实现。但是注意，使用Redis，ActiveMQ理解事件驱动可以，但是不能等同事件驱动，事件驱动还有很多其他场景下体现，笔者功力不够，无法一一介绍，怕人误解，特此强调一下。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"},{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/tags/架构设计/"}]},{"title":"上一个电商项目的反思","slug":"rethink-1","date":"2017-09-11T13:02:43.000Z","updated":"2017-09-14T01:07:57.618Z","comments":true,"path":"2017/09/11/rethink-1/","link":"","permalink":"http://lexburner.github.io/2017/09/11/rethink-1/","excerpt":"","text":"加入中科软已经有了一个年头，从去年实习到今年转正，陆陆续续接触了大概四个项目。有电商类，互联网保险类，也经历过管理系统。幸运的是，这些项目都是从零开始，避免了让我去维护不堪入目的老旧系统。而这么多项目中令我印象最深刻的，就要属上一个电商项目了。这也是我接触到的真正意义的第一个微服务项目，到今天回首去看曾经的这个项目，有很多突破性地尝试，同时不可避免地也踩入了一些坑点，毕竟摸着石头过河。今天想聊聊我对上一个电商项目的反思。 项目简介准确的说是一个第三方的电商项目，商品来源是由主流电商的http接口提供（目前接入了京东，苏宁），打造我们自己的商城体系。使用的技术包括springboot，jpa，rpc框架使用的是motan，数据库使用的是oracle，基本都还算是主流的技术。 盲目地拆分微服务使用了springboot就是微服务了吗？使用rpc通信就是微服务了吗？刚接触到所谓的微服务架构时，无疑是让人兴奋的，但也没有太多的经验，以至于每提出一个新的需求，几乎就会新建一个服务。没有从宏观去思考如何拆分服务，那时还没有项目组成员尝试去使用领域驱动设计的思想去划分服务的边界，会议室中讨论最多的话题也是：我们的数据库该如何设计，而不是我们的领域该如何划分。项目初期，使用单体式的思想开发着分布式项目，新技术的引入还只是使人有点稍微的不顺手，但是项目越做越大后，越来越大的不适感逐渐侵蚀着我们的开发速度。 说道微服务的拆分，有很多个维度，这里主要谈两个维度： 系统维度：业务功能不同的需求，交给不同的系统完成，如订单，商品，地址，用户等系统需要拆分。 模块维度：基础架构层（如公用util），领域层，接口层，服务层，表现层的拆分。 在项目的初期，我们错误地认为微服务的拆分仅仅是系统维度的拆分，如商品系统和订单系统，而在模块维度上，缺少拆分的意识，如订单模块的表现层和服务层，我们虽然做了隔离（两个独立的tomcat）。但在后来，业务添加了一个新的需求：商城增加积分支持，让用户可以使用积分购买商品。我们突然发现，所谓的服务层和表现层严重的耦合，仅仅是在物理上进行了隔离，逻辑层面并没有拆分，这导致新的积分服务模块从原先的订单服务层拷贝了大量的代码。吸取了这个教训后，我们新的项目中采取了如下的分层方式： 其中比较关键的一点便是表现层与应用层的完全分离，交互完全使用DTO对象。不少同事产生了困惑，抱怨在表现层不能访问数据库，这让他们获取数据变得十分“麻烦”，应用层和表现层还多了一次数据拷贝的工作，用于将DO持久化对象转换成DTO对象。但这样的好处从长远来看，是不言而喻的。总结为以下几点： 1 应用层高度重用，没有表现形式的阻碍，PC端，移动端，外部服务都可以同时接入，需要组装什么样的数据，请自行组装。 2 应用层和领域层可以交由经验较为丰富的程序员负责，避免了一些低性能的数据操作，错误的并发控制等等。 3 解决远程调用数据懒加载的问题。从前的设计中，表现层拿到了领域层的对象，而领域层会使用懒加载技术，当表现层想要获取懒加载属性时，或得到一个no session的异常。在没有这个分层之前，如何方便地解决这个问题一度困扰了我们很长的一段时间。 数据库的滥用项目使用了oracle，我们所有的数据都存在于同一个oracle实例中，各个系统模块并没有做到物理层面的数据库隔离。这并不符合设计，一方面这给那些想要跨模块执行join操作的人留了后门，如执行订单模块和用户模块的级联查询；另一方面，还困扰了一部分对微服务架构不甚了解的程序员，在他们的想法中，同一个数据库实例反而方便了他们的数据操作。 严格意义上，不仅仅是不同系统之间的数据库不能互相访问。同一个系统维度的不同模块也应当限制，正如前面一节的分层架构中，表现层（web层）是不应该出现DAO的，pom文件中也不应该出现任何JPA，Hibernate，Mybatis一类的依赖，它所有的数据来源，必须是应用层。 另外一方面，由于历史遗留问题，需要对接一个老系统，他们的表和这个电商的oracle实例是同一个，而我竟然在他们的表上发现了触发器这种操作…在新的项目中，我们已经禁止使用数据库层面的触发器和物理约束。 在新的项目中，我们采用了阿里云的RDS(mysql)作为oracle的替代品，核心业务数据则放到了分布式数据库DRDS中，严格做到了数据库层面的拆分。 并发的控制电商系统不同于OA系统，CMS系统，余额，订单等等操作都是敏感操作，实实在在跟钱打交道的东西容不得半点马虎，然而即使是一些有经验的程序员，也写出了这样的扣减余额操作： 12345678public void reduce(String accountId,BigDecimal cost)&#123; Account account = accountService.findOne(accountId); BigDecimal balance = account.getBalance(); if(balance &gt; cost) balance = balance - cost;//用四则运算代替BigDecimal的api，方便表达 account.setBalance(balance); accountService.save(account);&#125; 很多人没有控制并发的意识，即使意识到了，也不知道如何根据业务场景采取合适的手段控制并发，是使用JPA中的乐观锁，还是使用数据库的行级自旋锁完成简单并发控制，还是for update悲观锁（这不建议被使用），还是基于redis或zookeeper一类的分布式锁？ 这种错误甚至都不容许等到code revivew时才被发现，而应该是尽力地杜绝。 代码规范小到java的变量的驼峰命名法，数据库中用‘_’分割单词，到业务代码该如何规范的书写，再到并发规范，性能调优。准确的说，没有人管理这些事，这样的工作落到了每个有悟性的开发者身上。模块公用的常量，系统公用的常量应当区分放置，禁止使用魔鬼数字，bool变量名不能以is开头等等细小的但是重要的规范，大量的条件查询findByxxx污染了DAO层，完全可以被predicates，criteria替代，RESTFUL规范指导设计web接口等等… 在新的项目中，一条条规范被逐渐添加到了项目单独的模块READ.me中。作为公司的一个junior developer，在建议其他成员使用规范开发项目时，得到的回应通常是：我的功能都已经实现了，干嘛要改；不符合规范又怎么样，要改你改时。有时候也是挺无力的，算是个人的一点牢骚吧。 软件设计的一点不足还是拿订单系统和商品系统来说事，虽然两个系统在物理上被拆分开了，但如果需要展示订单列表，订单详情，如今系统的设计会发起多次的远程调用，用于查询订单的归属商品，这是违背领域驱动设计的。订单中的商品就应当是归属于订单模块，正确的设计应该是使用冗余，代替频繁的跨网络节点远程调用。 另外一点便是高可用，由于机器内存的限制，所有的系统都只部署了单个实例，这其实并不是微服务的最佳实践。从系统应用，到zookeeper，redis，mq等中间件，都应当保证高可用，避免单点问题。没有真正实现做到横向扩展（知识理论上实现了），实在是有点遗憾。 系统没有熔断，降级处理，在新的项目中，由于我们引入了Spring Cloud，很多地方都可以out of box式使用框架提供的fallback处理，而这上一个电商项目由于框架的限制以及接口设计之初就没有预想到要做这样的操作，使得可靠性再减了几分。 自动化运维的缺失单体式应用的美好时代，只需要发布同一份war包。而微服务项目中，一切都变得不同，在我们这个不算特别庞大的电商系统中，需要被运行的服务模块也到达了30-40个。由于这个电商系统是部署在甲方自己的服务器中，一方面是业务部门的业务审批流程，一方面是如此众多的jar包运行，没有自动发布，没有持续集成。令我比较难忘的是初期发布版本，始终有一两个服务莫名奇妙的挂掉，对着终端中的服务列表，一个个排查，这种痛苦的经历。至今，这个系统仍然依靠运维人员，手动管理版本。 上一个项目有一些不可控的项目因素，而新的项目中，系统服务全部在阿里云上部署，也引入了Jenkins，一切都在逐渐变好，其他的devops工具仍然需要完善，以及docker一类的容器技术还未在计划日程之内，这些都是我们今年努力的目标。 总结原本积累了很多自己的想法，可惜落笔之后能够捕捉到一些点，便只汇聚成了上述这些，而这上一个电商项目在逐渐的迭代开发之后也变得越来越好了（我去了新的项目组后，其他同事负责了后续的开发）。这个经历，于我是非常珍贵的，它比那些大牛直接告诉我微服务设计的要素要更加有意义。知道了不足之处，经历了自己解决问题的过程，才会了解到好的方案的优势，了解到开源方案到底是为了解决什么样的问题而设计的。","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/categories/技术杂谈/"}],"tags":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/tags/技术杂谈/"}]},{"title":"浅析Spring中的事件驱动机制","slug":"event-1","date":"2017-09-10T12:03:58.000Z","updated":"2017-09-14T01:07:57.604Z","comments":true,"path":"2017/09/10/event-1/","link":"","permalink":"http://lexburner.github.io/2017/09/10/event-1/","excerpt":"","text":"今天来简单地聊聊事件驱动，其实写这篇文章挺令我挺苦恼的，因为事件驱动这个名词，我没有找到很好的定性解释，担心自己的表述有误，而说到事件驱动可能立刻联想到如此众多的概念：观察者模式，发布订阅模式，消息队列MQ，消息驱动，事件，EventSourcing…为了不产生歧义，笔者把自己所了解的这些模棱两可的概念都列了出来，再开始今天的分享。 在设计模式中，观察者模式可以算得上是一个非常经典的行为型设计模式，猫叫了，主人醒了，老鼠跑了，这一经典的例子，是事件驱动模型在设计层面的体现。 另一模式，发布订阅模式往往被人们等同于观察者模式，但我的理解是两者唯一区别，是发布订阅模式需要有一个调度中心，而观察者模式不需要，例如观察者的列表可以直接由被观察者维护。不过两者即使被混用，互相替代，通常不影响表达。 MQ，中间件级别的消息队列（e.g. ActiveMQ,RabbitMQ），可以认为是发布订阅模式的一个具体体现。事件驱动-&gt;发布订阅-&gt;MQ，从抽象到具体。 java和spring中都拥有Event的抽象，分别代表了语言级别和三方框架级别对事件的支持。 EventSourcing这个概念就要关联到领域驱动设计，DDD对事件驱动也是非常地青睐，领域对象的状态完全是由事件驱动来控制，由其衍生出了CQRS架构，具体实现框架有AxonFramework。 Nginx可以作为高性能的应用服务器（e.g. openResty），以及Nodejs事件驱动的特性，这些也是都是事件驱动的体现。 本文涵盖的内容主要是前面4点。 Spring对Event的支持Spring的文档对Event的支持翻译之后描述如下： ApplicationContext通过ApplicationEvent类和ApplicationListener接口进行事件处理。 如果将实现ApplicationListener接口的bean注入到上下文中，则每次使用ApplicationContext发布ApplicationEvent时，都会通知该bean。 本质上，这是标准的观察者设计模式。 而在spring4.2之后，提供了注解式的支持，我们可以使用任意的java对象配合注解达到同样的效果，首先来看看不适用注解如何在Spring中使用事件驱动机制。 定义业务需求：用户注册后，系统需要给用户发送邮件告知用户注册成功，需要给用户初始化积分；隐含的设计需求，用户注册后，后续需求可能会添加其他操作，如再发送一条短信等等，希望程序具有扩展性，以及符合开闭原则。 如果不使用事件驱动，代码可能会像这样子： 1234567891011121314151617public class UserService &#123; @Autowired EmailService emailService; @Autowired ScoreService scoreService; @Autowired OtherService otherService; public void register(String name) &#123; System.out.println(\"用户：\" + name + \" 已注册！\"); emailService.sendEmail(name); scoreService.initScore(name); otherService.execute(name); &#125; &#125; 要说有什么毛病，其实也不算有，因为可能大多数人在开发中都会这么写，喜欢写同步代码。但这么写，实际上并不是特别的符合隐含的设计需求，假设增加更多的注册项service，我们需要修改register的方法，并且让UserService注入对应的Service。而实际上，register并不关心这些“额外”的操作，如何将这些多余的代码抽取出去呢？便可以使用Spring提供的Event机制。 定义用户注册事件1234567public class UserRegisterEvent extends ApplicationEvent&#123; public UserRegisterEvent(String name) &#123; //name即source super(name); &#125;&#125; ApplicationEvent是由Spring提供的所有Event类的基类，为了简单起见，注册事件只传递了name（可以复杂的对象，但注意要了解清楚序列化机制）。 定义用户注册服务(事件发布者)123456789101112131415@Service // &lt;1&gt;public class UserService implements ApplicationEventPublisherAware &#123; // &lt;2&gt; public void register(String name) &#123; System.out.println(\"用户：\" + name + \" 已注册！\"); applicationEventPublisher.publishEvent(new UserRegisterEvent(name));// &lt;3&gt; &#125; private ApplicationEventPublisher applicationEventPublisher; // &lt;2&gt; @Override public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) &#123; // &lt;2&gt; this.applicationEventPublisher = applicationEventPublisher; &#125;&#125; 服务必须交给Spring容器托管 ApplicationEventPublisherAware是由Spring提供的用于为Service注入ApplicationEventPublisher事件发布器的接口，使用这个接口，我们自己的Service就拥有了发布事件的能力。 用户注册后，不再是显示调用其他的业务Service，而是发布一个用户注册事件。 定义邮件服务，积分服务，其他服务(事件订阅者)12345678@Service // &lt;1&gt;public class EmailService implements ApplicationListener&lt;UserRegisterEvent&gt; &#123; // &lt;2&gt; @Override public void onApplicationEvent(UserRegisterEvent userRegisterEvent) &#123; System.out.println(\"邮件服务接到通知，给 \" + userRegisterEvent.getSource() + \" 发送邮件...\");// &lt;3&gt; &#125;&#125; 事件订阅者的服务同样需要托管于Spring容器 ApplicationListener&lt;E extends ApplicationEvent&gt;接口是由Spring提供的事件订阅者必须实现的接口，我们一般把该Service关心的事件类型作为泛型传入。 处理事件，通过event.getSource()即可拿到事件的具体内容，在本例中便是用户的姓名。 其他两个Service，也同样编写，实际的业务操作仅仅是打印一句内容即可，篇幅限制，这里省略。 编写启动类1234567891011121314151617@SpringBootApplication@RestControllerpublic class EventDemoApp &#123; public static void main(String[] args) &#123; SpringApplication.run(EventDemoApp.class, args); &#125; @Autowired UserService userService; @RequestMapping(\"/register\") public String register()&#123; userService.register(\"kirito\"); return \"success\"; &#125;&#125; 当我们调用userService.register(“kirito”);方法时，控制台打印信息如下： 他们的顺序是无序的，如果需要控制顺序，需要重写order接口，这点不做介绍。其次，我们完成了用户注册和其他服务的解耦，这也是事件驱动的最大特性之一，如果需要在用户注册时完成其他操作，只需要再添加相应的事件订阅者即可。 Spring 对Event的注解支持上述的几个接口已经非常清爽了，如果习惯使用注解，Spring也提供了，不再需要显示实现 注解式的事件发布者123456789101112@Servicepublic class UserService &#123; public void register(String name) &#123; System.out.println(\"用户：\" + name + \" 已注册！\"); applicationEventPublisher.publishEvent(new UserRegisterEvent(name)); &#125; @Autowired private ApplicationEventPublisher applicationEventPublisher;&#125; Spring4.2之后，ApplicationEventPublisher自动被注入到容器中，采用Autowired即可获取。 注解式的事件订阅者12345678@Servicepublic class EmailService &#123; @EventListener public void listenUserRegisterEvent(UserRegisterEvent userRegisterEvent) &#123; System.out.println(\"邮件服务接到通知，给 \" + userRegisterEvent.getSource() + \" 发送邮件...\"); &#125;&#125; @EventListener注解完成了ApplicationListener&lt;E extends ApplicationEvent&gt;接口的使命。 更多的特性可以参考SpringFramework的文档。 Spring中事件的应用在以往阅读Spring源码的经验中，接触了不少使用事件的地方，大概列了以下几个，加深以下印象： Spring Security中使用AuthenticationEventPublisher处理用户认证成功，认证失败的消息处理。 1234567public interface AuthenticationEventPublisher &#123; void publishAuthenticationSuccess(Authentication authentication); void publishAuthenticationFailure(AuthenticationException exception, Authentication authentication);&#125; Hibernate中持久化对象属性的修改是如何被框架得知的？正是采用了一系列持久化相关的事件，如DefaultSaveEventListener，DefaultUpdateEventListener,事件非常多，有兴趣可以去org.hibernate.event包下查看。 Spring Cloud Zuul中刷新路由信息使用到的ZuulRefreshListener 1234567891011121314private static class ZuulRefreshListener implements ApplicationListener&lt;ApplicationEvent&gt; &#123; ... public void onApplicationEvent(ApplicationEvent event) &#123; if(!(event instanceof ContextRefreshedEvent) &amp;&amp; !(event instanceof RefreshScopeRefreshedEvent) &amp;&amp; !(event instanceof RoutesRefreshedEvent)) &#123; if(event instanceof HeartbeatEvent &amp;&amp; this.heartbeatMonitor.update(((HeartbeatEvent)event).getValue())) &#123; this.zuulHandlerMapping.setDirty(true); &#125; &#125; else &#123; this.zuulHandlerMapping.setDirty(true); &#125; &#125; &#125; Spring容器生命周期相关的一些默认Event 1ContextRefreshedEvent,ContextStartedEvent,ContextStoppedEvent,ContextClosedEvent,RequestHandledEvent 。。。其实吧，非常多。。。 总结本文暂时只介绍了Spring中的一些简单的事件驱动机制，相信如果之后再看到Event，Publisher，EventListener一类的单词后缀时，也能立刻和事件机制联系上了。再阅读Spring源码时，如果发现出现了某个Event，但由于不是同步调用，所以很容易被忽视，我一般习惯下意识的去寻找有没有提供默认的Listener，这样不至于漏掉一些“隐藏”的特性。下一篇文章打算聊一聊分布式场景下，事件驱动使用的注意点。 公众号刚刚创立，如果觉得文章不错，希望能分享到您的朋友圈，如果对文章有什么想法和建议，可以与我沟通。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"},{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/tags/架构设计/"}]},{"title":"从Feign使用注意点到RESTFUL接口设计规范","slug":"feign-1","date":"2017-09-09T06:43:28.000Z","updated":"2017-09-25T01:26:34.184Z","comments":true,"path":"2017/09/09/feign-1/","link":"","permalink":"http://lexburner.github.io/2017/09/09/feign-1/","excerpt":"","text":"最近项目中大量使用了Spring Cloud Feign来对接http接口，踩了不少坑，也产生了一些对RESTFUL接口设计的想法，特此一篇记录下。 [TOC] SpringMVC的请求参数绑定机制了解Feign历史的朋友会知道，Feign本身是Netflix的产品，Spring Cloud Feign是在原生Feign的基础上进行了封装，引入了大量的SpringMVC注解支持，这一方面使得其更容易被广大的Spring使用者开箱即用，但也产生了不小的混淆作用。所以在使用Spring Cloud Feign之前，笔者先介绍一下SpringMVC的一个入参机制。预设一个RestController，在本地的8080端口启动一个应用，用于接收http请求。 123456789@RestControllerpublic class BookController &#123; @RequestMapping(value = \"/hello\") // &lt;1&gt; public String hello(String name) &#123; // &lt;2&gt; return \"hello \" + name; &#125;&#125; 这个接口写起来非常简单，但实际springmvc做了非常多的兼容，使得这个接口可以接受多种请求方式。 RequestMapping代表映射的路径，使用GET,POST,PUT,DELETE方式都可以映射到该端点。 SpringMVC中常用的请求参数注解有（@RequestParam,@RequestBody,@PathVariable）等。name被默认当做@RequestParam。形参String name由框架使用字节码技术获取name这个名称，自动检测请求参数中key值为name的参数，也可以使用@RequestParam(“name”)覆盖变量本身的名称。当我们在url中携带name参数或者form表单中携带name参数时，会被获取到。 12345POST /hello HTTP/1.1Host: localhost:8080Content-Type: application/x-www-form-urlencodedname=formParam 或 12GET /hello?name=queryString HTTP/1.1Host: localhost:8080 Feign的请求参数绑定机制上述的SpringMVC参数绑定机制，大家应该都是非常熟悉的，但这一切在Feign中有些许的不同。 我们来看一个非常简单的，但是实际上错误的接口写法： 12345678//注意：错误的接口写法@FeignClient(\"book\")public interface BookApi &#123; @RequestMapping(value = \"/hello\",method = RequestMethod.GET) String hello(String name);&#125; 配置请求地址： 1234567ribbon: eureka: enabled: falsebook: ribbon: listOfServers: http://localhost:8080 我们按照写SpringMVC的RestController的习惯写了一个FeignClient，按照我们的一开始的想法，由于指定了请求方式是GET，那么name应该会作为QueryString拼接到Url中吧？发出一个这样的GET请求： 12GET /hello?name=xxx HTTP/1.1Host: localhost:8080 而实际上，RestController并没有接收到，我们在RestController一侧的应用中获得了一些提示： 并没有按照期望使用GET方式发送请求，而是POST方式 name参数没有被封装，获得了一个null值 查看文档发现，如果不加默认的注解，Feign则会对参数默认加上@RequestBody注解，而RequestBody一定是包含在请求体中的，GET方式无法包含。所以上述两个现象得到了解释。Feign在GET请求包含RequestBody时强制转成了POST请求，而不是报错。 理解清楚了这个机制我们就可以在开发Feign接口避免很多坑。而解决上述这个问题也很简单 在Feign接口中为name添加@RequestParam(“name”)注解，name必须指定，Feign的请求参数不会利用SpringMVC字节码的机制自动给定一个默认的名称。 由于Feign默认使用@RequestBody，也可以改造RestController，使用@RequestBody接收。但是，请求参数通常是多个，推荐使用上述的@RequestParam，而@RequestBody一般只用于传递对象。 Feign绑定复合参数指定请求参数的类型与请求方式，上述问题的出现实际上是由于在没有理清楚Feign内部机制的前提下想当然的和SpringMVC进行了类比。同样，在使用对象作为参数时，也需要注意这样的问题。 对于这样的接口 1234567891011121314151617@FeignClient(\"book\")public interface BookApi &#123; @RequestMapping(value = \"/book\",method = RequestMethod.POST) Book book(@RequestBody Book book); // &lt;1&gt; @RequestMapping(value = \"/book\",method = RequestMethod.POST) Book book(@RequestParam(\"id\") String id,@RequestParam(\"name\") String name); // &lt;2&gt; @RequestMapping(value = \"/book\",method = RequestMethod.POST) Book book(@RequestParam Map map); // &lt;3&gt; //错误的写法 @RequestMapping(value = \"/book\",method = RequestMethod.POST) Book book(@RequestParam Book book); // &lt;4&gt;&#125; 使用@RequestBody传递对象是最常用的方式。 如果参数并不是很多，可以平铺开使用@RequestParam 使用Map，这也是完全可以的，但不太符合面向对象的思想，不能从代码立刻看出该接口需要什么样的参数。 错误的用法，Feign没有提供这样的机制自动转换实体为Map。 Feign中使用@PathVariable与RESTFUL规范这涉及到一个如何设计RESTFUL接口的话题，我们知道在自从RESTFUL在2000年初被提出来之后，就不乏文章提到资源，契约规范，CRUD对应增删改查操作等等。下面笔者从两个实际的接口来聊聊自己的看法。 根据id查找用户接口： 1234567@FeignClient(\"user\")public interface UserApi &#123; @RequestMapping(value = \"/user/&#123;userId&#125;\",method = RequestMethod.GET) String findById(@PathVariable(\"id\") String userId);&#125; 这应该是没有争议的，注意前面强调的，@PathVariable(“id”)括号中的id不可以忘记。那如果是“根据邮箱查找用户呢”?很有可能下意识的写出这样的接口： 1234567@FeignClient(\"user\")public interface UserApi &#123; @RequestMapping(value = \"/user/&#123;email&#125;\",method = RequestMethod.GET) String findByEmail(@PathVariable(\"email\") String email);&#125; 首先看看Feign的问题。email中通常包含’.‘这个特殊字符，如果在路径中包含，会出现意想不到的结果。我不想探讨如何去解决它（实际上可以使用{email:.+}的方式),因为我觉得这不符合设计。 再谈谈规范的问题。这两个接口是否是相似的，email是否应该被放到path中？这就要聊到RESTFUL的初衷，为什么userId这个属性被普遍认为适合出现在RESTFUL路径中，因为id本身起到了资源定位的作用，他是资源的标记。而email不同，它可能是唯一的，但更多的，它是资源的属性，所以，笔者认为不应该在路径中出现非定位性的动态参数。而是把email作为@RequestParam参数。 RESUFTL结构化查询笔者成功的从Feign的话题过度到了RESTFUL接口的设计问题，也导致了本文的篇幅变长了，不过也不打算再开一片文章谈了。 再考虑一个接口设计，查询某一个月某个用户的订单，可能还会携带分页参数，这时候参数变得很多，按照传统的设计，这应该是一个查询操作，也就是与GET请求对应，那是不是意味着应当将这些参数拼接到url后呢？再思考Feign，正如本文的第二段所述，是不支持GET请求携带实体类的，这让我们设计陷入了两难的境地。而实际上参考一些DSL语言的设计如elasticSearch，也是使用POST JSON的方式来进行查询的，所以在实际项目中，笔者并不是特别青睐CRUD与四种请求方式对应的这种所谓的RESTFUL规范，如果说设计RESTFUL应该遵循什么规范，那大概是另一些名词，如契约规范和领域驱动设计。 1234567@FeignClient(\"order\")public interface BookApi &#123; @RequestMapping(value = \"/order/history\",method = RequestMethod.POST) Page&lt;List&lt;Orders&gt;&gt; queryOrderHistory(@RequestBody QueryVO queryVO);&#125; RESTFUL行为限定在实际接口设计中，我遇到了这样的需求，用户模块的接口需要支持修改用户密码，修改用户邮箱，修改用户姓名，而笔者之前阅读过一篇文章，也是讲舍弃CRUD而是用领域驱动设计来规范RESTFUL接口的定义，与项目中我的想法不谋而合。看似这三个属性是同一个实体类的三个属性，完全可以如下设计： 1234567@FeignClient(&quot;user&quot;)public interface UserApi &#123; @RequestMapping(value = &quot;/user&quot;,method = RequestMethod.POST) User update(@RequestBody User user);&#125; 但实际上，如果再考虑多一层，就应该产生这样的思考：这三个功能所需要的权限一致吗？真的应该将他们放到一个接口中吗？实际上，笔者并不希望接口调用方传递一个实体，因为这样的行为是不可控的，完全不知道它到底是修改了什么属性，如果真的要限制行为，还需要在User中添加一个操作类型的字段，然后在接口实现方加以校验，这太麻烦了。而实际上，笔者觉得规范的设计应当如下： 12345678910111213@FeignClient(\"user\")public interface UserApi &#123; @RequestMapping(value = \"/user/&#123;userId&#125;/password/update\",method = RequestMethod.POST) ResultBean&lt;Boolean&gt; updatePassword(@PathVariable(\"userId) String userId,@RequestParam(\"password\") password); @RequestMapping(value = \"/user/&#123;userId&#125;/email/update\",method = RequestMethod.POST) ResultBean&lt;Boolean&gt; updateEmail(@PathVariable(\"userId) String userId,@RequestParam(\"email\") String email); @RequestMapping(value = \"/user/&#123;userId&#125;/username/update\",method = RequestMethod.POST) ResultBean&lt;Boolean&gt; updateUsername(@PathVariable(\"userId) String userId,@RequestParam(\"username\") String username);&#125; 一般意义上RESTFUL接口不应该出现动词，这里的update并不是一个动作，而是标记着操作的类型，因为针对某个属性可能出现的操作类型可能会有很多，所以我习惯加上一个update后缀，明确表达想要进行的操作，而不是仅仅依赖于GET，POST，PUT，DELETE。实际上，修改操作推荐使用的请求方式应当是PUT，这点笔者的理解是，已经使用update标记了行为，实际开发中不习惯使用PUT。 password，email，username都是user的属性，而userId是user的识别符号，所以userId以PathVariable的形式出现在url中，而三个属性出现在ReqeustParam中。 顺带谈谈逻辑删除，如果一个需求是删除用户的常用地址，这个api的操作类型，我通常也不会设计为DELETE请求，而是同样使用delete来标记操作行为 12@RequestMapping(value = \"/user/&#123;userId&#125;/address/&#123;addressId&#125;/delete\",method = RequestMethod.POST) ResultBean&lt;Boolean&gt; updateEmail(@PathVariable(\"userId\") String userId,@PathVariable(\"userId\") String email); 总结本文从Feign的使用注意点，聊到了RESTFUL接口的设计问题，其实是一个互相补充的行为。接口设计需要载体，所以我以Feign的接口风格谈了谈自己对RESTFUL设计的理解，而Feign中一些坑点，也正是我想要规范RESTFUL设计的出发点。如有对RESTFUL设计不同的理解，欢迎与我沟通。","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/tags/Spring-Cloud/"}]},{"title":"Re：从零开始的Spring Session(三)","slug":"Re：从零开始的Spring Session(三)","date":"2017-09-04T12:57:43.000Z","updated":"2017-09-05T01:53:23.568Z","comments":true,"path":"2017/09/04/Re：从零开始的Spring Session(三)/","link":"","permalink":"http://lexburner.github.io/2017/09/04/Re：从零开始的Spring Session(三)/","excerpt":"","text":"上一篇文章中，我们使用Redis集成了Spring Session。大多数的配置都是Spring Boot帮我们自动配置的，这一节我们介绍一点Spring Session较为高级的特性。 集成Spring Security之所以把Spring Session和Spring Security放在一起讨论，是因为我们的应用在集成Spring Security之后，用户相关的认证与Session密不可分，如果不注意一些细节，会引发意想不到的问题。 与Spring Session相关的依赖可以参考上一篇文章，这里给出增量的依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 我们引入依赖后，就已经自动配置了Spring Security，我们在application.yml添加一个内存中的用户： 1234security: user: name: admin password: admin 测试登录点沿用上一篇文章的端点，访问http://localhost:8080/test/cookie?browser=chrome端点后会出现http basic的认证框，我们输入admin/admin，即可获得结果，也遇到了第一个坑点，我们会发现每次请求，sessionId都会被刷新，这显然不是我们想要的结果。 这个现象笔者研究了不少源码，但并没有得到非常满意的解释，只能理解为SecurityAutoConfiguration提供的默认配置，没有触发到响应的配置，导致了session的不断刷新（如果读者有合理的解释可以和我沟通）。Spring Session之所以能够替换默认的tomcat httpSession是因为配置了springSessionRepositoryFilter这个过滤器，且提供了非常高的优先级，这归功于AbstractSecurityWebApplicationInitializer ，AbstractHttpSessionApplicationInitializer 这两个初始化器，当然，也保证了Spring Session会在Spring Security之前起作用。 而解决上述的诡异现象也比较容易（但原理不清），我们使用@EnableWebSecurity对Spring Security进行一些配置，即可解决这个问题。 1234567891011121314151617181920212223242526@EnableWebSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter &#123; // @formatter:off @Override protected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(\"/resources/**\").permitAll() .anyRequest().authenticated() .and() .httpBasic()//&lt;1&gt; .and() .logout().permitAll(); &#125; // @formatter:on // @formatter:off @Autowired public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception &#123; auth .inMemoryAuthentication() .withUser(\"admin\").password(\"admin\").roles(\"USER\");//&lt;2&gt; &#125; // @formatter:on&#125; 不想大费周章写一个登录页面，于是开启了http basic认证 配置了security config之后，springboot的autoConfig就会失效，于是需要手动配置用户。 再次请求，可以发现SessionId返回正常，@EnableWebSecurity似乎触发了相关的配置，当然了，我们在使用Spring Security时不可能使用autoconfig，但是这个现象的确是一个疑点。 使用自定义CookieSerializer12345678@Beanpublic CookieSerializer cookieSerializer() &#123; DefaultCookieSerializer serializer = new DefaultCookieSerializer(); serializer.setCookieName(\"JSESSIONID\"); serializer.setCookiePath(\"/\"); serializer.setDomainNamePattern(\"^.+?\\\\.(\\\\w+\\\\.[a-z]+)$\"); return serializer;&#125; 使用上述配置后，我们可以将Spring Session默认的Cookie Key从SESSION替换为原生的JSESSIONID。而CookiePath设置为根路径且配置了相关的正则表达式，可以达到同父域下的单点登录的效果，在未涉及跨域的单点登录系统中，这是一个非常优雅的解决方案。如果我们的当前域名是moe.cnkirito.moe，该正则会将Cookie设置在父域cnkirito.moe中，如果有另一个相同父域的子域名blog.cnkirito.moe也会识别这个Cookie，便可以很方便的实现同父域下的单点登录。 根据用户名查找用户归属的SESSION这个特性听起来非常有意思，你可以在一些有趣的场景下使用它，如知道用户名后即可删除其SESSION。一直以来我们都是通过线程绑定的方式，让用户操作自己的SESSION，包括获取用户名等操作。但如今它提供了一个反向的操作，根据用户名获取SESSION，恰巧，在一些项目中真的可以使用到这个特性，最起码，当别人问起你，或者讨论到和SESSION相关的知识时，你可以明晰一点，这是可以做到的。 我们使用Redis作为Session Store还有一个好处，就是其实现了FindByIndexNameSessionRepository接口，下面让我们来见证这一点。 123456789101112@Controllerpublic class CookieController &#123; @Autowired FindByIndexNameSessionRepository&lt;? extends ExpiringSession&gt; sessionRepository; @RequestMapping(\"/test/findByUsername\") @ResponseBody public Map findByUsername(@RequestParam String username) &#123; Map&lt;String, ? extends ExpiringSession&gt; usersSessions = sessionRepository.findByIndexNameAndIndexValue(FindByIndexNameSessionRepository.PRINCIPAL_NAME_INDEX_NAME, username); return usersSessions; &#125;&#125; 由于一个用户可能拥有多个Session，所以返回的是一个Map信息，而这里的username，则就是与Spring Security集成之后的用户名，最令人感动Spring厉害的地方，是这一切都是自动配置好的。我们在内存中配置的用户的username是admin，于是我们访问这个端点,可以看到如下的结果 连同我们存入session中的browser=chrome，browser=360都可以看见（只有键名）。 总结Spring Session对各种场景下的Session管理提供一套非常完善的实现。笔者所介绍的，仅仅是Spring Session常用的一些特性，更多的知识点可以在spring.io的文档中一览无余，以及本文中作者存在的一个疑惑，如有兴趣可与我沟通。","categories":[{"name":"Spring Session","slug":"Spring-Session","permalink":"http://lexburner.github.io/categories/Spring-Session/"}],"tags":[{"name":"Spring Session","slug":"Spring-Session","permalink":"http://lexburner.github.io/tags/Spring-Session/"},{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"}]},{"title":"Re：从零开始的Spring Session(二)","slug":"Re：从零开始的Spring Session(二)","date":"2017-09-03T12:06:12.000Z","updated":"2017-09-04T01:37:18.453Z","comments":true,"path":"2017/09/03/Re：从零开始的Spring Session(二)/","link":"","permalink":"http://lexburner.github.io/2017/09/03/Re：从零开始的Spring Session(二)/","excerpt":"上一篇文章介绍了一些Session和Cookie的基础知识，这篇文章开始正式介绍Spring Session是如何对传统的Session进行改造的。官网这么介绍Spring Session： Spring Session provides an API and implementations for managing a user’s session information. It also provides transparent integration with: HttpSession - allows replacing the HttpSession in an application container (i.e. Tomcat) neutral way. Additional features include: Clustered Sessions - Spring Session makes it trivial to support clustered sessions without being tied to an application container specific solution. Multiple Browser Sessions - Spring Session supports managing multiple users’ sessions in a single browser instance (i.e. multiple authenticated accounts similar to Google). RESTful APIs - Spring Session allows providing session ids in headers to work with RESTful APIs WebSocket - provides the ability to keep the HttpSession alive when receiving WebSocket messages 其具体的特性非常之多，具体的内容可以从文档中了解到，笔者做一点自己的总结，Spring Session的特性包括但不限于以下： 使用GemFire来构建C/S架构的httpSession（不关注） 使用第三方仓储来实现集群session管理，也就是常说的分布式session容器，替换应用容器（如tomcat的session容器）。仓储的实现，Spring Session提供了三个实现（redis，mongodb，jdbc），其中redis使我们最常用的。程序的实现，使用AOP技术，几乎可以做到透明化地替换。（核心） 可以非常方便的扩展Cookie和自定义Session相关的Listener，Filter。 可以很方便的与Spring Security集成，增加诸如findSessionsByUserName，rememberMe，限制同一个账号可以同时在线的Session数（如设置成1，即可达到把前一次登录顶掉的效果）等等 介绍完特性，下面开始一步步集成Spring Session","text":"上一篇文章介绍了一些Session和Cookie的基础知识，这篇文章开始正式介绍Spring Session是如何对传统的Session进行改造的。官网这么介绍Spring Session： Spring Session provides an API and implementations for managing a user’s session information. It also provides transparent integration with: HttpSession - allows replacing the HttpSession in an application container (i.e. Tomcat) neutral way. Additional features include: Clustered Sessions - Spring Session makes it trivial to support clustered sessions without being tied to an application container specific solution. Multiple Browser Sessions - Spring Session supports managing multiple users’ sessions in a single browser instance (i.e. multiple authenticated accounts similar to Google). RESTful APIs - Spring Session allows providing session ids in headers to work with RESTful APIs WebSocket - provides the ability to keep the HttpSession alive when receiving WebSocket messages 其具体的特性非常之多，具体的内容可以从文档中了解到，笔者做一点自己的总结，Spring Session的特性包括但不限于以下： 使用GemFire来构建C/S架构的httpSession（不关注） 使用第三方仓储来实现集群session管理，也就是常说的分布式session容器，替换应用容器（如tomcat的session容器）。仓储的实现，Spring Session提供了三个实现（redis，mongodb，jdbc），其中redis使我们最常用的。程序的实现，使用AOP技术，几乎可以做到透明化地替换。（核心） 可以非常方便的扩展Cookie和自定义Session相关的Listener，Filter。 可以很方便的与Spring Security集成，增加诸如findSessionsByUserName，rememberMe，限制同一个账号可以同时在线的Session数（如设置成1，即可达到把前一次登录顶掉的效果）等等 介绍完特性，下面开始一步步集成Spring Session ##使用Redis集成Spring Session 引入依赖，Spring Boot的版本采用1.5.4 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 配置 配置类开启Redis Http Session 12345@Configuration@EnableRedisHttpSessionpublic class HttpSessionConfig &#123;&#125; 基本是0配置，只需要让主配置扫描到@EnableRedisHttpSession即可 配置文件application.yml，配置连接的redis信息 12345spring: redis: host: localhost port: 6379 database: 0 编写测试Controller，以便于观察Spring Session的特性，和前一篇文章使用同样的代码 12345678910111213141516171819202122@Controllerpublic class CookieController &#123; @RequestMapping(\"/test/cookie\") public String cookie(@RequestParam(\"browser\") String browser, HttpServletRequest request, HttpSession session) &#123; //取出session中的browser Object sessionBrowser = session.getAttribute(\"browser\"); if (sessionBrowser == null) &#123; System.out.println(\"不存在session，设置browser=\" + browser); session.setAttribute(\"browser\", browser); &#125; else &#123; System.out.println(\"存在session，browser=\" + sessionBrowser.toString()); &#125; Cookie[] cookies = request.getCookies(); if (cookies != null &amp;&amp; cookies.length &gt; 0) &#123; for (Cookie cookie : cookies) &#123; System.out.println(cookie.getName() + \" : \" + cookie.getValue()); &#125; &#125; return \"index\"; &#125;&#125; 启动类省略，下面开始测试。 在浏览器中访问如下端点：http://localhost:8080/test/cookie?browser=chrome，下面是连续访问4次的结果 12345671 不存在session，设置browser=chrome2 存在session，browser=chrome SESSION : 70791b17-83e1-42db-8894-73fbd2f2a1593 存在session，browser=chrome SESSION : 70791b17-83e1-42db-8894-73fbd2f2a1594 存在session，browser=chrome SESSION : 70791b17-83e1-42db-8894-73fbd2f2a159 如果还记得上一篇文章中运行结果的话，会发现和原生的session管理是有一些差别，原先的信息中我们记得Cookie中记录的Key值是JSESSIONID，而替换成RedisHttpSession之后变成了SESSION。接着观察redis中的变化： 解析一下这个redis store，如果不纠结于细节，可以跳过，不影响使用。 ​1 spring:session是默认的Redis HttpSession前缀（redis中，我们常用’:’作为分割符）。 2 每一个session都会有三个相关的key，第三个key最为重要，它是一个HASH数据结构，将内存中的session信息序列化到了redis中。如上文的browser，就被记录为sessionAttr:browser=chrome,还有一些meta信息，如创建时间，最后访问时间等。 3 另外两个key，expirations:1504446540000和sessions:expires:7079…我发现大多数的文章都没有对其分析，前者是一个SET类型，后者是一个STRING类型，可能会有读者发出这样的疑问，redis自身就有过期时间的设置方式TTL，为什么要额外添加两个key来维持session过期的特性呢？这需要对redis有一定深入的了解才能想到这层设计。当然这不是本节的重点，简单提一下：redis清除过期key的行为是一个异步行为且是一个低优先级的行为，用文档中的原话来说便是，可能会导致session不被清除。于是引入了专门的expiresKey，来专门负责session的清除，包括我们自己在使用redis时也需要关注这一点。在开发层面，我们仅仅需要关注第三个key就行了。 总结本节主要讲解了Spring Boot如何集成Spring Session，下一节将介绍更加复杂的特性。包括自定义Cookie序列化策略，与Spring Security的集成，根据用户名查找session等特性以及使用注意点。","categories":[{"name":"Spring Session","slug":"Spring-Session","permalink":"http://lexburner.github.io/categories/Spring-Session/"}],"tags":[{"name":"Spring Session","slug":"Spring-Session","permalink":"http://lexburner.github.io/tags/Spring-Session/"},{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"}]},{"title":"Re:从零开始的Spring Session(一)","slug":"Re：从零开始的Spring Session(一)","date":"2017-09-03T07:27:04.000Z","updated":"2017-09-04T01:51:48.012Z","comments":true,"path":"2017/09/03/Re：从零开始的Spring Session(一)/","link":"","permalink":"http://lexburner.github.io/2017/09/03/Re：从零开始的Spring Session(一)/","excerpt":"Session和Cookie这两个概念，在学习java web开发之初，大多数人就已经接触过了。最近在研究跨域单点登录的实现时，发现对于Session和Cookie的了解，并不是很深入，所以打算写两篇文章记录一下自己的理解。在我们的应用集成Spring Session之前，先补充一点Session和Cookie的关键知识。 Session与Cookie基础由于http协议是无状态的协议，为了能够记住请求的状态，于是引入了Session和Cookie的机制。我们应该有一个很明确的概念，那就是Session是存在于服务器端的，在单体式应用中，他是由tomcat管理的，存在于tomcat的内存中，当我们为了解决分布式场景中的session共享问题时，引入了redis，其共享内存，以及支持key自动过期的特性，非常契合session的特性，我们在企业开发中最常用的也就是这种模式。但是只要你愿意，也可以选择存储在JDBC，Mongo中，这些，spring都提供了默认的实现，在大多数情况下，我们只需要引入配置即可。而Cookie则是存在于客户端，更方便理解的说法，可以说存在于浏览器。Cookie并不常用，至少在我不长的web开发生涯中，并没有什么场景需要我过多的关注Cookie。http协议允许从服务器返回Response时携带一些Cookie，并且同一个域下对Cookie的数量有所限制，之前说过Session的持久化依赖于服务端的策略，而Cookie的持久化则是依赖于本地文件。虽然说Cookie并不常用，但是有一类特殊的Cookie却是我们需要额外关注的，那便是与Session相关的sessionId，他是真正维系客户端和服务端的桥梁。","text":"Session和Cookie这两个概念，在学习java web开发之初，大多数人就已经接触过了。最近在研究跨域单点登录的实现时，发现对于Session和Cookie的了解，并不是很深入，所以打算写两篇文章记录一下自己的理解。在我们的应用集成Spring Session之前，先补充一点Session和Cookie的关键知识。 Session与Cookie基础由于http协议是无状态的协议，为了能够记住请求的状态，于是引入了Session和Cookie的机制。我们应该有一个很明确的概念，那就是Session是存在于服务器端的，在单体式应用中，他是由tomcat管理的，存在于tomcat的内存中，当我们为了解决分布式场景中的session共享问题时，引入了redis，其共享内存，以及支持key自动过期的特性，非常契合session的特性，我们在企业开发中最常用的也就是这种模式。但是只要你愿意，也可以选择存储在JDBC，Mongo中，这些，spring都提供了默认的实现，在大多数情况下，我们只需要引入配置即可。而Cookie则是存在于客户端，更方便理解的说法，可以说存在于浏览器。Cookie并不常用，至少在我不长的web开发生涯中，并没有什么场景需要我过多的关注Cookie。http协议允许从服务器返回Response时携带一些Cookie，并且同一个域下对Cookie的数量有所限制，之前说过Session的持久化依赖于服务端的策略，而Cookie的持久化则是依赖于本地文件。虽然说Cookie并不常用，但是有一类特殊的Cookie却是我们需要额外关注的，那便是与Session相关的sessionId，他是真正维系客户端和服务端的桥梁。 代码示例用户发起请求，服务器响应请求，并做一些用户信息的处理，随后返回响应给用户；用户再次发起请求，携带sessionId，服务器便能够识别，这个用户就是之前请求的那个。 使用Springboot编写一个非常简单的服务端，来加深对其的理解。需求很简单，当浏览器访问localhost:8080/test/cookie?browser=xxx时，如果没有获取到session，则将request中的browser存入session；如果获取到session，便将session中的browser值输出。顺便将request中的所有cookie打印出来。 12345678910111213141516171819202122@Controllerpublic class CookieController &#123; @RequestMapping(\"/test/cookie\") public String cookie(@RequestParam(\"browser\") String browser, HttpServletRequest request, HttpSession session) &#123; //取出session中的browser Object sessionBrowser = session.getAttribute(\"browser\"); if (sessionBrowser == null) &#123; System.out.println(\"不存在session，设置browser=\" + browser); session.setAttribute(\"browser\", browser); &#125; else &#123; System.out.println(\"存在session，browser=\" + sessionBrowser.toString()); &#125; Cookie[] cookies = request.getCookies(); if (cookies != null &amp;&amp; cookies.length &gt; 0) &#123; for (Cookie cookie : cookies) &#123; System.out.println(cookie.getName() + \" : \" + cookie.getValue()); &#125; &#125; return \"index\"; &#125;&#125; 我们没有引入其他任何依赖，看看原生的session机制是什么。 1 使用chrome浏览器，访问localhost:8080/test/cookie?browser=chrome,控制台输出如下： 1Session Info: 不存在session，设置browser=chrome 既没有session，也没有cookie，我们将browser=chrome设置到session中。 再次访问同样的端点，控制台输出如下： 12Session Info: 存在session，browser=chromeCookie Info: JSESSIONID : 4CD1D96E04FC390EA6C60E8C40A636AF 多次访问之后，控制台依旧打印出同样的信息。 稍微解读下这个现象，可以验证一些结论。当服务端往session中保存一些数据时，Response中自动添加了一个Cookie：JSESSIONID：xxxx,再后续的请求中，浏览器也是自动的带上了这个Cookie，服务端根据Cookie中的JSESSIONID取到了对应的session。这验证了一开始的说法，客户端服务端是通过JSESSIONID进行交互的，并且，添加和携带key为JSESSIONID的Cookie都是tomcat和浏览器自动帮助我们完成的，这很关键。 2 使用360浏览器，访问localhost:8080/test/cookie?browser=360 第一次访问： 1Session Info: 不存在session，设置browser=360 后续访问： 12Session Info: 存在session，browser=360Cookie Info: JSESSIONID : 320C21A645A160C4843D076204DA2F40 为什么要再次使用另一个浏览器访问呢？先卖个关子，我们最起码可以得出结论，不同浏览器，访问是隔离的，甚至重新打开同一个浏览器，JSESSIONID也是不同的。另外可以尝试把保存session的操作注视掉，则可以发现Response中就不会返回JSESSIONID了，即这是一次无状态的请求。 安全问题其实上述的知识点，都是非常浅显的，之所以啰嗦一句，是为了引出这一节的内容，以及方便观察后续我们引入Spring Session之后的发生的变化。 还记得上一节的代码示例中，我们使用了两个浏览器： chrome浏览器访问时，JSESSIONID为4CD1D96E04FC390EA6C60E8C40A636AF，后端session记录的值为：browser=chrome。 360浏览器访问时，JSESSIONID为320C21A645A160C4843D076204DA2F40,后端session记录的值为：browser=360。 我们使用chrome插件Edit this Cookie，将chrome浏览器中的JSESSIONID修改为360浏览器中的值 同样访问原来的端点：localhost:8080/test/cookie?browser=chrome，得到的输出如下： 12存在session，browser=360JSESSIONID : 320C21A645A160C4843D076204DA2F40 证实了一点，存放在客户端的Cookie的确是存在安全问题的，我们使用360的JSESSIONID“骗”过了服务器。毕竟，服务器只能通过Cookie中的JSESSIONID来辨别身份。（这提示我们不要在公共场合保存Cookie信息，现在的浏览器在保存Cookie时通常会让你确定一次） 下一篇文章，将正式讲解如何在应用中集成Spring Session。","categories":[{"name":"Spring Session","slug":"Spring-Session","permalink":"http://lexburner.github.io/categories/Spring-Session/"}],"tags":[{"name":"Spring Session","slug":"Spring-Session","permalink":"http://lexburner.github.io/tags/Spring-Session/"},{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"}]},{"title":"解析Spring中的ResponseBody和RequestBody","slug":"解析Spring中的ResponseBody和RequestBody","date":"2017-08-30T04:44:21.000Z","updated":"2017-08-31T01:32:09.921Z","comments":true,"path":"2017/08/30/解析Spring中的ResponseBody和RequestBody/","link":"","permalink":"http://lexburner.github.io/2017/08/30/解析Spring中的ResponseBody和RequestBody/","excerpt":"","text":"spring，restful，前后端分离这些关键词都是大家耳熟能详的关键词了，一般spring常常需要与前端、第三方使用JSON，XML等形式进行交互，你也一定不会对@RequestBody和@ResponseBody这两个注解感到陌生。 @ResponseBody的使用由于@ResponseBody和@RequestBody的内部实现是同样的原理（封装请求和封装响应），所以本文以@ResponseBody为主要入手点，理解清楚任何一者，都可以同时掌握另一者。 如果想要从spring获得一个json形式返回值，操作起来是非常容易的。首先定义一个实体类: 1234public class Book &#123; private Integer id; private String bookName;&#125; 接着定义一个后端端点： 123456789@RestControllerpublic class BookController &#123; @GetMapping(value = \"/book/&#123;bookId&#125;\") public Book getBook(@PathVariable(\"bookId\") Integer bookId) &#123; return new Book(bookId, \"book\" + bookId); &#125;&#125; 在RestController中，相当于给所有的xxxMapping端点都添加了@ResponseBody注解，不返回视图，只返回数据。使用http工具访问这个后端端点localhost:8080/book/2，便可以得到如下的响应： 1234&#123; \"id\": 2, \"bookName\": \"book2\"&#125; 这是一个最简单的返回JSON对象的使用示例了，相信这样的代码很多人在项目中都写过。 添加XML解析如果我们需要将Book对象以XML的形式返回，该如何操作呢？这也很简单，给Book对象添加@XmlRootElement注解，让spring内部能够解析XML对象。 12345@XmlRootElementpublic class Book &#123; private Integer id; private String bookName;&#125; 在我们未对web层的BookController做任何改动之前，尝试访问localhost:8080/book/2时，会发现得到的结果仍然是前面的JSON对象。这也能够理解，因为Book对象如今既可以被解析为XML，也可以被解析为JSON，我们隐隐察觉这背后有一定的解析顺序关系，但不着急，先看看如何让RestController返回XML解析结果。 方法1 http客户端指定接收的返回结果类型 http协议中，可以给请求头添加Accept属性，笔者常用的http客户端是idea自带的Test RESTful Web Service以及chrome的插件Postman。简单的调试，前者基本可以满足我们大多数的需求，而这里为了给大家更直观的体验，笔者使用了Postman。以code形式展示： 123GET /book/2 HTTP/1.1Host: localhost:8080Accept: application/xml 响应内容如下： 12345&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;&lt;book&gt; &lt;bookName&gt;book2&lt;/bookName&gt; &lt;id&gt;2&lt;/id&gt;&lt;/book&gt; 方法2 在RestController后端端点中指定返回类型 修改后的RestController如下所示 123456789@RestControllerpublic class BookController &#123; @GetMapping(value = \"/book/&#123;bookId&#125;\", produces = &#123;\"application/xml\"&#125;) public Book getBook(@PathVariable(\"bookId\") Integer bookId) &#123; return new Book(bookId, \"book\" + bookId); &#125;&#125; 此时即使将请求中的Accept: application/xml去除，依旧可以返回上述的XML结果。 通常情况下，我们的服务端返回的形式一般是固定的，即限定了是JSON，XML中的一种，不建议依赖于客户端添加Accept的信息，而是在服务端限定produces类型。 详解Accpect与producesAccpect包含在http协议的请求头中，其本身代表着客户端发起请求时，期望返回的响应结果的媒体类型。如果服务端可能返回多个媒体类型，则可以通过Accpect指定具体的类型。 produces是Spring为我们提供的注解参数，代表着服务端能够支持返回的媒体类型，我们注意到produces后跟随的是一个数组类型，也就意味着服务端支持多种媒体类型的响应。 在上一节中，我们未显示指定produces值时，其实就隐式的表明，支持XML形式，JSON形式的媒体类型响应。从实验结果，我们也可以看出，当请求未指定Accpect，响应未指定produces时，具体采用何种形式返回是有Spring控制的。在接口交互时，最良好的对接方式，当然是客户端指定Accpect，服务端指定produces，这样可以避免模棱两可的请求响应，避免出现意想不到的对接结果。 详解ContentType与consumes恰恰和Accpect&amp;produces相反，这两个参数是与用于限制请求的。理解了前两者的含义，这两个参数可以举一反三理解清楚。 ContentType包含在http协议的请求头中，其本身代表着客户端发起请求时，告知服务端自己的请求媒体类型是什么。 consumes是Spring为我们提供的注解参数，代表着服务端能够支持处理的请求媒体类型，同样是一个数组，意味着服务端支持多种媒体类型的请求。一般而言，consumes与produces对请求响应媒体类型起到的限制作用，我们给他一个专有名词：窄化。 http请求响应媒体类型一览上面描述的4个属性：Accpect与produces，ContentType与consumes究竟有哪些类型与之对应呢？我只将常用的一些列举了出来： 媒体类型 含义 text/html HTML格式 text/plain 纯文本格式 text/xml, application/xml XML数据格式 application/json JSON数据格式 image/gif gif图片格式 image/png png图片格式 application/octet-stream 二进制流数据 application/ x-www-form-urlencoded form表单数据 multipart/form-data 含文件的form表单 其中有几个类型值得一说，web开发中我们常用的提交表单操作，其默认的媒体类型就是application/ x-www-form-urlencoded，而当表单中包含文件时，大家估计都踩过坑，需要将enctype=multipart/form-data设置在form参数中。text/html也就是常见的网页了，json与xml常用于数据交互，其他不再赘述。 而在JAVA中，提供了MediaType这样的抽象，来与http的媒体类型进行对应。‘/’之前的名词，如text，application被称为类型（type），‘/’之后被称为子类型(subType)。 详解HttpMessageConverter我们想要搞懂Spring到底如何完成众多实体类等复杂类型的数据转换以及与媒体类型的对应，就必须要搞懂HttpMessageConverter这个顶级接口： 1234567891011public interface HttpMessageConverter&lt;T&gt; &#123; boolean canRead(Class&lt;?&gt; var1, MediaType var2); boolean canWrite(Class&lt;?&gt; var1, MediaType var2); List&lt;MediaType&gt; getSupportedMediaTypes(); T read(Class&lt;? extends T&gt; var1, HttpInputMessage var2) throws IOException, HttpMessageNotReadableException; void write(T var1, MediaType var2, HttpOutputMessage var3) throws IOException, HttpMessageNotWritableException;&#125; 大致能看出Spring的处理思路。下面的流程图可以更好方便我们的理解： 对于添加了@RequestBody和@ResponseBody注解的后端端点，都会经历由HttpMessageConverter进行的数据转换的过程。而在Spring启动之初，就已经有一些默认的转换器被注册了。通过在RequestResponseBodyMethodProcessor 中打断点，我们可以获取到一个converters列表： 源码方面不做过多的解读，有兴趣的朋友可以研究一下RequestResponseBodyMethodProcessor 中的handleReturnValue方法，包含了转换的核心实现。 自定义HttpMessageConverter前面已经提及了消息转换器是通过判断媒体类型来调用响应的转换类的，不禁引发了我们的思考，如果我们遇到了不常用的MediaType，或者自定义的MediaType，又想要使用Spring的@RequestBody，@ResponseBody注解，该如何添加代码呢？下面我们通过自定义一个HttpMessageConverter来了解Spring内部的转换过程。 先定义我们的需求，自定一个MediaType：application/toString，当返回一个带有@ResponseBody注解的实体类时，将该实体类的ToString作为响应内容。 1 首先重写Book的ToString方法，方便后期效果展示 1234567@Overridepublic String toString() &#123; return \"~~~Book&#123;\" + \"id=\" + id + \", bookName='\" + bookName + '\\'' + \"&#125;~~~\";&#125; 2 编写自定义的消息转换器 123456789101112131415161718192021222324public class ToStringHttpMessageConverter extends AbstractHttpMessageConverter&lt;Object&gt; &#123; public ToStringHttpMessageConverter() &#123; super(new MediaType(\"application\", \"toString\", Charset.forName(\"UTF-8\")));// &lt;1&gt; &#125; @Override protected boolean supports(Class&lt;?&gt; clazz) &#123; return true; &#125; //从请求体封装数据 对应RequestBody 用String接收 @Override protected Object readInternal(Class&lt;?&gt; clazz, HttpInputMessage inputMessage) throws IOException, HttpMessageNotReadableException &#123; return StreamUtils.copyToString(inputMessage.getBody(), Charset.forName(\"UTF-8\")); &#125; //从响应体封装数据 对应ResponseBody @Override protected void writeInternal(Object o, HttpOutputMessage outputMessage) throws IOException, HttpMessageNotWritableException &#123; String result = o.toString();//&lt;2&gt; outputMessage.getBody().write(result.getBytes()); &#125;&#125; 此处指定了支持的媒体类型 调用类的ToString方法，将结果写入到输出流中 3 配置自定义的消息转换器 12345678@Configurationpublic class WebMvcConfig extends WebMvcConfigurerAdapter&#123; @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; converters.add(new ToStringHttpMessageConverter()); &#125;&#125; 4 配置后端端点，指定生产类型 12345678@RestControllerpublic class BookController &#123; @GetMapping(value = \"/book/&#123;bookId&#125;\",produces = &#123;\"application/toString\",\"application/json\",\"application/xml\"&#125;) public Book getBook(@PathVariable(\"bookId\") Integer bookId) &#123; return new Book(bookId, \"book\" + bookId); &#125;&#125; 此处只是为了演示，添加了三个生产类型，我们的后端端点可以支持输出三种类型，而具体输出哪一者，则依赖客户端的Accept指定。 5 客户端请求 123GET /book/2 HTTP/1.1Host: localhost:8080Accept: application/toString 响应结果如下： 1​~~~Book&#123;id=2, bookName=&apos;book2&apos;&#125;~~~ 此时，你可以任意指定Accept的类型，即可获得不同形式的Book返回结果，可以是application/toString，application/json，application/xml，都会对应各自的HttpMessageConverter。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"}]},{"title":"XML与javabean的转换","slug":"javabean-xml","date":"2017-08-25T19:41:27.000Z","updated":"2017-09-04T01:37:18.614Z","comments":true,"path":"2017/08/26/javabean-xml/","link":"","permalink":"http://lexburner.github.io/2017/08/26/javabean-xml/","excerpt":"XML可以说是一种被时代淘汰的数据传输格式，毕竟相比较JSON，其语法，表现形式，以及第三方类库的支持，都要略逊一筹，但最近在对接一些老接口时，主要还是以XML为主，而翻阅相关的文档以及博客，没看到很好的文章介绍如何使用xml进行数据传输，所以简单写下此文，做一下记录。内心多多少少还是会抵制对接如此老旧的接口，不过生活还是要继续。 Code First先上一段代码，展示一下如何封装，讲解放到后面 一个典型的对接方提供的XML如下： 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;&lt;ORDER&gt; &lt;ORDER_NO&gt;10086&lt;/ORDER_NO&gt; &lt;TOTAL_PRICE&gt;3.14&lt;/TOTAL_PRICE&gt; &lt;CREATE_TIME&gt;2017-08-26 03:39:30&lt;/CREATE_TIME&gt; &lt;ORDER_ITEMS&gt; &lt;ORDER_ITEM&gt; &lt;GOODS_NAME&gt;德芙&lt;/GOODS_NAME&gt; &lt;NUM&gt;3&lt;/NUM&gt; &lt;/ORDER_ITEM&gt; &lt;ORDER_ITEM&gt; &lt;GOODS_NAME&gt;旺仔&lt;/GOODS_NAME&gt; &lt;NUM&gt;10&lt;/NUM&gt; &lt;/ORDER_ITEM&gt; &lt;/ORDER_ITEMS&gt;&lt;/ORDER&gt; 而我们要对应的实体类，则应当如下： 12345678910111213141516171819@XmlRootElement(name = \"ORDER\")// &lt;1&gt;@XmlAccessorType(XmlAccessType.FIELD)// &lt;1&gt;public class Order &#123; @XmlElement(name = \"ORDER_NO\")// &lt;1&gt; private String orderNo; @XmlElement(name = \"TOTAL_PRICE\") private BigDecimal totalPrice; @XmlElement(name = \"CREATE_TIME\") @XmlJavaTypeAdapter(DateAdapter.class) // &lt;2&gt; private Date createTime; @XmlElementWrapper(name = \"ORDER_ITEMS\") // &lt;3&gt; @XmlElement(name = \"ORDER_ITEM\") private List&lt;OrderItem&gt; orderItems;&#125; 12345678910@XmlAccessorType(XmlAccessType.FIELD)public class OrderItem &#123; @XmlElement(name = \"GOODS_NAME\") private String goodsName; @XmlElement(name = \"NUM\") private Integer num;&#125; 我举的这个示例基本包含一般情况下所有可能出现的需求 常用注解XmlRootElement，XmlAccessorType，XmlElement 日期转换的适配器注解 如何在XML中设置集合 在介绍这三点之前，先给出转换的工具类","text":"XML可以说是一种被时代淘汰的数据传输格式，毕竟相比较JSON，其语法，表现形式，以及第三方类库的支持，都要略逊一筹，但最近在对接一些老接口时，主要还是以XML为主，而翻阅相关的文档以及博客，没看到很好的文章介绍如何使用xml进行数据传输，所以简单写下此文，做一下记录。内心多多少少还是会抵制对接如此老旧的接口，不过生活还是要继续。 Code First先上一段代码，展示一下如何封装，讲解放到后面 一个典型的对接方提供的XML如下： 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;&lt;ORDER&gt; &lt;ORDER_NO&gt;10086&lt;/ORDER_NO&gt; &lt;TOTAL_PRICE&gt;3.14&lt;/TOTAL_PRICE&gt; &lt;CREATE_TIME&gt;2017-08-26 03:39:30&lt;/CREATE_TIME&gt; &lt;ORDER_ITEMS&gt; &lt;ORDER_ITEM&gt; &lt;GOODS_NAME&gt;德芙&lt;/GOODS_NAME&gt; &lt;NUM&gt;3&lt;/NUM&gt; &lt;/ORDER_ITEM&gt; &lt;ORDER_ITEM&gt; &lt;GOODS_NAME&gt;旺仔&lt;/GOODS_NAME&gt; &lt;NUM&gt;10&lt;/NUM&gt; &lt;/ORDER_ITEM&gt; &lt;/ORDER_ITEMS&gt;&lt;/ORDER&gt; 而我们要对应的实体类，则应当如下： 12345678910111213141516171819@XmlRootElement(name = \"ORDER\")// &lt;1&gt;@XmlAccessorType(XmlAccessType.FIELD)// &lt;1&gt;public class Order &#123; @XmlElement(name = \"ORDER_NO\")// &lt;1&gt; private String orderNo; @XmlElement(name = \"TOTAL_PRICE\") private BigDecimal totalPrice; @XmlElement(name = \"CREATE_TIME\") @XmlJavaTypeAdapter(DateAdapter.class) // &lt;2&gt; private Date createTime; @XmlElementWrapper(name = \"ORDER_ITEMS\") // &lt;3&gt; @XmlElement(name = \"ORDER_ITEM\") private List&lt;OrderItem&gt; orderItems;&#125; 12345678910@XmlAccessorType(XmlAccessType.FIELD)public class OrderItem &#123; @XmlElement(name = \"GOODS_NAME\") private String goodsName; @XmlElement(name = \"NUM\") private Integer num;&#125; 我举的这个示例基本包含一般情况下所有可能出现的需求 常用注解XmlRootElement，XmlAccessorType，XmlElement 日期转换的适配器注解 如何在XML中设置集合 在介绍这三点之前，先给出转换的工具类 转换工具类1234567891011121314151617181920212223242526272829public class XML &#123; public static String toXmlString(Object obj) &#123; String result; try &#123; JAXBContext context = JAXBContext.newInstance(obj.getClass()); Marshaller marshaller = context.createMarshaller(); StringWriter writer = new StringWriter(); marshaller.marshal(obj, writer); result = writer.toString(); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; return result; &#125; public static &lt;T&gt; T parseObject(String input, Class&lt;T&gt; claaz) &#123; Object result; try &#123; JAXBContext context = JAXBContext.newInstance(claaz); Unmarshaller unmarshaller = context.createUnmarshaller(); result = unmarshaller.unmarshal(new StringReader(input)); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; return (T) result; &#125;&#125; JSON工具类中，笔者习惯于使用fastjson，所以干脆连同工具类类名命名和方法命名都按照了它的风格，只有两个方法。 注解的介绍给实体类加上注解，再使用工具类，就可以实现实体和XML的相互转换了。那么前面提到的三个注意点中的相关注解分别代表了什么含义呢？ @XmlRootElement 作用域：类 代表一个XML对象的根节点，常使用name属性来可以指定生成XML之后的具体名称 @XmlElement 作用域：字段，方法，参数（不常用） 代表一个XML对象的普通界点信息，常使用name属性来指定生成XML之后的具体名称。需要注意与@XmlAccessorType搭配使用时，有一些注意点，见下 @XmlAccessorType 作用域：类，包（不常用） 告诉解析器，在解析XML时要如何获取类的字段属性，有4个枚举类型： | 枚举类型 | 访问方式 || ——————————- | ——————————- || XmlAccessType.FIELD | 成员变量 || XmlAccessType.PROPERTY | public getter,setter || XmlAccessType.PUBLIC_MEMBER（默认） | public getter,setter+public成员变量 || XmlAccessType.NONE | 必须显示指定@XmlElement | 我们上述的例子中，使用的方式是在类上配置@XmlAccessorType(XmlAccessType.FIELD)，基于成员变量访问属性，并且，在每一个成员变量之上都显示指定了name=xxx；而如果配置@XmlAccessorType(XmlAccessType.PUBLIC_MEMBER)即默认配置，则你需要将@XmlElement注解写在getter方法上,笔者比较习惯例子中的写法。需要注意点的一点是，如果@XmlAccessorType与@XmlElement的配置不对应，很容易触发自动的转换方式，会导致某个节点出现两次的异常。 @XmlJavaTypeAdapter 作用域：字段,方法,类,包,参数（前三者常用） java内置的xml日期转换类不能满足我们的需求（可以动手试试看默认日期的格式是什么），以及遇到自定义的类，需要配置转换器，就可以使用这个注解，@XmlJavaTypeAdapter注解接收一个自定义的Adapter，需要继承自XmlAdapter&lt;ValueType,BoundType&gt;抽象类，一个常用的日期转化适配器如下： 1234567891011121314151617181920212223public class DateAdapter extends XmlAdapter&lt;String, Date&gt; &#123; static ThreadLocal&lt;DateFormat&gt; sdf ; static &#123; sdf =new ThreadLocal&lt;DateFormat&gt;() &#123; @Override protected DateFormat initialValue() &#123; return new SimpleDateFormat(\"yyyy-MM-dd hh:mm:ss\"); &#125; &#125;; &#125; @Override public Date unmarshal(String v) throws Exception &#123; return sdf.get().parse(v); &#125; @Override public String marshal(Date v) throws Exception &#123; return sdf.get().format(v); &#125;&#125; 使用Adapter的弊端也很明显，一个适配器只能对应一个日期的格式，在实际开发中我们往往会将日期区分成天维度的日期和秒维度的日期，不能像大多数JSON那样拥有灵活的注解，如果有读者有想到好的解决方案，欢迎跟我沟通。涉及到日期格式转化，时刻不要忘记SimpleDateFormat线程不安全这一点。 @XmlElementWrapper XML中表示集合时，在最外层通常会有一个Xxxs或者XxxList这样的标签，可以通过@XmlElementWrapper实现，其中name就代表额外添加的包裹信息是什么,如上文的OrderItems。 一些其他的转换工具类我们主要任务是实现XML字符串和javabean之间转换，不是解析XML，所以dom4j一类的类库不用考虑。熟悉spring的人会了解到一点，spring其实已经封装了xml转换相关的类，即org.springframework.oxm.jaxb.Jaxb2Marshaller这个类，他的顶层接口是org.springframework.oxm.Marshaller和org.springframework.oxm.UnMarshaller。而在java规范中，也存在同名的接口：javax.xml.bind.Marshaller,javax.xml.bind.UnMarshaller，这点在使用中需要注意下。笔者的建议是，这种数据格式转换操作，应当尽量引入最少的依赖。所以使用javax的类库下的相关方法进行封装。上述的工具类，仅仅只需要引入javax包，即可使用了。非常方便、","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"XML","slug":"XML","permalink":"http://lexburner.github.io/tags/XML/"}]},{"title":"sinosoft代码规范","slug":"project-rules","date":"2017-08-25T04:18:45.000Z","updated":"2017-12-27T07:40:43.635Z","comments":true,"path":"2017/08/25/project-rules/","link":"","permalink":"http://lexburner.github.io/2017/08/25/project-rules/","excerpt":"","text":"介绍本文档主要针对我们项目内部正在使用的框架，以及代码审查发现的一些共性问题提出一些开发规范。 JavaBean规范1 驼峰命名法【强制】 2 布尔类型规范【强制】【说明】所有的布尔类型不允许以is开头，否则会导致部分序列化，hibernate框架出现解析异常。【反例】原来项目的BaseDomain中标记逻辑删除的字段,在部分场景下会出现问题 123456789101112@Column(name = \"is_delete\")private Boolean isDelete = false;public Boolean getIsDelete() &#123; return isDelete; &#125;public void setIsDelete(Boolean isDelete) &#123; if(deleteFlag) this.deleteDate = new Date(); this.isDelete = isDelete;&#125; tips: 使用intellij idea的快捷键（for eclipse）alt+shift+r，或者菜单栏Refactor-&gt;Rename，可以重构字段名称【正例】 12@Column(name = \"is_delete\")private Boolean deleteFlag = false; 3 装箱类型优于原生类型【推荐】在业务代码中，更加推荐使用装箱类型Integer Double Boolean…【说明】在未设值的情况下，基础类型具有默认值，而装箱类型为null以Boolean类型为例，如果使用boolean，那么在未复制时，无法得知其到底是被赋值成了false，还是未赋值 领域模型规范首先理解各个常用的领域模型的含义： 领域模型 全称 中文含义 DO Domain Object 领域对象 DTO Data Transfer Object 数据传输对象 VO View Object 视图对象 对于View Object，PO等等其他一些的对象不在此做要求，只说明一下常用的几个DO就是我们最常用的数据库持久对象，是OOP对于现实中的抽象，一般使用orm框架映射到数据库DTO这一层，目前我们的项目还没有投入使用，即将考虑投入使用，理论上来说，两个微服务模块是严禁共享数据库的所以A模块要查询B模块的数据，需要使用B模块app层暴露出来的api来查询，其中B模块返回的实体，不能是直接从数据库中查询出来的DO，而应该是DO转换而成的DTO。以及其他服务服务用语传输的变量，都叫做DTOVO就是常存在于视图层模板渲染使用的实体类 【推荐】领域模型命名规范【说明】由于DO这一层大家已经养成了习惯，不做要求了。DTO有些特殊，他常常与业务的传输对象相关，而不限于以DTO结尾，如xxxQuery也可以是DTO对象。VO对象推荐以VO结尾。注意：不要命名为Vo,Dto。 包结构规范1 包命名【强制】 格式如下：公司名.模块名.层次名包名应当尽量使用能够概括模块总体含义,单词义,单数,不包含特殊字符的单词【正例】: sinosoftgz.message.admin【反例】: sinosoftgz.mailsms.admin sinosoftgz.mail.sms.admin 2 包结构【推荐】当项目模块的职责较为复杂，且考虑到以后拓展的情况下，单个模块依旧包含着很多小的业务模块时，应当优先按照业务区分包名 【反例】: 123456789101112131415161718192021sinosoftgz.message.admin config 模块公用Config.java service 模块公用Service.java Mail私有Service.java MailTemplateService.java MailMessageService.java Sms私有Service.java SmsTemplateService.java SmsMessageService.java web 模块公用Controller.java IndexController.java Mail私有Controller.java MailTemplateController.java MailMessageController.java Sms私有Controller.java SmsTemplateController.java SmsMessageController.java MailSmsAdminApp.java 【正例】: 12345678910111213141516171819202122232425262728293031sinosoftgz.message.admin config 模块公用Config.java service 模块公用Service.java web 模块公用Controller.java IndexController.java mail config MailConfig.java service Mail私有Service.java MailTemplateService.java MailMessageService.java web Mail私有Controller.java MailTemplateController.java MailMessageController.java sms config Smsconfig.java service Sms私有Service.java SmsTemplateService.java SmsMessageService.java web Sms私有Controller.java SmsTemplateController.java SmsMessageController.java MessageAdminApp.java service和controller以及其他业务模块相关的包相隔太远，或者干脆全部丢到一个包内，单纯用前缀区分，会形成臃肿，充血的包结构。如果是项目结构较为单一，可以仅仅使用前缀区分；如果是项目中业务模块有明显的区分条件，应当单独作为一个包，用包名代表业务模块的含义。 容易忽视的细节1 运算溢出【强制】 【反例】Integer a = Integer b * Integer c; 【正例】Long a = Integer b * Integer c;(强转) 整数相乘可能会溢出，需要使用Long接收 2 Double类型的精度问题【强制】 Double不能用于商业计算，使用BigDecimal代替 3 BigDecimal规范【强制】 【反例】 12BigDecimal totalMoney = new BigDecimal(\"100.42\");BigDecimal averageMoney = totalMoney.divide(new BigDecimal(\"22\")); 【正例】 12BigDecimal totalMoney = new BigDecimal(\"100.42\");BigDecimal averageMoney = totalMoney.divide(new BigDecimal(\"22\"),3); 业务实体类中的与金额相关的变量统一使用BigDecimal,四则运算采用BigDecimal的相关api进行。做除法时需要额外注意保留精度的问题，否则可能会报异常，并且不易被测试出 4 equals规范【强制】 【反例】 123456Integer a = 2333;Integer b = 2333;System.out.println(a == b);//fasleInteger a = 2;Integer b = 2;System.out.println(a == b);//true 【正例】 1a.equals(b) 要注意正确的比较方法，谨慎使用==，它比较的是引用 数据库规范1 必要的地方必须添加索引，如唯一索引，作为条件查询的列【强制】 不添加索引，会造成全表扫描，浪费性能。 2 生产环境，uat环境，不允许使用jpa.hibernate.ddl-auto: create自动建表，每次ddl的修改需要保留脚本，统一管理【强制】3 业务数据不能使用deleteBy…而要使用逻辑删除setDeleteFlag(true),查询时，findByxxxAndDeleteFlag(xxx,false)【强制】 4 如有可替代方案，则禁止使用存储过程和触发器【强制】 5 字段的长度和类型需要按照实际含义定制【推荐】 【反例】 12345@Entityclass Person&#123; private String name; private Integer age;&#125; 【正例】 1234567@Entityclass Person&#123; @Column(columnDefinition = \"varchar(50)\") private String name; @Column(columnDefinition = \"int(3)\") private Integer age;&#125; 明确字段的长度和类型可以迫使开发者去思考字段所处的业务场景，在性能上，字段长度也可以加强索引的性能。 6 使用外键不要使用数据库层面的约束【强制】 不便于数据迁移，统一在应用层控制关联。 ORM规范【强制】条件查询超过三个参数的，使用criteriaQuery，predicates 而不能使用springdata的findBy 【反例】 12345678910111213public Page&lt;GatewayApiDefine&gt; findAll(GatewayApiDefine gatewayApiDefine,Pageable pageable)&#123; if(Lang.isEmpty(gatewayApiDefine.getRole()))&#123; gatewayApiDefine.setRole(\"\"); &#125; if(Lang.isEmpty(gatewayApiDefine.getApiName()))&#123; gatewayApiDefine.setApiName(\"\"); &#125; if(Lang.isEmpty(gatewayApiDefine.getEnabled()))&#123; return gatewayApiDefineDao.findByRoleLikeAndApiNameLikeOrderByLastUpdatedDesc(\"%\"+gatewayApiDefine.getRole()+\"%\",\"%\"+gatewayApiDefine.getApiName()+\"%\",pageable); &#125;else&#123; return gatewayApiDefineDao.findByRoleLikeAndApiNameLikeAndEnabledOrderByLastUpdatedDesc(\"%\"+gatewayApiDefine.getRole()+\"%\",\"%\"+gatewayApiDefine.getApiName()+\"%\",gatewayApiDefine.getEnabled(),pageable); &#125; &#125; 在Dao层定义了大量的findBy方法，在Service写了过多的if else判断，导致业务逻辑不清晰 【正例】 123456789101112131415161718192021222324252627public Page&lt;MailTemplateConfig&gt; findAll(MailTemplateConfig mailTemplateConfig, Pageable pageable) &#123; Specification querySpecification = (Specification&lt;MailTemplateConfig&gt;) (root, criteriaQuery, criteriaBuilder) -&gt; &#123; List&lt;Predicate&gt; predicates = new ArrayList&lt;&gt;(); predicates.add(criteriaBuilder.isFalse(root.get(\"deleteFlag\"))); //级联查询mailTemplate if (!Lang.isEmpty(mailTemplateConfig.getMailTemplate())) &#123; //短信模板名称 if (!Lang.isEmpty(mailTemplateConfig.getMailTemplate().getTemplateName())) &#123; predicates.add(criteriaBuilder.like(root.join(\"mailTemplate\").get(\"templateName\"), String.format(\"%%%s%%\", mailTemplateConfig.getMailTemplate().getTemplateName()))); &#125; //短信模板类型 if (!Lang.isEmpty(mailTemplateConfig.getMailTemplate().getTemplateType())) &#123; predicates.add(criteriaBuilder.equal(root.join(\"mailTemplate\").get(\"templateType\"), mailTemplateConfig.getMailTemplate().getTemplateType())); &#125; &#125; //产品分类 if (!Lang.isEmpty(mailTemplateConfig.getProductType())) &#123; predicates.add(criteriaBuilder.equal(root.get(\"productType\"), mailTemplateConfig.getProductType())); &#125; //客户类型 if (!Lang.isEmpty(mailTemplateConfig.getConsumerType())) &#123; predicates.add(criteriaBuilder.equal(root.get(\"consumerType\"), mailTemplateConfig.getConsumerType())); &#125; return criteriaBuilder.and(predicates.toArray(new Predicate[predicates.size()])); &#125;; return mailTemplateConfigRepos.findAll(querySpecification, pageable); &#125; 条件查询是admin模块不可避免的一个业务功能，使用criteriaQuery可以轻松的添加条件，使得代码容易维护，他也可以进行分页，排序，连表操作，充分发挥jpa面向对象的特性，使得业务开发变得快捷。 数据结构1 集合中迭代过程中增删数据使用迭代器完成 【反例】 12345678List&lt;String&gt; a = new ArrayList&lt;String&gt;();a.add(\"1\"); a.add(\"2\"); for (String temp : a) &#123; if(\"1\".equals(temp))&#123; a.remove(temp); &#125; &#125; 【正例】 1234567Iterator&lt;String&gt; it = a.iterator(); while(it.hasNext())&#123; String temp = it.next(); if((\"1\".equals(temp))&#123; it.remove(); &#125; &#125; 2 hashCode和equals重写规范【强制】 作为Map键值，Set值的实体类，务必重写hashCode与equals方法，可参考《effective java》。重写时务必做到以下几点 自反性: x.equals(x) 一定是true 对null: x.equals(null) 一定是false 对称性: x.equals(y) 和 y.equals(x)结果一致 传递性: a 和 b equals , b 和 c equals，那么 a 和 c也一定equals。 一致性: 在某个运行时期间，2个对象的状态的改变不会不影响equals的决策结果，那么，在这个运行时期间，无论调用多少次equals，都返回相同的结果。做到无状态。 禁止使用魔法数字【模型层与业务层】【强制】一些固定业务含义的代码可以使用枚举类型，或者final static常量表示，在设值时，不能直接使用不具备业务含义的数值。 【反例】 1234567891011//实体类定义/** * 发送设置标志 (1：立即发送 2：预设时间发送 ) */@Column(columnDefinition = \"varchar(1) comment '发送设置标志'\")protected String sendFlag;//业务代码赋值使用MailMessage mailMessage = new MailMessage();mailMessage.setSendSuccessFlag(\"1\");mailMessage.setValidStatus(\"0\");mailMessage.setCustom(true); 【正例】：使用final static常量: 1234567891011121314151617181920212223242526272829303132333435//实体类定义 /** * 发送设置标志 * * @see sendFlag */ public final static String SEND_FLAG_NOW = \"1\"; //立即发送 public final static String SEND_FLAG_DELAY = \"2\"; //预设时间发送 /** * 发送成功标志 * * @see sendSuccessFlag */ public final static Map&lt;String, String&gt; SEND_SUCCESS_FLAG_MAP = new LinkedHashMap&lt;&gt;(); public final static String SEND_WAIT = \"0\"; public final static String SEND_SUCCESS = \"1\"; public final static String SEND_FAIL = \"2\"; static &#123; SEND_SUCCESS_FLAG_MAP.put(SEND_WAIT, \"未发送\"); SEND_SUCCESS_FLAG_MAP.put(SEND_SUCCESS, \"发送成功\"); SEND_SUCCESS_FLAG_MAP.put(SEND_FAIL, \"发送失败\"); &#125; /** * 发送设置标志 (1：立即发送 2：预设时间发送 ) */ @Column(columnDefinition = \"varchar(1) comment '发送设置标志'\") protected String sendFlag;//业务代码赋值使用MailMessage mailMessage = new MailMessage();mailMessage.setSendSuccessFlag(MailMessage.SEND_WAIT);mailMessage.setValidStatus(MailMessage.VALID_WAIT);mailMessage.setCustom(true); 【说明】魔法数字不能使代码一眼能够看明白到底赋的是什么值，并且，实体类发生变化后，可能会导致赋值错误，与预期赋值不符合且错误不容易被发现。 【正例】：也可以使用枚举类型避免魔法数字 1234567891011121314151617181920212223242526protected String productType;protected String productName;@Enumerated(EnumType.STRING)protected ConsumerTypeEnum consumerType;@Enumerated(EnumType.STRING)protected PolicyTypeEnum policyType;@Enumerated(EnumType.STRING)protected ReceiverEnum receiver;public enum ConsumerTypeEnum &#123; PERSONAL, ORGANIZATION; public String getLabel() &#123; switch (this) &#123; case PERSONAL: return \"个人\"; case ORGANIZATION: return \"团体\"; default: return \"\"; &#125; &#125;&#125; 【视图层】【推荐】例如，页面迭代select的option，不应该在view层判断，而应该在后台传入map在前台迭代【正例】：12345678model.put(\"typeMap\",typeMap);模板类型：&lt;select type=\"text\" name=\"templateType\"&gt; &lt;option value=\"\"&gt;全部&lt;/option&gt; &lt;#list typeMap?keys as key&gt; &lt;option &lt;#if ((mailTemplate.templateType!\"\")==key)&gt;selected=\"selected\"&lt;/#if&gt;value=\"$&#123;key&#125;\"&gt;$&#123;typeMap[key]&#125;&lt;/option&gt; &lt;/#list&gt;&lt;/select&gt; 【反例】：12345678模板类型：&lt;select type=\"text\" name=\"templateType\"&gt; &lt;option value=\"\"&gt;全部&lt;/option&gt; &lt;option &lt;#if $&#123;xxx.templateType!&#125;==\"1\" selected=\"selected\"&lt;/#if&gt; value=\"1\"&gt;承保通知&lt;/option&gt; ... &lt;option &lt;#if $&#123;xxx.templateType!&#125;==\"5\" selected=\"selected\"&lt;/#if&gt; value=\"5\"&gt;核保通知&lt;/option&gt;&lt;/select&gt; 否则修改后台代码后，前端页面也要修改，设计原则应当是修改一处，其他全部变化。且 1，2…,5的含义可能会变化，不能从页面得知value和option的含义是否对应。 并发处理项目中会出现很多并发问题，要做到根据业务选择合适的并发解决方案，避免线程安全问题 1 simpleDateFormat有并发问题，不能作为static类变量【强制】【反例】：这是我在某个项目模块中，发现的一段代码1234567891011Class XxxController&#123; public final static SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd hh:mm:ss\"); @RequestMapping(\"/xxxx\") public String xxxx(String dateStr)&#123; XxxEntity xxxEntity = new XxxEntity(); xxxEntity.setDate(simpleDateFormat.parse(dateStr)); xxxDao.save(xxxEntity); return \"xxx\"; &#125;&#125; 【说明】SimpleDateFormat 是线程不安全的类，不能作为静态类变量给多线程并发访问。如果不了解多线程，可以将其作为实例变量，每次使用时都new一个出来使用。不过更推荐使用ThreadLocal来维护，减少new的开销。【正例】一个使用ThreadLocal维护SimpleDateFormat的线程安全的日期转换类：1234567891011121314151617public class ConcurrentDateUtil &#123; private static ThreadLocal&lt;DateFormat&gt; threadLocal = new ThreadLocal&lt;DateFormat&gt;() &#123; @Override protected DateFormat initialValue() &#123; return new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); &#125; &#125;; public static Date parse(String dateStr) throws ParseException &#123; return threadLocal.get().parse(dateStr); &#125; public static String format(Date date) &#123; return threadLocal.get().format(date); &#125;&#125; 2 名称唯一性校验出现的线程安全问题【推荐】各个项目的admin模块在需求中经常会出现要求名称不能重复，即唯一性问题。通常在前台做ajax校验，后台使用select count(1) from table_name where name=?的方式查询数据库。这么做无可厚非，但是在极端的情况下，会出现并发问题。两个线程同时插入一条相同的name，如果没有做并发控制，会导致出现脏数据。如果仅仅是后台系统，那么没有必要加锁去避免，只需要对数据库加上唯一索引，并且再web层或者service层捕获数据异常即可。【正例】： 12345678910111213141516171819202122232425262728293031323334353637//实体类添加唯一索引@Entity@Table(name = \"mns_mail_template\", uniqueConstraints = &#123;@UniqueConstraint(columnNames = &#123;\"templateName\"&#125;)&#125;)public class MailTemplate extends AbstractTemplate &#123; /** * 模板名称 */ @Column(columnDefinition = \"varchar(160) comment '模板名称'\") private String templateName;&#125;//业务代码捕获异常@RequestMapping(value = &#123;\"/saveOrUpdate\"&#125;, method = RequestMethod.POST) @ResponseBody public AjaxResponseVo saveOrUpdate(MailTemplate mailTemplate) &#123; AjaxResponseVo ajaxResponseVo = new AjaxResponseVo(AjaxResponseVo.STATUS_CODE_SUCCESS, \"操作成功\", \"邮件模板定义\", AjaxResponseVo.CALLBACK_TYPE_CLOSE_CURRENT); try &#123; //管理端新增时初始化一些数据 if (Lang.isEmpty(mailTemplate.getId())) &#123; mailTemplate.setValidStatus(MailTemplate.VALID_WAIT); &#125; mailTemplateService.save(mailTemplate); &#125; catch (DataIntegrityViolationException ce) &#123; ajaxResponseVo.setStatusCode(AjaxResponseVo.STATUS_CODE_ERROR); ajaxResponseVo.setMessage(\"模板名称已经存在\"); ajaxResponseVo.setCallbackType(null); logger.error(ce); &#125; catch (Exception e) &#123; ajaxResponseVo.setStatusCode(AjaxResponseVo.STATUS_CODE_ERROR); ajaxResponseVo.setMessage(\"操作失败!\"); ajaxResponseVo.setCallbackType(null); logger.error(e); &#125; return ajaxResponseVo; &#125; 【说明】关于其他一些并发问题,如分布式锁，CAS，不仅仅是一篇文档能够讲解清楚的，需要对开发有很深的理解。 3 余额扣减，库存扣减，积分发放等敏感并发操作【强制】 这一块通常交给有经验的开发来完成，但所有人都需要注意。原则是事务保障，幂等保障等等设计原则。 【反例】 123456//Transaction startUser user = UserDao.findById(\"1\");user.setBalance(user.getBalance()+100.00);...//其他耗时操作UserDao.save(user);//Transaction commit 【正例】 12345678//Transaction startlock...User user = UserDao.findById(\"1\");user.setBalance(user.getBalance()+100.00);...//其他耗时操作UserDao.save(user);release lock...//Transaction commit 并发场景必须加锁，根据业务场景决定到底加什么锁，sychronized，ReentrantLock，version乐观锁，for update悲观锁（不推荐），redis，zookeeper实现的分布式锁等等。 moton使用注意事项1 包的扫描【注意】 每个模块都要扫描自身的项目结构12345678910mail-sms-admin:application.ymlmotan: client-group: sinosoftrpc client-access-log: false server-group: sinosoftrpc server-access-log: false export-port: $&#123;random.int[9001,9999]&#125; zookeeper-host: 127.0.0.1:2181 annotaiong-package: sinosoftgz.message.admin app模块由于将api-impl脱离出了自身的模块，通常还需要扫描api-impl的模块 配置pom.xml依赖 1234&lt;dependency&gt; &lt;groupId&gt;sinosoftgz&lt;/groupId&gt; &lt;artifactId&gt;mail-sms-api-impl&lt;/artifactId&gt;&lt;/dependency&gt; 配置spring ioc扫描 AutoImportConfig.java 123@ComponentScans(&#123; @ComponentScan(basePackages = &#123;\"sinosoftgz.message.app\", \"sinosoftgz.message.api\"&#125;)&#125;) 配置motan扫描 mail-sms-app:application.yml 12345678motan: annotaiong-package: sinosoftgz.message.app,sinosoftgz.message.api client-group: sinosoftrpc client-access-log: true server-group: sinosoftrpc server-access-log: true export-port: $&#123;random.int[9001,9999]&#125; zookeeper-host: localhost:2181 2 motan跨模块传输实体类时懒加载失效【注意】遇到的时候注意一下，由于jpa，hibernate懒加载的问题，因为其内部使用动态代理去实现的懒加载，导致懒加载对象无法被正确的跨模块传输，此时需要进行深拷贝。【正例】： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * 深拷贝OrderMain对象，主要用于防止Hibernate序列化懒加载Session关闭问题 * &lt;p/&gt; * // * @param order * * @return */ public OrderMain cpyOrder(OrderMain from, OrderMain to) &#123; OrderMain orderMainNew = to == null ? new OrderMain() : to; Copys copys = Copys.create(); List&lt;OrderItem&gt; orderItemList = new ArrayList&lt;&gt;(); List&lt;SubOrder&gt; subOrders = new ArrayList&lt;&gt;(); List&lt;OrderGift&gt; orderGifts = new ArrayList&lt;&gt;(); List&lt;OrderMainAttr&gt; orderMainAttrs = new ArrayList&lt;&gt;(); OrderItem orderItemTmp; SubOrder subOrderTmp; OrderGift orderGiftTmp; OrderMainAttr orderMainAttrTmp; copys.from(from).excludes(\"orderItems\", \"subOrders\", \"orderGifts\", \"orderAttrs\").to(orderMainNew).clear(); if (!Lang.isEmpty(from.getOrderItems())) &#123; for (OrderItem i : from.getOrderItems()) &#123; orderItemTmp = new OrderItem(); copys.from(i).excludes(\"order\").to(orderItemTmp).clear(); orderItemTmp.setOrder(orderMainNew); orderItemList.add(orderItemTmp); &#125; orderMainNew.setOrderItems(orderItemList); &#125; SubOrderItem subOrderItem; List&lt;SubOrderItem&gt; subOrderItemList = new ArrayList&lt;&gt;(); if (from.getSubOrders() != null) &#123; for (SubOrder s : from.getSubOrders()) &#123; subOrderTmp = new SubOrder(); copys.from(s).excludes(\"order\", \"subOrderItems\").to(subOrderTmp).clear(); subOrderTmp.setOrder(from); for (SubOrderItem soi : s.getSubOrderItems()) &#123; subOrderItem = new SubOrderItem(); copys.from(soi).excludes(\"order\", \"subOrder\", \"orderItem\").to(subOrderItem).clear(); subOrderItem.setOrder(orderMainNew); subOrderItem.setSubOrder(subOrderTmp); subOrderItemList.add(subOrderItem); if (!Lang.isEmpty(soi.getOrderItem())) &#123; for (OrderItem i : orderMainNew.getOrderItems()) &#123; if (i.getId().equals(soi.getOrderItem().getId())) &#123; subOrderItem.setOrderItem(soi.getOrderItem()); &#125; else &#123; subOrderItem.setOrderItem(soi.getOrderItem()); &#125; &#125; &#125; &#125; subOrderTmp.setSubOrderItems(subOrderItemList); subOrders.add(subOrderTmp); &#125; orderMainNew.setSubOrders(subOrders); &#125; if (from.getOrderGifts() != null) &#123; for (OrderGift og : from.getOrderGifts()) &#123; orderGiftTmp = new OrderGift(); copys.from(og).excludes(\"order\").to(orderGiftTmp).clear(); orderGiftTmp.setOrder(orderMainNew); orderGifts.add(orderGiftTmp); &#125; orderMainNew.setOrderGifts(orderGifts); &#125; if (from.getOrderAttrs() != null) &#123; for (OrderMainAttr attr : from.getOrderAttrs()) &#123; orderMainAttrTmp = new OrderMainAttr(); copys.from(attr).excludes(\"order\").to(orderMainAttrTmp).clear(); orderMainAttrTmp.setOrder(orderMainNew); orderMainAttrs.add(orderMainAttrTmp); &#125; orderMainNew.setOrderAttrs(orderMainAttrs); &#125; return orderMainNew; &#125; 公用常量规范1 模块常量【强制】模块自身公用的常量放置于模块的Constants 类中，以final static的方式声明1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class Constants &#123; public static final String birthdayPattern = \"yyyy-MM-dd\"; //生日格式 public static final String inputTimePattern = \"yyyy-MM-dd HH:mm:ss\"; //录入时间格式 public static class PolicyType &#123; public static final String personal = \"0\"; //个单 public static final String group = \"1\"; //团单 &#125; public static class InsuredNature &#123; public static final String naturePerson = \"1\"; //自然人 public static final String artificialPerson = \"0\"; //法人 &#125; public static class InsuredIdentity &#123; public static final String myself = \"0\"; //本人 &#125; public static class JfeeFlag &#123; public static final String noFeeFlag = \"0\"; //非见费标志 public static final String feeFlag = \"1\"; //见费标志 &#125; public static class ItemKindFlag &#123; public static final String mainRiskFlag = \"1\"; //主险标志 public static final String additionalRiskFlag = \"2\"; //附加险标志 public static final String otherRiskFlag = \"3\"; //其它标志 &#125; public static class CalculateAmountFlag &#123; public static final String calculateFlag = \"Y\"; //计算保额标志 public static final String noCalculateFlag = \"N\"; //不计算保额标志 &#125; public static class LimitGrade &#123; public static final String policyLevel = \"1\"; //限额/免赔保单级别 public static final String clauseLevel = \"2\"; //限额/免赔条款级别 &#125; /** * 批改类型 * * 命名规则：对象（可选）+行为 */ public static class EndorType &#123; public static final String collectivePolicyInsuredModify = \"22\"; //团单变更被保险人 public static final String collectivePolicyInsuredAdd = \"Z1\"; //团单批增被保险人 public static final String collectivePolicyInsuredRemove = \"J1\"; //团单批减被保险人 public static final String surrender = \"04\"; //全单退保 public static final String withdraw = \"05\"; //注销 public static final String insurancePeriodModify = \"06\"; //平移保险期限 public static final String applicantModify = \"H01\"; //更改投保人 public static final String customerModify = \"50\"; //变更客户信息 public static final String insuredModify = \"29\"; //变更被保人职业 public static final String individualPolicyBeneficiaryModify = \"03\"; //变更受益人信息 public static final String engageModify = \"15\"; //变更特别约定 public static final String individualPolicyInsuredModify = \"77\";//个单变更被保人 &#125;&#125; Constants类在一个限界上下文只能有一个，一个限界上下文包含了一整个业务模块（如policy-admin,policy-admin,policy-api,policy-model）构成一个限界上下文 在Constants类中使用静态内部类尽量细化到常量的归属，不要散放 2 项目常量【强制】项目公用的常量放置于util模块的GlobalContants类中，以内部类和final static的方式声明 1234567891011121314151617181920public abstract class GlobalContants &#123; /** * 返回的状态 */ public class ResponseStatus&#123; public static final String SUCCESS = \"success\";//成功 public static final String ERROR = \"error\";//错误 &#125; /** * 响应状态 */ public class ResponseString&#123; public static final String STATUS = \"status\";//状态 public static final String ERROR_CODE = \"error\";// 错误代码 public static final String MESSAGE = \"message\";//消息 public static final String DATA = \"data\";//数据 &#125; ...&#125; 日志规范1 打印日志时不允许拼接字符串【强制】 【反例】log.debug ( “Load No.” + i + “ object, “ + object ); 【正例】log.debug( “Load No.{} object, {}” , i , object ); 字符串的计算是在编译期，日志级别如果是INFO，就等于在浪费机器的性能，无谓的字符串拼接。 2 预防空指针【强制】 【反例】log.debug( “Load student(id={}), name: {}” , id , student.getName() ); 【正例】log.debug( “Load student(id={}), student: {}” , id , student ); 不要在日志中调用对象的方法获取值，除非确保该对象肯定不为 null，否则很有可能会因为日志的问题而导致应用产生空指针异常。实现需要打印日志的实体类的toString方法或者使用JSON.toString 3 输出异常信息 【反例】log.error(e.getMessage,e); log.error(“邮件发送失败，接收人姓名：{} ，e : {}”, username, e); 【正例】log.error(“邮件发送失败，接收人姓名：{}”, username, e); e包含了全部的异常堆栈信息，是e.getMessage的父集，出现异常一定要保证输出堆栈信息。并且要保证exception作为log的重载方法的最后一个参数。 4 Logger声明规范 【正例】Logger logger = LoggerFactory.getLogger(Student.class); 保证某个类的字节码作为日志跟踪标识，方便定位日志的出处。","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/categories/技术杂谈/"}],"tags":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/tags/技术杂谈/"},{"name":"代码规范","slug":"代码规范","permalink":"http://lexburner.github.io/tags/代码规范/"}]},{"title":"博客搬家","slug":"博客搬家","date":"2017-08-22T08:38:44.000Z","updated":"2017-08-23T05:09:40.250Z","comments":true,"path":"2017/08/22/博客搬家/","link":"","permalink":"http://lexburner.github.io/2017/08/22/博客搬家/","excerpt":"","text":"陆陆续续，写博客已经写了有4年多了，之前一直在CSDN维护博客（博客旧址），最近有了点空余时间，使用hexo搭了这个博客，的确比CSDN清爽多了，首先感谢@程序猿DD推荐的icarus模板，国人开发的一个hexo模板，插件支持可能不是很完善，但是样式非常让人喜欢。 作为一个前端弱渣，搭建博客的过程还是遇到了不少的困难。原先是打算直接使用github个人主页作为博客地址，hexo对git有很好的支持，源代码和博客静态页面都托管在了github，master分支放静态页面，hexo分支放源文件。可惜的是国内坑爹的网速,github.io的访问速度不尽如人意（github.com倒还好），于是在宇泽学妹@ntzyz的帮助下，搞了github的hook，本地提交到github时，代理服务器自动向master分支拉取页面，同时设置反向代理和https。由于hexo是静态文件搭建的博客，这种方式可以说是非常合适的。所以，国内的朋友浏览本博客可以直接访问https://www.cnkirito.moe，如果有国外代理的朋友可以直接访问我的github个人主页https://lexburner.github.io。 目前博客功能还不算完善，缺少评论，分享，和一些小插件，以后逐渐完善，不过不影响主要功能。以后这儿就作为我主要更新博客的地方了！","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/categories/技术杂谈/"}],"tags":[]},{"title":"一个DDD指导下的实体类设计案例","slug":"一个DDD指导下的实体类设计案例","date":"2017-08-21T07:59:52.000Z","updated":"2017-08-22T04:31:22.057Z","comments":true,"path":"2017/08/21/一个DDD指导下的实体类设计案例/","link":"","permalink":"http://lexburner.github.io/2017/08/21/一个DDD指导下的实体类设计案例/","excerpt":"1 引子项目开发中的工具类代码总是随着项目发展逐渐变大，在公司诸多的公用代码中，笔者发现了一个简单的，也是经常被使用的类：BaseDomain，引起了我的思考。在我们公司的开发习惯中，数据库实体类通常会继承一个叫做BaseDomain的类，这个类很简单，主要用来填充一些数据库实体公用的属性，它的设计如下：","text":"1 引子项目开发中的工具类代码总是随着项目发展逐渐变大，在公司诸多的公用代码中，笔者发现了一个简单的，也是经常被使用的类：BaseDomain，引起了我的思考。在我们公司的开发习惯中，数据库实体类通常会继承一个叫做BaseDomain的类，这个类很简单，主要用来填充一些数据库实体公用的属性，它的设计如下：1234567891011121314151617181920212223@MappedSuperclass &lt;1&gt;public class BaseDomain &#123; private Boolean deleteFlag; &lt;2&gt; private Date deleteDate; private Date lastUpdateDate; private Date createDate; @Version &lt;3&gt; private Integer version; @PrePersist &lt;4&gt; public void init()&#123; Date now = new Date(); deleteFlag = false; createDate = lastUpdateDate = now; &#125; @PreUpdate &lt;4&gt; public void update()&#123; lastUpdateDate = new Date(); &#125; &#125; 小小的一个类其实还是蕴含了不少的知识点在里面，至少可以包含以下几点： 被其他类继承后，父类的字段不会被忽略，也就意味着子类没有必要自己写这一堆公用的属性了。 逻辑删除标识，业务类的删除必须是这种打标识的行为，不能进行物理删除。值得一提的是，公司原先的该字段被命名成了isDelete，这不符合变量命名的规范，会导致一些序列化框架出现问题，而delete是数据库的保留字，所以本文中用deleteFlag。 使用version作为乐观锁的实现，version的自增以及版本失效异常受@Version该注解的影响，是由框架控制的。 创建日期，更新日期等等属性，在我们使用JPA的save方法后，框架会自动去填充相应的值。 2 发现问题与解决问题这个基类使用的频次是怎么样的呢？every class！是的，公司的每个开发者在新增一个实体类时总是优先写上Xxx extends BaseDomain 。初级开发者总是有什么学什么，他们看到公司原来的代码都是会继承这个类，以及周围的同事也是这么写着，他们甚至不知道version乐观锁的实现，不知道类的创建日期更新日期是在基类中被声明的；高级开发者能够掌握我上面所说的那些技术要点，尽管开发中因此遇到一些不适，但也是尽可能的克服。等等，上面说到添加这个基类后，对开发造成了不适感，这引起了我的思考，下面就来谈谈直观的有哪些不适感以及解决方案。 2.1 没有物理删除，只有逻辑删除真正delete操作不会再出现了,物理删除操作被setDeleteFlag(true)代替。在列表展示中，再也不能使用findAll()操作了，而是需要使用findByDeleteFlagFalse()。更多的数据库查询操作，都要考虑到，deleteFlag=true的那些记录，不应该被影响到。 解决问题：在DDD中，值得推崇的方式是使用specification模式来解决这个问题，对应到实际开发中，也就是JPA的Predicate，或者是熟悉Hibernate的人所了解的Criteria。但不可避免的一点是由于只有逻辑删除，导致了我们的数据库越来越大（解决方法不是没有，正是EventSouring+CQRS架构，这属于DDD的高级实践，本文不进行讨论）。从技术开发角度出发，这的确使得我们的编码变得稍微复杂了一点，但是其业务意义远大于这点开发工作量，所以是值得的。 2.2 级联查询变得麻烦一个会员有多个通信地址，多个银行卡。反映到实体设计，便是这样的： 1234567891011public class Member extends BaseDomain&#123; private String username; @OneToMany private List&lt;MemberAddress&gt; memberAddresses; @OneToMany private List&lt;BankCard&gt; bankCards; &#125; 其中，MemberAddress及BankCard都继承了BaseDomain。使用orm框架自带的级联功能，我们本可以查询出会员信息时，顺带查出其对应的通讯地址列表和银行卡列表。但现在不是那么的美好了，使用级联查询，可能会查询出已经被删除的MemberAddress，BankCard，只能在应用层进行deleteFlag的判断，从而过滤被删除的信息，这无法避免，因为框架不认识逻辑删除标识！ 解决问题：这个问题和2.3节的问题，恰恰是促成我写这篇文章的初衷，这与DDD有着密不可分的关联。DDD将对象划分成了entity（实体）和value object（值对象）。如果仔细分析下上面的业务并且懂一点DDD，你会立刻意识到。Member对象就是一个entity，而MemberAddress以及BankCard则是value object（username也是value object）。value object的一个重要特点，就是作为entity的修饰，从业务角度出发，MemberAddress和BankCard的确是为了更好描述Member信息，而抽象出的一个集合。而value object的另一特性，不可变性，指导了我们，不应该让MemberAddress，BankCard继承BaseDomain。说了这么多，就是想从一个理论的高度，让那些设计一个新实体便继承BaseDomain的人戒掉这个习惯。在value object丧失了deleteFlag，lastUpdateDate等属性后，可能会引发一些的质疑，他们会声称：“数据库里面member_address这张表没有lastUpdateDate字段了，我再也无法得知这条会员地址最后修改的时间了!”。是的，从逻辑意义上看，地址并没有改变，而改变的只是会员自己的地址，这个UpdateDate字段在地址上极为不合理，应该是会员的修改。也就是说lastUpdateDate应该反映到Member上。实际的开发经验告诉我，从前那么多的value object继承了BaseDomain，99%不会使用到其中的相关属性，如果真的需要使用，那么请单独为类添加，而不是继承BaseDomain。其次这些人犯了另一个错误，我们设计一个系统时，应该是entity first，而不应该database first。DDD告诉我们一个软件开发的大忌，到现在2017年，仍然有大帮的人在问：“我要实现xxxx功能，我的数据库应该如何设计？”这些人犯了根本性的错误，就是把软件的目的搞错了，软件研究的是什么？是研究如何使用计算机来解决实际（领域）问题，而不是去研究数据应该如何保存更合理。我的公司中有不少的程序员新人，希望这番话能够帮助那些“步入歧途”的从业人员 “走上正路”。软件设计应该从“数据库驱动”走向“领域驱动”，而DDD的实践经验正是为设计和开发大型复杂的软件系统提供了实践指导。 2.3 乐观锁的尴尬地位再说回BaseDomain中的version字段，由于MemberAddress和BankCard这样的value object也被赋予了乐观锁的行为，这意味着加锁的粒度变小了。DDD的指导下，改动也可以理解为由Member这个根发出，统一由Member中的version来控制，这使锁的粒度变大了。换言之，从技术开发角度，对value object加上version可以允许同时（操作系统级别真正的同时）修改一个用户的地址信息和银行卡信息，甚至是多个银行卡中不同的银行卡，而单独由Member控制，则意味着，系统在同一时刻只能进行单独一项操作。在业务并发的一般角度上考虑，一个用户是不会出现多线程修改行为的。而从软件设计的角度，单独为value object 添加version，破坏了value object的不可变性，若要修改，应当是被整个替换。 解决方案：在一般情况下，请不要为value object添加乐观锁。如果有一个场景下，你的value object需要出现版本控制，那可能有两种情况：1 你的value object是压根不是value object，可能是一个entity 2 聚合根划分错误 ….这，要真是这样源头都弄错了，压根没法聊了对吧 3 总结BaseDomain这样的设计本身并不是我想要强调的重点，但是既然出现了BaseDomain这样的设计，那么它究竟应该被什么样的实体继承，就是需要被考虑的了。DDD下，识别aggregate root，entity，value object，是整个软件设计的核心点，在本文中，判别是否继承BaseDomain的前提，就是这个对象是entity，还是value object。大家都是存在数据库中的，但是地位是不一样的。 本文若有什么不足之处，欢迎DDD爱好者指出。","categories":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"http://lexburner.github.io/categories/领域驱动设计/"}],"tags":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"http://lexburner.github.io/tags/领域驱动设计/"}]},{"title":"使用spring validation完成数据后端校验","slug":"使用spring validation完成数据后端校验","date":"2017-08-16T07:52:52.000Z","updated":"2017-08-22T04:30:17.483Z","comments":true,"path":"2017/08/16/使用spring validation完成数据后端校验/","link":"","permalink":"http://lexburner.github.io/2017/08/16/使用spring validation完成数据后端校验/","excerpt":"前言数据的校验是交互式网站一个不可或缺的功能，前端的js校验可以涵盖大部分的校验职责，如用户名唯一性，生日格式，邮箱格式校验等等常用的校验。但是为了避免用户绕过浏览器，使用http工具直接向后端请求一些违法数据，服务端的数据校验也是必要的，可以防止脏数据落到数据库中，如果数据库中出现一个非法的邮箱格式，也会让运维人员头疼不已。我在之前保险产品研发过程中，系统对数据校验要求比较严格且追求可变性及效率，曾使用drools作为规则引擎，兼任了校验的功能。而在一般的应用，可以使用本文将要介绍的validation来对数据进行校验。 简述JSR303/JSR-349，hibernate validation，spring validation之间的关系。JSR303是一项标准,JSR-349是其的升级版本，添加了一些新特性，他们规定一些校验规范即校验注解，如@Null，@NotNull，@Pattern，他们位于javax.validation.constraints包下，只提供规范不提供实现。而hibernate validation是对这个规范的实践（不要将hibernate和数据库orm框架联系在一起），他提供了相应的实现，并增加了一些其他校验注解，如@Email，@Length，@Range等等，他们位于org.hibernate.validator.constraints包下。而万能的spring为了给开发者提供便捷，对hibernate validation进行了二次封装，显示校验validated bean时，你可以使用spring validation或者hibernate validation，而spring validation另一个特性，便是其在springmvc模块中添加了自动校验，并将校验信息封装进了特定的类中。这无疑便捷了我们的web开发。本文主要介绍在springmvc中自动校验的机制。","text":"前言数据的校验是交互式网站一个不可或缺的功能，前端的js校验可以涵盖大部分的校验职责，如用户名唯一性，生日格式，邮箱格式校验等等常用的校验。但是为了避免用户绕过浏览器，使用http工具直接向后端请求一些违法数据，服务端的数据校验也是必要的，可以防止脏数据落到数据库中，如果数据库中出现一个非法的邮箱格式，也会让运维人员头疼不已。我在之前保险产品研发过程中，系统对数据校验要求比较严格且追求可变性及效率，曾使用drools作为规则引擎，兼任了校验的功能。而在一般的应用，可以使用本文将要介绍的validation来对数据进行校验。 简述JSR303/JSR-349，hibernate validation，spring validation之间的关系。JSR303是一项标准,JSR-349是其的升级版本，添加了一些新特性，他们规定一些校验规范即校验注解，如@Null，@NotNull，@Pattern，他们位于javax.validation.constraints包下，只提供规范不提供实现。而hibernate validation是对这个规范的实践（不要将hibernate和数据库orm框架联系在一起），他提供了相应的实现，并增加了一些其他校验注解，如@Email，@Length，@Range等等，他们位于org.hibernate.validator.constraints包下。而万能的spring为了给开发者提供便捷，对hibernate validation进行了二次封装，显示校验validated bean时，你可以使用spring validation或者hibernate validation，而spring validation另一个特性，便是其在springmvc模块中添加了自动校验，并将校验信息封装进了特定的类中。这无疑便捷了我们的web开发。本文主要介绍在springmvc中自动校验的机制。 引入依赖我们使用maven构建springboot应用来进行demo演示。 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 我们只需要引入spring-boot-starter-web依赖即可，如果查看其子依赖，可以发现如下的依赖： 12345678&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;&lt;/dependency&gt; 验证了我之前的描述，web模块使用了hibernate-validation，并且databind模块也提供了相应的数据绑定功能。 构建启动类无需添加其他注解，一个典型的启动类​1234567@SpringBootApplicationpublic class ValidateApp &#123; public static void main(String[] args) &#123; SpringApplication.run(ValidateApp.class, args); &#125;&#125; 创建需要被校验的实体类1234567891011121314151617public class Foo &#123; @NotBlank private String name; @Min(18) private Integer age; @Pattern(regexp = \"^1(3|4|5|7|8)\\\\d&#123;9&#125;$\",message = \"手机号码格式错误\") @NotBlank(message = \"手机号码不能为空\") private String phone; @Email(message = \"邮箱格式错误\") private String email; //... getter setter&#125; 使用一些比较常用的校验注解，还是比较浅显易懂的，字段上的注解名称即可推断出校验内容，每一个注解都包含了message字段，用于校验失败时作为提示信息，特殊的校验注解，如Pattern（正则校验），还可以自己添加正则表达式。 在@Controller中校验数据springmvc为我们提供了自动封装表单参数的功能，一个添加了参数校验的典型controller如下所示。 123456789101112131415@Controllerpublic class FooController &#123; @RequestMapping(\"/foo\") public String foo(@Validated Foo foo &lt;1&gt;, BindingResult bindingResult &lt;2&gt;) &#123; if(bindingResult.hasErrors())&#123; for (FieldError fieldError : bindingResult.getFieldErrors()) &#123; //... &#125; return \"fail\"; &#125; return \"success\"; &#125;&#125; 值得注意的地方： 参数Foo前需要加上@Validated注解，表明需要spring对其进行校验，而校验的信息会存放到其后的BindingResult中。注意，必须相邻，如果有多个参数需要校验，形式可以如下。foo(@Validated Foo foo, BindingResult fooBindingResult ，@Validated Bar bar, BindingResult barBindingResult);即一个校验类对应一个校验结果。 校验结果会被自动填充，在controller中可以根据业务逻辑来决定具体的操作，如跳转到错误页面。 一个最基本的校验就完成了，总结下框架已经提供了哪些校验：JSR提供的校验注解:12345678910111213@Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Min(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max=, min=) 被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction) 被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past 被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 @Pattern(regex=,flag=) 被注释的元素必须符合指定的正则表达式 Hibernate Validator提供的校验注解： 12345@NotBlank(message =) 验证字符串非null，且长度必须大于0 @Email 被注释的元素必须是电子邮箱地址 @Length(min=,max=) 被注释的字符串的大小必须在指定的范围内 @NotEmpty 被注释的字符串的必须非空 @Range(min=,max=,message=) 被注释的元素必须在合适的范围内 校验实验我们对上面实现的校验入口进行一次测试请求：访问 http://localhost:8080/foo?name=xujingfeng&amp;email=000&amp;age=19 可以得到如下的debug信息： 实验告诉我们，校验结果起了作用。并且，可以发现当发生多个错误，spring validation不会在第一个错误发生后立即停止，而是继续试错，告诉我们所有的错误。debug可以查看到更多丰富的错误信息，这些都是spring validation为我们提供的便捷特性，基本适用于大多数场景。 你可能不满足于简单的校验特性，下面进行一些补充。 分组校验如果同一个类，在不同的使用场景下有不同的校验规则，那么可以使用分组校验。未成年人是不能喝酒的，而在其他场景下我们不做特殊的限制，这个需求如何体现同一个实体，不同的校验规则呢？ 改写注解，添加分组： 12345678Class Foo&#123; @Min(value = 18,groups = &#123;Adult.class&#125;) private Integer age; public interface Adult&#123;&#125; public interface Minor&#123;&#125;&#125; 这样表明，只有在Adult分组下，18岁的限制才会起作用。 Controller层改写： 123456789101112131415161718192021@RequestMapping(\"/drink\")public String drink(@Validated(&#123;Foo.Adult.class&#125;) Foo foo, BindingResult bindingResult) &#123; if(bindingResult.hasErrors())&#123; for (FieldError fieldError : bindingResult.getFieldErrors()) &#123; //... &#125; return \"fail\"; &#125; return \"success\";&#125;@RequestMapping(\"/live\")public String live(@Validated Foo foo, BindingResult bindingResult) &#123; if(bindingResult.hasErrors())&#123; for (FieldError fieldError : bindingResult.getFieldErrors()) &#123; //... &#125; return \"fail\"; &#125; return \"success\";&#125; drink方法限定需要进行Adult校验，而live方法则不做限制。 自定义校验业务需求总是比框架提供的这些简单校验要复杂的多，我们可以自定义校验来满足我们的需求。自定义spring validation非常简单，主要分为两步。 1 自定义校验注解我们尝试添加一个“字符串不能包含空格”的限制。 123456789101112131415161718192021222324@Target(&#123;METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER&#125;)@Retention(RUNTIME)@Documented@Constraint(validatedBy = &#123;CannotHaveBlankValidator.class&#125;)&lt;1&gt;public @interface CannotHaveBlank &#123; //默认错误消息 String message() default \"不能包含空格\"; //分组 Class&lt;?&gt;[] groups() default &#123;&#125;; //负载 Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;; //指定多个时使用 @Target(&#123;FIELD, METHOD, PARAMETER, ANNOTATION_TYPE&#125;) @Retention(RUNTIME) @Documented @interface List &#123; CannotHaveBlank[] value(); &#125;&#125; 我们不需要关注太多东西，使用spring validation的原则便是便捷我们的开发，例如payload，List ，groups，都可以忽略。 自定义注解中指定了这个注解真正的验证者类。 2 编写真正的校验者类 1234567891011121314151617181920212223public class CannotHaveBlankValidator implements &lt;1&gt; ConstraintValidator&lt;CannotHaveBlank, String&gt; &#123; @Override public void initialize(CannotHaveBlank constraintAnnotation) &#123; &#125; @Override public boolean isValid(String value, ConstraintValidatorContext context &lt;2&gt;) &#123; //null时不进行校验 if (value != null &amp;&amp; value.contains(\" \")) &#123; &lt;3&gt; //获取默认提示信息 String defaultConstraintMessageTemplate = context.getDefaultConstraintMessageTemplate(); System.out.println(\"default message :\" + defaultConstraintMessageTemplate); //禁用默认提示信息 context.disableDefaultConstraintViolation(); //设置提示语 context.buildConstraintViolationWithTemplate(\"can not contains blank\").addConstraintViolation(); return false; &#125; return true; &#125;&#125; 所有的验证者都需要实现ConstraintValidator接口，它的接口也很形象，包含一个初始化事件方法，和一个判断是否合法的方法。 1234public interface ConstraintValidator&lt;A extends Annotation, T&gt; &#123; void initialize(A constraintAnnotation); boolean isValid(T value, ConstraintValidatorContext context);&#125; ConstraintValidatorContext 这个上下文包含了认证中所有的信息，我们可以利用这个上下文实现获取默认错误提示信息，禁用错误提示信息，改写错误提示信息等操作。 一些典型校验操作，或许可以对你产生启示作用。 值得注意的一点是，自定义注解可以用在METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER之上，ConstraintValidator的第二个泛型参数T，是需要被校验的类型。 手动校验可能在某些场景下需要我们手动校验，即使用校验器对需要被校验的实体发起validate，同步获得校验结果。理论上我们既可以使用Hibernate Validation提供Validator，也可以使用Spring对其的封装。在spring构建的项目中，提倡使用经过spring封装过后的方法，这里两种方法都介绍下： Hibernate Validation： 123456789Foo foo = new Foo();foo.setAge(22);foo.setEmail(\"000\");ValidatorFactory vf = Validation.buildDefaultValidatorFactory();Validator validator = vf.getValidator();Set&lt;ConstraintViolation&lt;Foo&gt;&gt; set = validator.validate(foo);for (ConstraintViolation&lt;Foo&gt; constraintViolation : set) &#123; System.out.println(constraintViolation.getMessage());&#125; 由于依赖了Hibernate Validation框架，我们需要调用Hibernate相关的工厂方法来获取validator实例，从而校验。 在spring framework文档的Validation相关章节，可以看到如下的描述： Spring provides full support for the Bean Validation API. This includes convenient support for bootstrapping a JSR-303/JSR-349 Bean Validation provider as a Spring bean. This allows for a javax.validation.ValidatorFactory or javax.validation.Validator to be injected wherever validation is needed in your application. Use the LocalValidatorFactoryBean to configure a default Validator as a Spring bean: bean id=”validator” class=”org.springframework.validation.beanvalidation.LocalValidatorFactoryBean” The basic configuration above will trigger Bean Validation to initialize using its default bootstrap mechanism. A JSR-303/JSR-349 provider, such as Hibernate Validator, is expected to be present in the classpath and will be detected automatically. 上面这段话主要描述了spring对validation全面支持JSR-303、JSR-349的标准，并且封装了LocalValidatorFactoryBean作为validator的实现。值得一提的是，这个类的责任其实是非常重大的，他兼容了spring的validation体系和hibernate的validation体系，也可以被开发者直接调用，代替上述的从工厂方法中获取的hibernate validator。由于我们使用了springboot，会触发web模块的自动配置，LocalValidatorFactoryBean已经成为了Validator的默认实现，使用时只需要自动注入即可。 12345678910111213141516@AutowiredValidator globalValidator; &lt;1&gt;@RequestMapping(\"/validate\")public String validate() &#123; Foo foo = new Foo(); foo.setAge(22); foo.setEmail(\"000\"); Set&lt;ConstraintViolation&lt;Foo&gt;&gt; set = globalValidator.validate(foo);&lt;2&gt; for (ConstraintViolation&lt;Foo&gt; constraintViolation : set) &#123; System.out.println(constraintViolation.getMessage()); &#125; return \"success\";&#125; 真正使用过Validator接口的读者会发现有两个接口，一个是位于javax.validation包下，另一个位于org.springframework.validation包下，注意我们这里使用的是前者javax.validation，后者是spring自己内置的校验接口，LocalValidatorFactoryBean同时实现了这两个接口。 此处校验接口最终的实现类便是LocalValidatorFactoryBean。 基于方法校验12345678910111213141516171819202122@RestController@Validated &lt;1&gt;public class BarController &#123; @RequestMapping(\"/bar\") public @NotBlank &lt;2&gt; String bar(@Min(18) Integer age &lt;3&gt;) &#123; System.out.println(\"age : \" + age); return \"\"; &#125; @ExceptionHandler(ConstraintViolationException.class) public Map handleConstraintViolationException(ConstraintViolationException cve)&#123; Set&lt;ConstraintViolation&lt;?&gt;&gt; cves = cve.getConstraintViolations();&lt;4&gt; for (ConstraintViolation&lt;?&gt; constraintViolation : cves) &#123; System.out.println(constraintViolation.getMessage()); &#125; Map map = new HashMap(); map.put(\"errorCode\",500); return map; &#125;&#125; 为类添加@Validated注解 校验方法的返回值和入参 添加一个异常处理器，可以获得没有通过校验的属性相关信息 基于方法的校验，个人不推荐使用，感觉和项目结合的不是很好。 使用校验框架的一些想法理论上spring validation可以实现很多复杂的校验，你甚至可以使你的Validator获取ApplicationContext，获取spring容器中所有的资源，进行诸如数据库校验，注入其他校验工具，完成组合校验（如前后密码一致）等等操作，但是寻求一个易用性和封装复杂性之间的平衡点是我们作为工具使用者应该考虑的，我推崇的方式，是仅仅使用自带的注解和自定义注解，完成一些简单的，可复用的校验。而对于复杂的校验，则包含在业务代码之中，毕竟如用户名是否存在这样的校验，仅仅依靠数据库查询还不够，为了避免并发问题，还是得加上唯一索引之类的额外工作，不是吗？","categories":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"},{"name":"Validation","slug":"Validation","permalink":"http://lexburner.github.io/tags/Validation/"}]},{"title":"Re：从零开始的Spring Security OAuth2（三）","slug":"Re：从零开始的Spring Security OAuth2（三）","date":"2017-08-10T06:22:12.000Z","updated":"2017-08-22T03:19:56.161Z","comments":true,"path":"2017/08/10/Re：从零开始的Spring Security OAuth2（三）/","link":"","permalink":"http://lexburner.github.io/2017/08/10/Re：从零开始的Spring Security OAuth2（三）/","excerpt":"上一篇文章中我们介绍了获取token的流程，这一篇重点分析一下，携带token访问受限资源时，内部的工作流程。 @EnableResourceServer与@EnableAuthorizationServer还记得我们在第一节中就介绍过了OAuth2的两个核心概念，资源服务器与身份认证服务器。我们对两个注解进行配置的同时，到底触发了内部的什么相关配置呢？ 上一篇文章重点介绍的其实是与身份认证相关的流程，即如果获取token，而本节要分析的携带token访问受限资源，自然便是与@EnableResourceServer相关的资源服务器配置了。 我们注意到其相关配置类是ResourceServerConfigurer，内部关联了ResourceServerSecurityConfigurer和HttpSecurity。前者与资源安全配置相关，后者与http安全配置相关。（类名比较类似，注意区分，以Adapter结尾的是适配器，以Configurer结尾的是配置器，以Builder结尾的是建造器，他们分别代表不同的设计模式，对设计模式有所了解可以更加方便理解其设计思路） 1234567891011public class ResourceServerConfigurerAdapter implements ResourceServerConfigurer &#123; @Override public void configure(ResourceServerSecurityConfigurer resources &lt;1&gt; ) throws Exception &#123; &#125; @Override public void configure(HttpSecurity http) throws Exception &#123; http.authorizeRequests().anyRequest().authenticated(); &#125;&#125; ResourceServerSecurityConfigurer显然便是我们分析的重点了。","text":"上一篇文章中我们介绍了获取token的流程，这一篇重点分析一下，携带token访问受限资源时，内部的工作流程。 @EnableResourceServer与@EnableAuthorizationServer还记得我们在第一节中就介绍过了OAuth2的两个核心概念，资源服务器与身份认证服务器。我们对两个注解进行配置的同时，到底触发了内部的什么相关配置呢？ 上一篇文章重点介绍的其实是与身份认证相关的流程，即如果获取token，而本节要分析的携带token访问受限资源，自然便是与@EnableResourceServer相关的资源服务器配置了。 我们注意到其相关配置类是ResourceServerConfigurer，内部关联了ResourceServerSecurityConfigurer和HttpSecurity。前者与资源安全配置相关，后者与http安全配置相关。（类名比较类似，注意区分，以Adapter结尾的是适配器，以Configurer结尾的是配置器，以Builder结尾的是建造器，他们分别代表不同的设计模式，对设计模式有所了解可以更加方便理解其设计思路） 1234567891011public class ResourceServerConfigurerAdapter implements ResourceServerConfigurer &#123; @Override public void configure(ResourceServerSecurityConfigurer resources &lt;1&gt; ) throws Exception &#123; &#125; @Override public void configure(HttpSecurity http) throws Exception &#123; http.authorizeRequests().anyRequest().authenticated(); &#125;&#125; ResourceServerSecurityConfigurer显然便是我们分析的重点了。 ResourceServerSecurityConfigurer（了解）其核心配置如下所示： 123456789101112131415161718192021222324public void configure(HttpSecurity http) throws Exception &#123; AuthenticationManager oauthAuthenticationManager = oauthAuthenticationManager(http); resourcesServerFilter = new OAuth2AuthenticationProcessingFilter();//&lt;1&gt; resourcesServerFilter.setAuthenticationEntryPoint(authenticationEntryPoint); resourcesServerFilter.setAuthenticationManager(oauthAuthenticationManager);//&lt;2&gt; if (eventPublisher != null) &#123; resourcesServerFilter.setAuthenticationEventPublisher(eventPublisher); &#125; if (tokenExtractor != null) &#123; resourcesServerFilter.setTokenExtractor(tokenExtractor);//&lt;3&gt; &#125; resourcesServerFilter = postProcess(resourcesServerFilter); resourcesServerFilter.setStateless(stateless); // @formatter:off http .authorizeRequests().expressionHandler(expressionHandler) .and() .addFilterBefore(resourcesServerFilter, AbstractPreAuthenticatedProcessingFilter.class) .exceptionHandling() .accessDeniedHandler(accessDeniedHandler)//&lt;4&gt; .authenticationEntryPoint(authenticationEntryPoint); // @formatter:on&#125; 这段是整个oauth2与HttpSecurity相关的核心配置，其中有非常多的注意点，顺带的都强调一下： 创建OAuth2AuthenticationProcessingFilter，即下一节所要介绍的OAuth2核心过滤器。 为OAuth2AuthenticationProcessingFilter提供固定的AuthenticationManager即OAuth2AuthenticationManager，它并没有将OAuth2AuthenticationManager添加到spring的容器中，不然可能会影响spring security的普通认证流程（非oauth2请求），只有被OAuth2AuthenticationProcessingFilter拦截到的oauth2相关请求才被特殊的身份认证器处理。 设置了TokenExtractor默认的实现—-BearerTokenExtractor，这个类在下一节介绍。 相关的异常处理器，可以重写相关实现，达到自定义异常的目的。 还记得我们在一开始的配置中配置了资源服务器，是它触发了相关的配置。123@Configuration@EnableResourceServerprotected static class ResourceServerConfiguration extends ResourceServerConfigurerAdapter &#123;&#125; 核心过滤器 OAuth2AuthenticationProcessingFilter（掌握）回顾一下我们之前是如何携带token访问受限资源的：http://localhost:8080/order/1?access_token=950a7cc9-5a8a-42c9-a693-40e817b1a4b0唯一的身份凭证，便是这个access_token，携带它进行访问，会进入OAuth2AuthenticationProcessingFilter之中，其核心代码如下： 1234567891011121314151617181920212223242526272829303132public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain)&#123; final HttpServletRequest request = (HttpServletRequest) req; final HttpServletResponse response = (HttpServletResponse) res; try &#123; //从请求中取出身份信息，即access_token Authentication authentication = tokenExtractor.extract(request); if (authentication == null) &#123; ... &#125; else &#123; request.setAttribute(OAuth2AuthenticationDetails.ACCESS_TOKEN_VALUE, authentication.getPrincipal()); if (authentication instanceof AbstractAuthenticationToken) &#123; AbstractAuthenticationToken needsDetails = (AbstractAuthenticationToken) authentication; needsDetails.setDetails(authenticationDetailsSource.buildDetails(request)); &#125; //认证身份 Authentication authResult = authenticationManager.authenticate(authentication); ... eventPublisher.publishAuthenticationSuccess(authResult); //将身份信息绑定到SecurityContextHolder中 SecurityContextHolder.getContext().setAuthentication(authResult); &#125; &#125; catch (OAuth2Exception failed) &#123; ... return; &#125; chain.doFilter(request, response);&#125; 整个过滤器便是oauth2身份鉴定的关键，在源码中，对这个类有一段如下的描述 A pre-authentication filter for OAuth2 protected resources. Extracts an OAuth2 token from the incoming request and uses it to populate the Spring Security context with an {@link OAuth2Authentication} (if used in conjunction with an {@link OAuth2AuthenticationManager}). OAuth2保护资源的预先认证过滤器。如果与OAuth2AuthenticationManager结合使用，则会从到来的请求之中提取一个OAuth2 token，之后使用OAuth2Authentication来填充Spring Security上下文。 其中涉及到了两个关键的类TokenExtractor，AuthenticationManager。相信后者这个接口大家已经不陌生，但前面这个类之前还未出现在我们的视野中。 OAuth2的身份管理器–OAuth2AuthenticationManager（掌握）在之前的OAuth2核心过滤器中出现的AuthenticationManager其实在我们意料之中，携带access_token必定得经过身份认证，但是在我们debug进入其中后，发现了一个出乎意料的事，AuthenticationManager的实现类并不是我们在前面文章中聊到的常用实现类ProviderManager，而是OAuth2AuthenticationManager。 图1 新的AuthenticationManager实现类OAuth2AuthenticationManager 回顾我们第一篇文章的配置，压根没有出现过这个OAuth2AuthenticationManager，并且它脱离了我们熟悉的认证流程（第二篇文章中的认证管理器UML图是一张经典的spring security结构类图），它直接重写了容器的顶级身份认证接口，内部维护了一个ClientDetailService和ResourceServerTokenServices，这两个核心类在 Re：从零开始的Spring Security Oauth2（二）有分析过。在ResourceServerSecurityConfigurer的小节中我们已经知晓了它是如何被框架自动配置的，这里要强调的是OAuth2AuthenticationManager是密切与token认证相关的，而不是与获取token密切相关的。 其判别身份的关键代码如下： 123456789101112131415161718public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; ... String token = (String) authentication.getPrincipal(); //最终还是借助tokenServices根据token加载身份信息 OAuth2Authentication auth = tokenServices.loadAuthentication(token); ... checkClientDetails(auth); if (authentication.getDetails() instanceof OAuth2AuthenticationDetails) &#123; OAuth2AuthenticationDetails details = (OAuth2AuthenticationDetails) authentication.getDetails(); ... &#125; auth.setDetails(authentication.getDetails()); auth.setAuthenticated(true); return auth;&#125; 说到tokenServices这个密切与token相关的接口，这里要强调下，避免产生误解。tokenServices分为两类，一个是用在AuthenticationServer端，第二篇文章中介绍的 123456789public interface AuthorizationServerTokenServices &#123; //创建token OAuth2AccessToken createAccessToken(OAuth2Authentication authentication) throws AuthenticationException; //刷新token OAuth2AccessToken refreshAccessToken(String refreshToken, TokenRequest tokenRequest) throws AuthenticationException; //获取token OAuth2AccessToken getAccessToken(OAuth2Authentication authentication);&#125; 而在ResourceServer端有自己的tokenServices接口： 12345678public interface ResourceServerTokenServices &#123; //根据accessToken加载客户端信息 OAuth2Authentication loadAuthentication(String accessToken) throws AuthenticationException, InvalidTokenException; //根据accessToken获取完整的访问令牌详细信息。 OAuth2AccessToken readAccessToken(String accessToken);&#125; 具体内部如何加载，和AuthorizationServer大同小异，只是从tokenStore中取出相应身份的流程有点区别，不再详细看实现类了。 TokenExtractor（了解）这个接口只有一个实现类，而且代码非常简单 1234567891011121314151617181920212223242526272829303132333435363738public class BearerTokenExtractor implements TokenExtractor &#123; private final static Log logger = LogFactory.getLog(BearerTokenExtractor.class); @Override public Authentication extract(HttpServletRequest request) &#123; String tokenValue = extractToken(request); if (tokenValue != null) &#123; PreAuthenticatedAuthenticationToken authentication = new PreAuthenticatedAuthenticationToken(tokenValue, \"\"); return authentication; &#125; return null; &#125; protected String extractToken(HttpServletRequest request) &#123; // first check the header... String token = extractHeaderToken(request); // bearer type allows a request parameter as well if (token == null) &#123; ... //从requestParameter中获取token &#125; return token; &#125;/** * Extract the OAuth bearer token from a header. */ protected String extractHeaderToken(HttpServletRequest request) &#123; Enumeration&lt;String&gt; headers = request.getHeaders(\"Authorization\"); while (headers.hasMoreElements()) &#123; // typically there is only one (most servers enforce that) ... //从Header中获取token &#125; return null; &#125;&#125; 它的作用在于分离出请求中包含的token。也启示了我们可以使用多种方式携带token。1 在Header中携带 123http://localhost:8080/order/1Header：Authentication：Bearer f732723d-af7f-41bb-bd06-2636ab2be135 2 拼接在url中作为requestParam 1http://localhost:8080/order/1?access_token=f732723d-af7f-41bb-bd06-2636ab2be135 3 在form表单中携带 123http://localhost:8080/order/1form param：access_token=f732723d-af7f-41bb-bd06-2636ab2be135 异常处理OAuth2在资源服务器端的异常处理不算特别完善，但基本够用，如果想要重写异常机制，可以直接替换掉相关的Handler，如权限相关的AccessDeniedHandler。具体的配置应该在@EnableResourceServer中被覆盖，这是适配器+配置器的好处。 总结到这儿，Spring Security OAuth2的整个内部流程就算是分析结束了。本系列的文章只能算是揭示一个大概的流程，重点还是介绍相关设计+接口，想要了解更多的细节，需要自己去翻看源码，研究各个实现类。在分析源码过程中总结出的一点经验，与君共勉： 先掌握宏观，如研究UML类图，搞清楚关联 分析顶级接口，设计是面向接口的，不重要的部分，具体实现类甚至都可以忽略 学会对比，如ResourceServer和AuthenticationServer是一种对称的设计，整个框架内部的类非常多，但分门别类的记忆，会加深记忆。如ResourceServerTokenServices ，AuthenticationServerTokenServices就一定是作用相关，但所属领域不同的两个接口 熟悉设计模式，spring中涉及了大量的设计模式，在框架的设计中也是遵循着设计模式的规范，如以Adapter结尾，便是运用了适配器模式；以Factory结尾，便是运用了适配器模式；Template结尾，便是运用了模板方法模式；Builder结尾，便是运用了建造者模式… 一点自己的理解：对源码的理解和灵感，这一切都建立自身的编码经验之上，自己遵循规范便能更好的理解别人同样遵守规范的代码。相对的，阅读好的源码，也能帮助我们自身提升编码规范。","categories":[{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","permalink":"http://lexburner.github.io/categories/Spring-Security-OAuth2/"}],"tags":[{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","permalink":"http://lexburner.github.io/tags/Spring-Security-OAuth2/"}]},{"title":"Re：从零开始的Spring Security OAuth2（二）","slug":"Re：从零开始的Spring Security OAuth2（二）","date":"2017-08-09T06:58:52.000Z","updated":"2017-08-22T04:27:58.368Z","comments":true,"path":"2017/08/09/Re：从零开始的Spring Security OAuth2（二）/","link":"","permalink":"http://lexburner.github.io/2017/08/09/Re：从零开始的Spring Security OAuth2（二）/","excerpt":"本文开始从源码的层面，讲解一些Spring Security Oauth2的认证流程。本文较长，适合在空余时间段观看。且涉及了较多的源码，非关键性代码以…代替。 准备工作首先开启debug信息： 123logging: level: org.springframework: DEBUG 可以完整的看到内部的运转流程。 client模式稍微简单一些，使用client模式获取token http://localhost:8080/oauth/token?client_id=client_1&amp;client_secret=123456&amp;scope=select&amp;grant_type=client_credentials 由于debug信息太多了，我简单按照顺序列了一下关键的几个类： 1234ClientCredentialsTokenEndpointFilterDaoAuthenticationProviderTokenEndpointTokenGranter","text":"本文开始从源码的层面，讲解一些Spring Security Oauth2的认证流程。本文较长，适合在空余时间段观看。且涉及了较多的源码，非关键性代码以…代替。 准备工作首先开启debug信息： 123logging: level: org.springframework: DEBUG 可以完整的看到内部的运转流程。 client模式稍微简单一些，使用client模式获取token http://localhost:8080/oauth/token?client_id=client_1&amp;client_secret=123456&amp;scope=select&amp;grant_type=client_credentials 由于debug信息太多了，我简单按照顺序列了一下关键的几个类： 1234ClientCredentialsTokenEndpointFilterDaoAuthenticationProviderTokenEndpointTokenGranter @EnableAuthorizationServer上一篇博客中我们尝试使用了password模式和client模式，有一个比较关键的endpoint：/oauth/token。从这个入口开始分析，spring security oauth2内部是如何生成token的。获取token，与第一篇文章中的两个重要概念之一有关，也就是AuthorizationServer与ResourceServer中的AuthorizationServer。 在之前的配置中 123@Configuration@EnableAuthorizationServerprotected static class AuthorizationServerConfiguration extends AuthorizationServerConfigurerAdapter &#123;&#125; 出现了AuthorizationServerConfigurerAdapter 关键类，他关联了三个重要的配置类，分别是 1234567891011121314public class AuthorizationServerConfigurerAdapter implements AuthorizationServerConfigurer &#123; @Override public void configure(AuthorizationServerSecurityConfigurer security &lt;1&gt;) throws Exception&#123; &#125; @Override public void configure(ClientDetailsServiceConfigurer clients &lt;2&gt;) throws Exception &#123; &#125; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints &lt;3&gt;) throws Exception &#123; &#125;&#125; 配置AuthorizationServer安全认证的相关信息，创建ClientCredentialsTokenEndpointFilter核心过滤器 配置OAuth2的客户端相关信息 配置AuthorizationServerEndpointsConfigurer众多相关类，包括配置身份认证器，配置认证方式，TokenStore，TokenGranter，OAuth2RequestFactory 我们逐步分析其中关键的类 客户端身份认证核心过滤器ClientCredentialsTokenEndpointFilter（掌握）截取关键的代码，可以分析出大概的流程在请求到达/oauth/token之前经过了ClientCredentialsTokenEndpointFilter这个过滤器，关键方法如下 1234567891011121314public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException, IOException, ServletException &#123; ... String clientId = request.getParameter(\"client_id\"); String clientSecret = request.getParameter(\"client_secret\"); ... clientId = clientId.trim(); UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken(clientId, clientSecret); return this.getAuthenticationManager().authenticate(authRequest);&#125; 顶级身份管理者AuthenticationManager（掌握）用来从请求中获取client_id,client_secret，组装成一个UsernamePasswordAuthenticationToken作为身份标识，使用容器中的顶级身份管理器AuthenticationManager去进行身份认证（AuthenticationManager的实现类一般是ProviderManager。而ProviderManager内部维护了一个List,真正的身份认证是由一系列AuthenticationProvider去完成。而AuthenticationProvider的常用实现类则是DaoAuthenticationProvider，DaoAuthenticationProvider内部又聚合了一个UserDetailsService接口，UserDetailsService才是获取用户详细信息的最终接口，而我们上一篇文章中在内存中配置用户，就是使用了UserDetailsService的一个实现类InMemoryUserDetailsManager）。UML类图可以大概理解下这些类的关系，省略了授权部分。 图1 认证相关UML类图可能机智的读者会发现一个问题，我前面一篇文章已经提到了client模式是不存在“用户”的概念的，那么这里的身份认证是在认证什么呢？debug可以发现UserDetailsService的实现被适配成了ClientDetailsUserDetailsService，这个设计是将client客户端的信息（client_id,client_secret）适配成用户的信息(username,password)，这样我们的认证流程就不需要修改了。经过ClientCredentialsTokenEndpointFilter之后，身份信息已经得到了AuthenticationManager的验证。接着便到达了TokenEndpoint。## Token处理端点TokenEndpoint（掌握）前面的两个ClientCredentialsTokenEndpointFilter和AuthenticationManager可以理解为一些前置校验，和身份封装，而这个类一看名字就知道和我们的token是密切相关的。1234567891011121314151617181920@FrameworkEndpointpublic class TokenEndpoint extends AbstractEndpoint &#123; @RequestMapping(value = \"/oauth/token\", method=RequestMethod.POST) public ResponseEntity&lt;OAuth2AccessToken&gt; postAccessToken(Principal principal, @RequestParam Map&lt;String, String&gt; parameters) throws HttpRequestMethodNotSupportedException &#123; ... String clientId = getClientId(principal); ClientDetails authenticatedClient = getClientDetailsService().loadClientByClientId(clientId);//&lt;1&gt; ... TokenRequest tokenRequest = getOAuth2RequestFactory().createTokenRequest(parameters, authenticatedClient);//&lt;2&gt; ... OAuth2AccessToken token = getTokenGranter().grant(tokenRequest.getGrantType(), tokenRequest);//&lt;3&gt; ... return getResponse(token); &#125; private TokenGranter tokenGranter;&#125; 加载客户端信息 结合请求信息，创建TokenRequest 将TokenRequest传递给TokenGranter颁发token 省略了一些校验代码之后，真正的/oauth/token端点暴露在了我们眼前，其中方法参数中的Principal经过之前的过滤器，已经被填充了相关的信息，而方法的内部则是依赖了一个TokenGranter 来颁发token。其中OAuth2AccessToken的实现类DefaultOAuth2AccessToken就是最终在控制台得到的token序列化之前的原始类:​12345678910public class DefaultOAuth2AccessToken implements Serializable, OAuth2AccessToken &#123; private static final long serialVersionUID = 914967629530462926L; private String value; private Date expiration; private String tokenType = BEARER_TYPE.toLowerCase(); private OAuth2RefreshToken refreshToken; private Set&lt;String&gt; scope; private Map&lt;String, Object&gt; additionalInformation = Collections.emptyMap(); //getter,setter&#125;1234567891011121314@org.codehaus.jackson.map.annotate.JsonSerialize(using = OAuth2AccessTokenJackson1Serializer.class)@org.codehaus.jackson.map.annotate.JsonDeserialize(using = OAuth2AccessTokenJackson1Deserializer.class)@com.fasterxml.jackson.databind.annotation.JsonSerialize(using = OAuth2AccessTokenJackson2Serializer.class)@com.fasterxml.jackson.databind.annotation.JsonDeserialize(using = OAuth2AccessTokenJackson2Deserializer.class)public interface OAuth2AccessToken &#123; public static String BEARER_TYPE = \"Bearer\"; public static String OAUTH2_TYPE = \"OAuth2\"; public static String ACCESS_TOKEN = \"access_token\"; public static String TOKEN_TYPE = \"token_type\"; public static String EXPIRES_IN = \"expires_in\"; public static String REFRESH_TOKEN = \"refresh_token\"; public static String SCOPE = \"scope\"; ...&#125;一个典型的样例token响应,如下所示，就是上述类序列化后的结果：1234567&#123; \"access_token\":\"950a7cc9-5a8a-42c9-a693-40e817b1a4b0\", \"token_type\":\"bearer\", \"refresh_token\":\"773a0fcd-6023-45f8-8848-e141296cb3cb\", \"expires_in\":27036, \"scope\":\"select\" &#125;## TokenGranter（掌握）先从UML类图对TokenGranter接口的设计有一个宏观的认识图2 TokenGranter相关UML类图 TokenGranter的设计思路是使用CompositeTokenGranter管理一个List列表，每一种grantType对应一个具体的真正授权者，在debug过程中可以发现CompositeTokenGranter 内部就是在循环调用五种TokenGranter实现类的grant方法，而granter内部则是通过grantType来区分是否是各自的授权类型。 123456789101112131415161718public class CompositeTokenGranter implements TokenGranter &#123; private final List&lt;TokenGranter&gt; tokenGranters; public CompositeTokenGranter(List&lt;TokenGranter&gt; tokenGranters) &#123; this.tokenGranters = new ArrayList&lt;TokenGranter&gt;(tokenGranters); &#125; public OAuth2AccessToken grant(String grantType, TokenRequest tokenRequest) &#123; for (TokenGranter granter : tokenGranters) &#123; OAuth2AccessToken grant = granter.grant(grantType, tokenRequest); if (grant!=null) &#123; return grant; &#125; &#125; return null; &#125;&#125; 五种类型分别是： ResourceOwnerPasswordTokenGranter ==&gt; password密码模式 AuthorizationCodeTokenGranter ==&gt; authorization_code授权码模式 ClientCredentialsTokenGranter ==&gt; client_credentials客户端模式 ImplicitTokenGranter ==&gt; implicit简化模式 RefreshTokenGranter ==&gt;refresh_token 刷新token专用 以客户端模式为例，思考如何产生token的，则需要继续研究5种授权者的抽象类：AbstractTokenGranter 123456789101112131415161718192021222324252627282930313233343536public abstract class AbstractTokenGranter implements TokenGranter &#123; protected final Log logger = LogFactory.getLog(getClass()); //与token相关的service，重点 private final AuthorizationServerTokenServices tokenServices; //与clientDetails相关的service，重点 private final ClientDetailsService clientDetailsService; //创建oauth2Request的工厂，重点 private final OAuth2RequestFactory requestFactory; private final String grantType; ... public OAuth2AccessToken grant(String grantType, TokenRequest tokenRequest) &#123; ... String clientId = tokenRequest.getClientId(); ClientDetails client = clientDetailsService.loadClientByClientId(clientId); validateGrantType(grantType, client); logger.debug(\"Getting access token for: \" + clientId); return getAccessToken(client, tokenRequest); &#125; protected OAuth2AccessToken getAccessToken(ClientDetails client, TokenRequest tokenRequest) &#123; return tokenServices.createAccessToken(getOAuth2Authentication(client, tokenRequest)); &#125; protected OAuth2Authentication getOAuth2Authentication(ClientDetails client, TokenRequest tokenRequest) &#123; OAuth2Request storedOAuth2Request = requestFactory.createOAuth2Request(client, tokenRequest); return new OAuth2Authentication(storedOAuth2Request, null); &#125; ...&#125; 回过头去看TokenEndpoint中，正是调用了这里的三个重要的类变量的相关方法。由于篇幅限制，不能延展太多，不然没完没了，所以重点分析下AuthorizationServerTokenServices是何方神圣。 AuthorizationServerTokenServices（了解）AuthorizationServer端的token操作service，接口设计如下： 12345678910public interface AuthorizationServerTokenServices &#123; //创建token OAuth2AccessToken createAccessToken(OAuth2Authentication authentication) throws AuthenticationException; //刷新token OAuth2AccessToken refreshAccessToken(String refreshToken, TokenRequest tokenRequest) throws AuthenticationException; //获取token OAuth2AccessToken getAccessToken(OAuth2Authentication authentication);&#125; 在默认的实现类DefaultTokenServices中，可以看到token是如何产生的，并且了解了框架对token进行哪些信息的关联。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Transactionalpublic OAuth2AccessToken createAccessToken(OAuth2Authentication authentication) throws AuthenticationException &#123; OAuth2AccessToken existingAccessToken = tokenStore.getAccessToken(authentication); OAuth2RefreshToken refreshToken = null; if (existingAccessToken != null) &#123; if (existingAccessToken.isExpired()) &#123; if (existingAccessToken.getRefreshToken() != null) &#123; refreshToken = existingAccessToken.getRefreshToken(); // The token store could remove the refresh token when the // access token is removed, but we want to // be sure... tokenStore.removeRefreshToken(refreshToken); &#125; tokenStore.removeAccessToken(existingAccessToken); &#125; else &#123; // Re-store the access token in case the authentication has changed tokenStore.storeAccessToken(existingAccessToken, authentication); return existingAccessToken; &#125; &#125; // Only create a new refresh token if there wasn't an existing one // associated with an expired access token. // Clients might be holding existing refresh tokens, so we re-use it in // the case that the old access token // expired. if (refreshToken == null) &#123; refreshToken = createRefreshToken(authentication); &#125; // But the refresh token itself might need to be re-issued if it has // expired. else if (refreshToken instanceof ExpiringOAuth2RefreshToken) &#123; ExpiringOAuth2RefreshToken expiring = (ExpiringOAuth2RefreshToken) refreshToken; if (System.currentTimeMillis() &gt; expiring.getExpiration().getTime()) &#123; refreshToken = createRefreshToken(authentication); &#125; &#125; OAuth2AccessToken accessToken = createAccessToken(authentication, refreshToken); tokenStore.storeAccessToken(accessToken, authentication); // In case it was modified refreshToken = accessToken.getRefreshToken(); if (refreshToken != null) &#123; tokenStore.storeRefreshToken(refreshToken, authentication); &#125; return accessToken;&#125; 简单总结一下AuthorizationServerTokenServices的作用，他提供了创建token，刷新token，获取token的实现。在创建token时，他会调用tokenStore对产生的token和相关信息存储到对应的实现类中，可以是redis，数据库，内存，jwt。 总结本篇总结了使用客户端模式获取Token时，spring security oauth2内部的运作流程，重点是在分析AuthenticationServer相关的类。其他模式有一定的不同，但抽象功能是固定的，只是具体的实现类会被相应地替换。阅读spring的源码，会发现它的设计中出现了非常多的抽象接口，这对我们理清楚内部工作流程产生了不小的困扰，我的方式是可以借助UML类图，先从宏观理清楚作者的设计思路，这会让我们的分析事半功倍。 下一篇文章重点分析用户携带token访问受限资源时，spring security oauth2内部的工作流程。即ResourceServer相关的类。","categories":[{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","permalink":"http://lexburner.github.io/categories/Spring-Security-OAuth2/"}],"tags":[{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","permalink":"http://lexburner.github.io/tags/Spring-Security-OAuth2/"}]},{"title":"Re：从零开始的Spring Security OAuth2（一）","slug":"Re：从零开始的Spring Security OAuth2（一）","date":"2017-08-08T07:16:52.000Z","updated":"2017-08-22T04:27:03.734Z","comments":true,"path":"2017/08/08/Re：从零开始的Spring Security OAuth2（一）/","link":"","permalink":"http://lexburner.github.io/2017/08/08/Re：从零开始的Spring Security OAuth2（一）/","excerpt":"##前言今天来聊聊一个接口对接的场景，A厂家有一套HTTP接口需要提供给B厂家使用，由于是外网环境，所以需要有一套安全机制保障，这个时候oauth2就可以作为一个方案。 关于oauth2，其实是一个规范，本文重点讲解spring对他进行的实现，如果你还不清楚授权服务器，资源服务器，认证授权等基础概念，可以移步理解OAuth 2.0 - 阮一峰，这是一篇对于oauth2很好的科普文章。 需要对spring security有一定的配置使用经验，用户认证这一块，spring security oauth2建立在spring security的基础之上。第一篇文章主要是讲解使用springboot搭建一个简易的授权，资源服务器，在文末会给出具体代码的github地址。后续文章会进行spring security oauth2的相关源码分析。java中的安全框架如shrio，已经有跟我学shiro - 开涛，非常成体系地，深入浅出地讲解了apache的这个开源安全框架，但是spring security包括oauth2一直没有成体系的文章，学习它们大多依赖于较少的官方文档，理解一下基本的使用配置；通过零散的博客，了解一下他人的使用经验；打断点，分析内部的工作流程；看源码中的接口设计，以及注释，了解设计者的用意。spring的各个框架都运用了很多的设计模式，在学习源码的过程中，也大概了解了一些套路。spring也在必要的地方添加了适当的注释，避免了源码阅读者对于一些细节设计的理解产生偏差，让我更加感叹，spring不仅仅是一个工具框架，更像是一个艺术品。","text":"##前言今天来聊聊一个接口对接的场景，A厂家有一套HTTP接口需要提供给B厂家使用，由于是外网环境，所以需要有一套安全机制保障，这个时候oauth2就可以作为一个方案。 关于oauth2，其实是一个规范，本文重点讲解spring对他进行的实现，如果你还不清楚授权服务器，资源服务器，认证授权等基础概念，可以移步理解OAuth 2.0 - 阮一峰，这是一篇对于oauth2很好的科普文章。 需要对spring security有一定的配置使用经验，用户认证这一块，spring security oauth2建立在spring security的基础之上。第一篇文章主要是讲解使用springboot搭建一个简易的授权，资源服务器，在文末会给出具体代码的github地址。后续文章会进行spring security oauth2的相关源码分析。java中的安全框架如shrio，已经有跟我学shiro - 开涛，非常成体系地，深入浅出地讲解了apache的这个开源安全框架，但是spring security包括oauth2一直没有成体系的文章，学习它们大多依赖于较少的官方文档，理解一下基本的使用配置；通过零散的博客，了解一下他人的使用经验；打断点，分析内部的工作流程；看源码中的接口设计，以及注释，了解设计者的用意。spring的各个框架都运用了很多的设计模式，在学习源码的过程中，也大概了解了一些套路。spring也在必要的地方添加了适当的注释，避免了源码阅读者对于一些细节设计的理解产生偏差，让我更加感叹，spring不仅仅是一个工具框架，更像是一个艺术品。 概述使用oauth2保护你的应用，可以分为简易的分为三个步骤 配置资源服务器 配置认证服务器 配置spring security 前两点是oauth2的主体内容，但前面我已经描述过了，spring security oauth2是建立在spring security基础之上的，所以有一些体系是公用的。 oauth2根据使用场景不同，分成了4种模式 授权码模式（authorization code） 简化模式（implicit） 密码模式（resource owner password credentials） 客户端模式（client credentials） 本文重点讲解接口对接中常使用的密码模式（以下简称password模式）和客户端模式（以下简称client模式）。授权码模式使用到了回调地址，是最为复杂的方式，通常网站中经常出现的微博，qq第三方登录，都会采用这个形式。简化模式不常用。 项目准备主要的maven依赖如下 12345678910111213141516171819&lt;!-- 注意是starter,自动配置 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 不是starter,手动配置 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security.oauth&lt;/groupId&gt; &lt;artifactId&gt;spring-security-oauth2&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 将token存储在redis中 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 我们给自己先定个目标，要干什么事？既然说到保护应用，那必须得先有一些资源，我们创建一个endpoint作为提供给外部的接口：123456789101112131415161718@RestControllerpublic class TestEndpoints &#123; @GetMapping(\"/product/&#123;id&#125;\") public String getProduct(@PathVariable String id) &#123; //for debug Authentication authentication = SecurityContextHolder.getContext().getAuthentication(); return \"product id : \" + id; &#125; @GetMapping(\"/order/&#123;id&#125;\") public String getOrder(@PathVariable String id) &#123; //for debug Authentication authentication = SecurityContextHolder.getContext().getAuthentication(); return \"order id : \" + id; &#125;&#125; 暴露一个商品查询接口，后续不做安全限制，一个订单查询接口，后续添加访问控制。 配置资源服务器和授权服务器由于是两个oauth2的核心配置，我们放到一个配置类中。为了方便下载代码直接运行，我这里将客户端信息放到了内存中，生产中可以配置到数据库中。token的存储一般选择使用redis，一是性能比较好，二是自动过期的机制，符合token的特性。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475@Configurationpublic class OAuth2ServerConfig &#123; private static final String DEMO_RESOURCE_ID = \"order\"; @Configuration @EnableResourceServer protected static class ResourceServerConfiguration extends ResourceServerConfigurerAdapter &#123; @Override public void configure(ResourceServerSecurityConfigurer resources) &#123; resources.resourceId(DEMO_RESOURCE_ID).stateless(true); &#125; @Override public void configure(HttpSecurity http) throws Exception &#123; // @formatter:off http // Since we want the protected resources to be accessible in the UI as well we need // session creation to be allowed (it's disabled by default in 2.0.6) .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.IF_REQUIRED) .and() .requestMatchers().anyRequest() .and() .anonymous() .and() .authorizeRequests()// .antMatchers(\"/product/**\").access(\"#oauth2.hasScope('select') and hasRole('ROLE_USER')\") .antMatchers(\"/order/**\").authenticated();//配置order访问控制，必须认证过后才可以访问 // @formatter:on &#125; &#125; @Configuration @EnableAuthorizationServer protected static class AuthorizationServerConfiguration extends AuthorizationServerConfigurerAdapter &#123; @Autowired AuthenticationManager authenticationManager; @Autowired RedisConnectionFactory redisConnectionFactory; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; //配置两个客户端,一个用于password认证一个用于client认证 clients.inMemory().withClient(\"client_1\") .resourceIds(DEMO_RESOURCE_ID) .authorizedGrantTypes(\"client_credentials\", \"refresh_token\") .scopes(\"select\") .authorities(\"client\") .secret(\"123456\") .and().withClient(\"client_2\") .resourceIds(DEMO_RESOURCE_ID) .authorizedGrantTypes(\"password\", \"refresh_token\") .scopes(\"select\") .authorities(\"client\") .secret(\"123456\"); &#125; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; endpoints .tokenStore(new RedisTokenStore(redisConnectionFactory)) .authenticationManager(authenticationManager); &#125; @Override public void configure(AuthorizationServerSecurityConfigurer oauthServer) throws Exception &#123; //允许表单认证 oauthServer.allowFormAuthenticationForClients(); &#125; &#125;&#125; 简单说下spring security oauth2的认证思路。 client模式，没有用户的概念，直接与认证服务器交互，用配置中的客户端信息去申请accessToken，客户端有自己的client_id,client_secret对应于用户的username,password，而客户端也拥有自己的authorities，当采取client模式认证时，对应的权限也就是客户端自己的authorities。 password模式，自己本身有一套用户体系，在认证时需要带上自己的用户名和密码，以及客户端的client_id,client_secret。此时，accessToken所包含的权限是用户本身的权限，而不是客户端的权限。 我对于两种模式的理解便是，如果你的系统已经有了一套用户体系，每个用户也有了一定的权限，可以采用password模式；如果仅仅是接口的对接，不考虑用户，则可以使用client模式。 配置spring security在spring security的版本迭代中，产生了多种配置方式，建造者模式，适配器模式等等设计模式的使用，spring security内部的认证flow也是错综复杂，在我一开始学习ss也产生了不少困惑，总结了一下配置经验：使用了springboot之后，spring security其实是有不少自动配置的，我们可以仅仅修改自己需要的那一部分，并且遵循一个原则，直接覆盖最需要的那一部分。这一说法比较抽象，举个例子。比如配置内存中的用户认证器。有两种配置方式 planA： 1234567@Beanprotected UserDetailsService userDetailsService()&#123; InMemoryUserDetailsManager manager = new InMemoryUserDetailsManager(); manager.createUser(User.withUsername(\"user_1\").password(\"123456\").authorities(\"USER\").build()); manager.createUser(User.withUsername(\"user_2\").password(\"123456\").authorities(\"USER\").build()); return manager;&#125; planB： 12345678910111213141516171819@Configuration@EnableWebSecuritypublic class SecurityConfiguration extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth.inMemoryAuthentication() .withUser(\"user_1\").password(\"123456\").authorities(\"USER\") .and() .withUser(\"user_2\").password(\"123456\").authorities(\"USER\"); &#125; @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception &#123; AuthenticationManager manager = super.authenticationManagerBean(); return manager; &#125;&#125; 你最终都能得到配置在内存中的两个用户，前者是直接替换掉了容器中的UserDetailsService，这么做比较直观；后者是替换了AuthenticationManager，当然你还会在SecurityConfiguration 复写其他配置，这么配置最终会由一个委托者去认证。如果你熟悉spring security，会知道AuthenticationManager和AuthenticationProvider以及UserDetailsService的关系，他们都是顶级的接口，实现类之间错综复杂的聚合关系…配置方式千差万别，但理解清楚认证流程，知道各个实现类对应的职责才是掌握spring security的关键。 下面给出我最终的配置： 123456789101112131415161718192021222324@Configuration@EnableWebSecuritypublic class SecurityConfiguration extends WebSecurityConfigurerAdapter &#123; @Bean @Override protected UserDetailsService userDetailsService()&#123; InMemoryUserDetailsManager manager = new InMemoryUserDetailsManager(); manager.createUser(User.withUsername(\"user_1\").password(\"123456\").authorities(\"USER\").build()); manager.createUser(User.withUsername(\"user_2\").password(\"123456\").authorities(\"USER\").build()); return manager; &#125; @Override protected void configure(HttpSecurity http) throws Exception &#123; // @formatter:off http .requestMatchers().anyRequest() .and() .authorizeRequests() .antMatchers(\"/oauth/*\").permitAll(); // @formatter:on &#125;&#125; 重点就是配置了一个UserDetailsService，和ClientDetailsService一样，为了方便运行，使用内存中的用户，实际项目中，一般使用的是数据库保存用户，具体的实现类可以使用JdbcDaoImpl或者JdbcUserDetailsManager。 获取token进行如上配置之后，启动springboot应用就可以发现多了一些自动创建的endpoints： 123456&#123;[/oauth/authorize]&#125;&#123;[/oauth/authorize],methods=[POST]&#123;[/oauth/token],methods=[GET]&#125;&#123;[/oauth/token],methods=[POST]&#125;&#123;[/oauth/check_token]&#125;&#123;[/oauth/error]&#125; 重点关注一下/oauth/token，它是获取的token的endpoint。启动springboot应用之后，使用http工具访问password模式： http://localhost:8080/oauth/token?username=user_1&amp;password=123456&amp;grant_type=password&amp;scope=select&amp;client_id=client_2&amp;client_secret=123456 响应如下：{&quot;access_token&quot;:&quot;950a7cc9-5a8a-42c9-a693-40e817b1a4b0&quot;,&quot;token_type&quot;:&quot;bearer&quot;,&quot;refresh_token&quot;:&quot;773a0fcd-6023-45f8-8848-e141296cb3cb&quot;,&quot;expires_in&quot;:27036,&quot;scope&quot;:&quot;select&quot;} client模式：http://localhost:8080/oauth/token?grant_type=client_credentials&amp;scope=select&amp;client_id=client_1&amp;client_secret=123456 响应如下：{&quot;access_token&quot;:&quot;56465b41-429d-436c-ad8d-613d476ff322&quot;,&quot;token_type&quot;:&quot;bearer&quot;,&quot;expires_in&quot;:25074,&quot;scope&quot;:&quot;select&quot;} 在配置中，我们已经配置了对order资源的保护，如果直接访问:http://localhost:8080/order/1会得到这样的响应:{&quot;error&quot;:&quot;unauthorized&quot;,&quot;error_description&quot;:&quot;Full authentication is required to access this resource&quot;}（这样的错误响应可以通过重写配置来修改） 而对于未受保护的product资源http://localhost:8080/product/1则可以直接访问，得到响应product id : 1 携带accessToken参数访问受保护的资源： 使用password模式获得的token:http://localhost:8080/order/1?access_token=950a7cc9-5a8a-42c9-a693-40e817b1a4b0，得到了之前匿名访问无法获取的资源：order id : 1 使用client模式获得的token:http://localhost:8080/order/1?access_token=56465b41-429d-436c-ad8d-613d476ff322，同上的响应order id : 1 我们重点关注一下debug后，对资源访问时系统记录的用户认证信息，可以看到如下的debug信息 password模式： client模式： 和我们的配置是一致的，仔细看可以发现两者的身份有些许的不同。想要查看更多的debug信息，可以选择下载demo代码自己查看，为了方便读者调试和验证，我去除了很多复杂的特性，基本实现了一个最简配置，涉及到数据库的地方也尽量配置到了内存中，这点记住在实际使用时一定要修改。 到这儿，一个简单的oauth2入门示例就完成了，一个简单的配置教程。token的工作原理是什么，它包含了哪些信息？spring内部如何对身份信息进行验证？以及上述的配置到底影响了什么？这些内容会放到后面的文章中去分析。 示例代码下载全部的代码可以在我的github上进行下载，项目使用springboot+maven构建：https://github.com/lexburner/oauth2-demo","categories":[{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","permalink":"http://lexburner.github.io/categories/Spring-Security-OAuth2/"}],"tags":[{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","permalink":"http://lexburner.github.io/tags/Spring-Security-OAuth2/"}]},{"title":"对于Spring Cloud Feign入门示例的一点思考","slug":"thinking-in-spring-cloud-feign","date":"2017-08-03T09:40:16.000Z","updated":"2017-08-22T04:26:14.457Z","comments":true,"path":"2017/08/03/thinking-in-spring-cloud-feign/","link":"","permalink":"http://lexburner.github.io/2017/08/03/thinking-in-spring-cloud-feign/","excerpt":"Spring Cloud FeignSpring Cloud Feign是一套基于Netflix Feign实现的声明式服务调用客户端。它使得编写Web服务客户端变得更加简单。我们只需要通过创建接口并用注解来配置它既可完成对Web服务接口的绑定。它具备可插拔的注解支持，包括Feign注解、JAX-RS注解。它也支持可插拔的编码器和解码器。Spring Cloud Feign还扩展了对Spring MVC注解的支持，同时还整合了Ribbon和Eureka来提供均衡负载的HTTP客户端实现。 分布式应用早在十几年前就开始出现，各自的应用运行在各自的tomcat，jboss一类的容器中，他们之间的相互调用变成了一种远程调用，而实现远程调用的方式很多。按照协议划分，可以有RPC，Webservice，http。不同的框架也对他们有了各自的实现，如dubbo(x)，motan就都是RPC框架，本文所要讲解的Feign便可以理解为一种http框架，用于分布式服务之间通过Http进行接口交互。说他是框架，有点过了，可以理解为一个http工具，只不过在spring cloud全家桶的体系中，它比httpclient，okhttp，retrofit这些http工具都要强大的多。 入门先用一个简单的例子，看看如何在项目中使用Feign。示例项目使用maven多module构建，采用springcloud的Dalston.SR1版本","text":"Spring Cloud FeignSpring Cloud Feign是一套基于Netflix Feign实现的声明式服务调用客户端。它使得编写Web服务客户端变得更加简单。我们只需要通过创建接口并用注解来配置它既可完成对Web服务接口的绑定。它具备可插拔的注解支持，包括Feign注解、JAX-RS注解。它也支持可插拔的编码器和解码器。Spring Cloud Feign还扩展了对Spring MVC注解的支持，同时还整合了Ribbon和Eureka来提供均衡负载的HTTP客户端实现。 分布式应用早在十几年前就开始出现，各自的应用运行在各自的tomcat，jboss一类的容器中，他们之间的相互调用变成了一种远程调用，而实现远程调用的方式很多。按照协议划分，可以有RPC，Webservice，http。不同的框架也对他们有了各自的实现，如dubbo(x)，motan就都是RPC框架，本文所要讲解的Feign便可以理解为一种http框架，用于分布式服务之间通过Http进行接口交互。说他是框架，有点过了，可以理解为一个http工具，只不过在spring cloud全家桶的体系中，它比httpclient，okhttp，retrofit这些http工具都要强大的多。 入门先用一个简单的例子，看看如何在项目中使用Feign。示例项目使用maven多module构建，采用springcloud的Dalston.SR1版本 1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Dalston.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 服务提供方在本例子中，使用两个应用模块，展示分布式应用中如何进行接口交互。restful-provider担任服务提供方，restful-consumer担任服务消费者。 restful-provider新建一个modulerestful-provider-app,模块中只需要写一个CalculateController.java即可 ​ 123456789101112131415@RestController@RequestMapping(\"/api\")public class CalculateController &#123; @PostMapping(\"/add\") public Integer add(@RequestParam Integer a,@RequestParam Integer b)&#123; return a+b; &#125; @PostMapping(\"/subtract\") public Integer subtract(@RequestParam Integer a,@RequestParam Integer b)&#123; return a-b; &#125;&#125; 配置文件application.yml： 12server: port: 7070 一个服务端就写好了，提供两个计算服务的接口，可以通过http访问 服务消费方 使用Feign编写消费方，在restful-consumer项目中，我们将接口的定义和消费者应用分成两个module，restful-consumer-api-definition和restful-consumer-app。 在接口定义模块中，只有一个Feign接口： 123456789@FeignClient(value = \"calculate\",path = \"/api\")public interface CalculateApi &#123; @PostMapping(path = \"/add\") Integer add(@RequestParam(\"a\") Integer a,@RequestParam(\"b\") Integer b); @PostMapping(path = \"/subtract\") Integer subtract(@RequestParam(\"a\") Integer a,@RequestParam(\"b\") Integer b);&#125; tip：@RequestParam中的参数值不能省略，否则会出现错误 restful-consumer-app依赖上面的restful-consumer-api-definition模块，并且启用Feign代理，自动生成一个远程调用。启动类配置： 123456789@EnableFeignClients(basePackages = &#123;\"sinosoftsh.consumer.api\"&#125;)@SpringBootApplicationpublic class ConsumerApp &#123; public static void main(String []args)&#123; SpringApplication.run(ConsumerApp.class,args); &#125;&#125; 使用@EnableFeignClients(basePackages = {&quot;sinosoftsh.consumer.api&quot;})扫描接口类所在的包，spring的容器中才会有代理实现类。 不要忘记配置消费者的相关属性，在application.yml中 12345678910111213server: port: 7080ribbon: eureka: enabled: falsecalculate: ribbon: listOfServers: localhost:7070logging: level: org.apache.http: trace 在CalculateApi 接口的定义中，我们使用了一个calculate作为服务名称，必须要在配置文件中配置calculate所在的ip地址才行，由于本文只是作为一个示例，所以没有使用注册中心，在配置中禁用了eureka。最后一行的日志配置，可以发现其实Feign内部其实使用的是现成的http工具：httpclient，okhttp3，可以通过配置替换实现 整体的项目结构如下： 图一 第一种依赖关系结构 再编写一个单元测试类，验证一下Feign是否被正确的配置了 12345678910111213@RestControllerpublic class ConsumerController &#123; @Autowired CalculateApi calculateApi; @RequestMapping(\"/test\") public String test() &#123; Integer result = calculateApi.add(1, 2); System.out.println(\"the result is \" + result); return \"success\"; &#125;&#125; 思考回顾一下我们入门实例，服务提供方使用的是一个RestController暴露计算服务，服务消费方使用http工具（Feign）进行远程调用，这再清晰不过了，也是符合软件设计的，因为Feign接口的定义是存在于消费方，所以是真正的松耦合。但是习惯了使用rpc共享接口的设计，我们也可以将接口定义在服务提供方，这样做的好处是，服务可能被多个消费者使用，不需要每个消费者都定义一次Feign接口。 图2 第二种依赖关系结构在restful-provider创建一个restful-provider-api-definition模块，将CalculateApi.java的定义迁移到服务提供方，相应的restful-provider-app也可以进行改造： 1234567891011121314151617@RestController@RequestMapping(\"/api\")public class CalculateController implements CalculateApi&#123;// @PostMapping(\"/add\") @Override public Integer add(@RequestParam Integer a,@RequestParam Integer b)&#123; return a+b; &#125;// @PostMapping(\"/subtract\") @Override public Integer subtract(@RequestParam Integer a,@RequestParam Integer b)&#123; return a-b; &#125;&#125; 因为接口的定义和服务提供方现在在一个限界上下文中，接口的定义同时也宣告了应该提供什么样的服务，所以直接继承CalculateApi。这里的理解比较绕，现在的设计中，CalculateApi在服务消费者和服务提供者中的定位是不一样的，服务消费者需要在启动类扫描CalculateApi所在的包，生成代理对象，远程调用；而在服务提供方则一定不能扫描CalculateApi所在的包，否则会污染容器中的CalculateApi实现类，要知道，CalculateController 之上有一个@RestController注解，意味着已经有一个本地代理实现了，我们也可以在服务提供方注入CalculateApi，便是进行的本地调用了，这符合我们的初衷：我自己的提供的服务，本地当然可以调用。在服务提供方的启动类上要额外注意@ComponentScan，@EnableFeignClients的扫描。 这样，当我们有多个消费者，只需要让他们配置Feign，并且引入服务提供方的接口定义，扫描，即可进行远程调用。有点类似于RPC的共享接口。 设计原则restful设计以语言无关，松耦合的优势著称。在Spring Cloud Feign的相关文档中有这样的描述： It is generally not advisable to share an interface between a server and a client. It introduces tight coupling, and also actually doesn’t work with Spring MVC in its current form (method parameter mapping is not inherited). 不建议使用上述改进后的共享接口的方式，并且警告我们，springmvc的注解在Feign接口中的定义和实现类中是不可继承的。关于这点，仁者见仁，智者见智。我们现在项目依旧是采用共享接口的方式，这样可以使得开发变得便捷，多个消费者不需要重复定义。 下面是关于耦合和共享接口的一些讨论： 1234https://github.com/spring-cloud/spring-cloud-netflix/issues/951https://github.com/spring-cloud/spring-cloud-netflix/issues/659https://github.com/spring-cloud/spring-cloud-netflix/issues/646https://jmnarloch.wordpress.com/2015/08/19/spring-cloud-designing-feign-client/ 注意事项 当接口定义中出现了实体类时，需要使用@RequestBody注解。多个实体类，则需要用一个大的vo对其进行包裹，要时刻记住，Feign接口最终是会转换成一次http请求。 接口定义中的注解和实现类中的注解要分别写一次，不能继承。 Feign调用一般配合eureka等注册中心使用，并且在客户端可以支持Hystrix机制，本文为了讲解共享接口这一设计，所以重心放在了Feign上，实际开发中，这些spring cloud的其他组件通常配套使用。 对http深入理解，在使用Feign时可以事半功倍。","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/tags/Spring-Cloud/"}]},{"title":"Re：从零开始的领域驱动设计","slug":"Re：从零开始的领域驱动设计","date":"2017-07-28T05:15:46.000Z","updated":"2017-08-22T05:41:09.257Z","comments":true,"path":"2017/07/28/Re：从零开始的领域驱动设计/","link":"","permalink":"http://lexburner.github.io/2017/07/28/Re：从零开始的领域驱动设计/","excerpt":"[TOC] 前言领域驱动的火爆程度不用我赘述，但是即便其如此得耳熟能详，但大多数人对其的认识，还只是停留在知道它的缩写是DDD，知道它是一种软件思想，或者知道它和微服务有千丝万缕的关系。Eric Evans对DDD的诠释是那么地惜字如金，而我所认识的领域驱动设计的专家又都是行业中的资深前辈，他们擅长于对软件设计进行高屋建瓴的论述，如果没有丰富的互联网从业经验，是不能从他们的分享中获取太多的营养的，可以用曲高和寡来形容。1000个互联网从业者，100个懂微服务，10个人懂领域驱动设计。 可能有很多和我一样的读者，在得知DDD如此火爆之后，尝试去读了开山之作《领域驱动设计——软件核心复杂性应对之道》，翻看了几张之后，晦涩的语句，不明所以的专业术语，加上翻译导致的语句流畅性，可以说观看体验并不是很好，特别是对于开发经验不是很多的读者。我总结了一下，为何这本书难以理解： 没有阅读软件设计丛书的习惯，更多人偏向于阅读偏应用层面的书籍，“talk is cheap，show me the code”往往更符合大多数人的习惯。2.没有太多的开发经验支撑。没有踩过坑，就不会意识到设计的重要性，无法产生共情。3.年代有些久远，这本书写于2004年，书中很多软件设计的反例，在当时是非常流行的，但是在现在已经基本绝迹了。大师之所以为大师，是因为其能跨越时代的限制，预见未来的问题，这也是为什么DDD在十几年前就被提出，却在微服务逐渐流行的现阶段才被大家重视。 诚然如标题所示，本文是领域驱动设计的一个入门文章，或者更多的是一个个人理解的笔记，笔者也正在学习DDD的路上，可能会有很多的疏漏。如有理解有偏颇的地方，还望各位指摘。","text":"[TOC] 前言领域驱动的火爆程度不用我赘述，但是即便其如此得耳熟能详，但大多数人对其的认识，还只是停留在知道它的缩写是DDD，知道它是一种软件思想，或者知道它和微服务有千丝万缕的关系。Eric Evans对DDD的诠释是那么地惜字如金，而我所认识的领域驱动设计的专家又都是行业中的资深前辈，他们擅长于对软件设计进行高屋建瓴的论述，如果没有丰富的互联网从业经验，是不能从他们的分享中获取太多的营养的，可以用曲高和寡来形容。1000个互联网从业者，100个懂微服务，10个人懂领域驱动设计。 可能有很多和我一样的读者，在得知DDD如此火爆之后，尝试去读了开山之作《领域驱动设计——软件核心复杂性应对之道》，翻看了几张之后，晦涩的语句，不明所以的专业术语，加上翻译导致的语句流畅性，可以说观看体验并不是很好，特别是对于开发经验不是很多的读者。我总结了一下，为何这本书难以理解： 没有阅读软件设计丛书的习惯，更多人偏向于阅读偏应用层面的书籍，“talk is cheap，show me the code”往往更符合大多数人的习惯。2.没有太多的开发经验支撑。没有踩过坑，就不会意识到设计的重要性，无法产生共情。3.年代有些久远，这本书写于2004年，书中很多软件设计的反例，在当时是非常流行的，但是在现在已经基本绝迹了。大师之所以为大师，是因为其能跨越时代的限制，预见未来的问题，这也是为什么DDD在十几年前就被提出，却在微服务逐渐流行的现阶段才被大家重视。 诚然如标题所示，本文是领域驱动设计的一个入门文章，或者更多的是一个个人理解的笔记，笔者也正在学习DDD的路上，可能会有很多的疏漏。如有理解有偏颇的地方，还望各位指摘。 认识领域驱动设计的意义领域驱动设计并不会绝对地提高项目的开发效率。 图1：复杂性与开发周期关系遵循领域驱动设计的规范使得项目初期的开发甚至不如不使用它来的快，原因有很多，程序员的素质，代码的规范，限界上下文的划分…甚至需求修改后导致需要重新建模。但是遵循领域驱动设计的规范，在项目越来越复杂之后，可以不至于让项目僵死。这也是为什么很多系统不断迭代着，最终就黄了。书名的副标题“软件核心复杂性应对之道”正是阐释了这一点## 模式： smart ui是个反模式可能很多读者还不知道smart ui是什么，但是在这本书写作期间，这种设计风格是非常流行的。在与一位领域驱动设计方面的资深专家的交谈中，他如下感慨到软件发展的历史：&gt;2003年时，正是delphi，vb一类的smart ui程序大行其道，java在那个年代，还在使用jsp来完成大量的业务逻辑操作，4000行的jsp是常见的事；2005年spring hibernate替换了EJB，社区一片欢呼，所有人开始拥护action，service，dao这样的贫血模型（充血模型，贫血模型会在下文论述）；2007年，Rails兴起，有人发现了Rails的activeRecord是涨血模型，引起了一片混战；直到现在的2017年，微服务成为主流系统架构。在现在这个年代，不懂个MVC分层，都不好意思说自己是搞java的，也不会有人在jsp里面写业务代码了（可以说模板技术freemarker,thymeleaf已经取代jsp了），但是在那个年代，还没有现在这么普遍地强调分层架构的重要性。这个章节其实并不重要，因为mvc一类的分层架构已经是大多数java初学者的“起点”了，大多数DDD的文章都不会赘述这一点，我这里列出来是为了让大家知晓这篇文章的时代局限性，在后续章节的理解中，也需要抱有这样的逻辑：这本书写于2004年。## 模式： Entity与Value Object我在不了解DDD时，就对这两个术语早有耳闻。entity又被称为reference object，我们通常所说的java bean在领域中通常可以分为这两类，（可别把value object和常用于前台展示的view object，vo混为一谈）entity的要义在于生命周期和标识，value object的要义在于无标识，通常情况下，entity在通俗意义上可以理解为数据库的实体，（不过不严谨），value object则一般作为一个单独的类，构成entity的一个属性。举两个例子来加深对entity和value object的理解。例1：以电商微服务系统中的商品模块，订单模块为例。将整个电商系统划分出商品和订单两个限界上下文（Bound Context）应该是没有争议的。如果是传统的单体应用，我们可以如何设计这两个模块的实体类呢？会不会是这样？1234567891011121314151617181920212223class Product&#123; String id;//主键 String skuId;//唯一识别号 String productName; Bigdecimal price; Category category;//分类 List&lt;Specification&gt; specifications;//规格 ... &#125;class Order&#123; String id;//主键 String orderNo;//订单号 List&lt;OrderItem&gt; orderItems;//订单明细 BigDecimal orderAmount;//总金额 ...&#125;class OrderItem&#123; String id; Product product;//关联商品 BigDecimal snapshotPrice;//下单时的价格&#125;看似好像没问题，考虑到了订单要保存下单时候的价格（当然，这是常识）但这么设计却存在诸多的问题。在分布式系统中，商品和订单这两个模块必然不在同一个模块，也就意味着不在同一个网段中。上述的类设计中直接将Product的列表存储到了Order中，也就是一对多的外键关联。这会导致，每次访问订单的商品列表，都需要发起n次远程调用。反思我们的设计，其实我们发现，订单BC的Product和商品BC的Product其实并不是同一个entity，在商品模块中，我们更关注商品的规格，种类，实时价格，这最直接地反映了我们想要买什么的欲望。而当生成订单后，我们只关心这个商品买的时候价格是多少，不会关心这个商品之后的价格变动，还有他的名称，仅仅是方便我们在订单的商品列表中定位这个商品。如何改造就变得明了了12345678class OrderItem&#123; String id; String productId;//只记录一个id用于必要的时候发起command操作 String skuId; String productName; ... BigDecimal snapshotPrice;//下单时的价格&#125;是的，我们做了一定的冗余，这使得即使商品模块的商品，名称发生了微调，也不会被订单模块知晓。这么做也有它的业务含义，用户会声称：我买的时候他的确就叫这个名字。记录productId和skuId的用意不是为了查询操作，而是方便申请售后一类的命令操作（command）。在这个例子中，Order 和 Product都是entity，而OrderItem则是value object（想想之前的定义，OrderItem作为一个类，的确是描述了Order这个entity的一个属性集合）。关于标识，我的理解是有两层含义，第一个是作为数据本身存储于数据库，主键id是一个标识，第二是作为领域对象本身，orderNo是一个标识，对于人而言，身份证是一个标识。而OrderItem中的productId，id不能称之为标识，因为整个OrderItem对象是依托于Order存在的，Order不存在，则OrderItem没有意义。例子2： 汽车和轮胎的关系是entity和value object吗？这个例子其实是一个陷阱题，因为他没有交代限界上下文（BC），场景不足以判断。对于用户领域而言，的确可以成立，汽车报废之后，很少有人会关心轮胎。轮胎和发动机，雨刮器，座椅地位一样，只是构成汽车的一些部件，和用户最紧密相关的，只有汽车这个entity，轮胎只是描述这个汽车的属性（value object）；场景切换到汽修厂，无论是汽车，还是轮胎，都是汽修厂密切关心的，每个轮胎都有自己的编号，一辆车报废了，可以安置到其他车上，这里，他们都是entity。这个例子是在说明这么一个道理，同样的事物，在不同的领域中，会有不同的地位。图2：《领域驱动设计》Value Object模式的示例 在单体应用中，可能会有人指出，这直接违背了数据库范式，但是领域驱动设计的思想正如他的名字那样，不是基于数据库的，而是基于领域的。微服务使得数据库发生了隔离，这样的设计思想可以更好的指导我们优化数据库。 模式： Repository 哲学家分析自然规律得出规范，框架编写者根据规范制定框架。有些框架，可能大家一直在用，但是却不懂其中蕴含的哲学。 ——来自于笔者的口胡 记得在刚刚接触mvc模式，常常用DAO层表示持久化层，在JPA+springdata中，抽象出了各式各样的xxxRepository，与DDD的Repository模式同名并不是巧合，jpa所表现出的正是一个充血模型（如果你遵循正确的使用方式的话），可以说是领域驱动设计的一个最佳实践。 开宗明义，在Martin Fowler理论中，有四种领域模型： 失血模型 贫血模型 充血模型 胀血模型详细的概念区别不赘述了，可以参见专门讲解4种模型的博客。他们在数据库开发中分别有不同的实现，用一个修改用户名的例子来分析。12345class User&#123; String id; String name; Integer age;&#125; 失血模型：跳过，可以理解为所有的操作都是直接操作数据库，在smart ui中可能会出现这样的情况。 贫血模型：123456789101112131415161718class UserDao &#123; @Autowired JdbcTemplate jdbcTemplate; public void updateName(String name,String id)&#123; jdbcTemplate.excute(\"update user u set u.name = ? where id=?\",name,id); &#125;&#125;class UserService&#123; @Autowired UserDao userDao; void updateName(String name,String id)&#123; userDao.updateName(name,id); &#125; &#125; 贫血模型中，dao是一类sql的集合，在项目中的表现就是写了一堆sql脚本，与之对应的service层，则是作为Transaction Script的入口。观察仔细的话，会发现整个过程中user对象都没出现过。 充血模型：1234567891011121314interface UserRepository extends JpaRepository&lt;User,String&gt;&#123; //springdata-jpa自动扩展出save findOne findAll方法&#125;class UserService&#123; @Autowoird UserRepository userRepository; void updateName(String name,String id)&#123; User user = userRepository.findOne(id); user.setName(name); userRepository.save(user); &#125;&#125; 充血模型中，整个修改操作是“隐性”的，对内存中user对象的修改直接影响到了数据库最终的结果，不需要关心数据库操作，只需要关注领域对象user本身。Repository模式就是在于此，屏蔽了数据库的实现。与贫血模型中user对象恰恰相反，整个流程没有出现sql语句。 涨血模型：没有具体的实现，可以这么理解：12345void updateName(String name,String id)&#123; User user = new User(id); user.setName(name); user.save();&#125; 我们在Repository模式中重点关注充血模型。为什么前面说：如果你遵循正确的使用方式的话，springdata才是对DDD的最佳实践呢？因为有的使用者会写出下面的代码：1234567interface UserRepository extends JpaRepository&lt;User,String&gt;&#123; @Query(\"update user set name=? where id=?\") @Modifying(clearAutomatically = true) @Transactional void updateName(String name,String id);&#125; 历史的车轮在滚滚倒退。本节只关注模型本身，不讨论使用中的一些并发问题，再来聊聊其他的一些最佳实践。1234567interface UserRepository extends JpaRepository&lt;User,String&gt;&#123; User findById();//√ 然后已经存在findOne了，只是为了做个对比 User findBy身份证号();//可以接受 User findBy名称();//× List&lt;权限&gt; find权限ByUserId();//×&#125; 理论上，一个Repository需要且仅需要包含三类方法loadBy标识，findAll，save（一般findAll（）就包含了分页，排序等多个方法，算作一类方法）。标识的含义和前文中entity的标识是同一个含义，在我个人的理解中，身份证可以作为一个用户的标识（这取决于你的设计，同样的逻辑还有订单中有业务含义的订单编号，保单中的投保单号等等），在数据库中，id也可以作为标识。findBy名称为什么不值得推崇，因为name并不是User的标识，名字可能会重复，只有在特定的现场场景中，名字才能具体对应到人。那应该如何完成“根据姓名查找可能的用户”这一需求呢？最方便的改造是使用Criteria，Predicate来完成视图的查询，哪怕只有一个非标识条件。在更完善的CQRS架构中，视图的查询则应该交由专门的View层去做，可以是数据库，可以是ES。findByUserId不值得推崇则是因为他违背了聚合根模式（下文会介绍），User的Repository只应该返回User对象。 软件设计初期，你是不是还在犹豫：是应该先设计数据库呢，还是应该设计实体呢？在Domain-Driven的指导下，你应当放弃Data-Driven。 模式 聚合和聚合根难住我的还有英文单词，初识这个概念时，忍不住发问：Aggregate是个啥。文中使用聚合的概念，来描述对象之间的关联，采用合适的聚合策略，可以避免一个很长，很深的对象引用路径。对划分模块也有很大的指导意义。 在微服务中我们常说划分服务模块，在领域驱动设计中，我们常说划分限界上下文。在面向对象的世界里，用抽象来封装模型中的引用，聚合就是指一组相关对象的集合，我们把它作为数据修改的单元。每个聚合都有一个聚合根(root)和一个边界(boundary)。边界定义了聚合内部有什么，而根则是一个特定的entity，两个聚合之间，只允许维护根引用，只能通过根引用去向深入引用其他引用变量。 例子还是沿用电商系统中的订单和商品模块。在聚合模式中，订单不能够直接关联到商品的规格信息，如果一定要查询，则应该通过订单关联到的商品，由商品去访问商品规格。在这个例子中，订单和商品分别是两个边界，而订单模块中的订单entity和商品模块中的商品entity就是分别是各自模块的root。遵循这个原则，可以使我们模块关系不那么的盘根错节，这也是众多领域驱动文章中不断强调的划分限界上下文是第一要义。 模式 包结构微服务有诸多的模块，而每个模块并不一定是那么的单一职责，比模块更细的分层，便是包的分层。我在阅读中，隐隐觉得这其中蕴含着一层哲学，但是几乎没有文章尝试解读它。领域驱动设计将其单独作为了一个模式进行了论述，篇幅不小。重点就是论述了一个思想：包结构应当具有高内聚性。 这次以一个真实的案例来介绍一下对高内聚的包结构的理解，项目使用maven多module搭建。我曾经开发过一个短信邮件平台模块，它在整个微服务系统中有两个职责，一：负责为其他模块提供短信邮件发送的远程调用接口，二：有一个后台页面，可以让管理员自定义发送短信，并且可以浏览全部的一，二两种类型发送的短信邮件记录。 在设计包结构之前，先是设计微服务模块。| module名 | 说明 | package类型 | 顶级包名 || ——- | ————— | ————– | ———————— || api | api接口定义，用于暴露服务 | jar | sinosoftgz.message.api || app | api实现者，真正的服务提供者 | executable jar | sinosoftgz.message.app || admin | 管理端应用 | executable jar | sinosoftgz.message.admin || model | 实体 | jar | sinosoftgz.message.model |api层定义了一系列的接口和接口依赖的一些java bean，model层也就是我们的领域层。这两个模块都会打成jar包，外部服务依赖api，api则由app模块使用rpc框架实现远程调用。admin和app连接同一个数据源，可以查询出短信邮件记录，admin需要自定义发送短信也是通过rpc调用。简单介绍完了这个项目后，重点来分析下需求，来看看如何构建包结构。mvc分层天然将controller，service，model，config层分割开，这符合DDD所推崇的分层架构模式（这个模式在原文中有描述，但我觉得和现在耳熟能详的分层结构没有太大的出入，所以没有放到本文中介绍），而我们的业务需求也将短信和邮件这两个领域拆分开了。那么，到底是mvc应该包含业务包结构呢？还是说业务包结构包含mvc呢？ mvc高于业务分层123456789101112131415161718192021//不够好的分层sinosoftgz.message.admin config CommonConfig.java service CommonService.java mail MailTemplateService.java MailMessageService.java sms SmsTemplateService.java SmsMessageService.java web IndexController.java mail MailTemplateController.java MailMessageController.java sms SmsTemplateController.java SmsMessageController.java MessageAdminApp.java 业务分层包含mvc123456789101112131415161718192021222324252627//高内聚的分层sinosoftgz.message.admin config CommonConfig.java service CommonService.java web IndexController.java mail config MailConfig.java service MailTemplateService.java MailMessageService.java web MailTemplateController.java MailMessageController.java sms config Smsconfig.java service SmsTemplateService.java SmsMessageService.java web SmsTemplateController.java SmsMessageController.java MessageAdminApp.java 业务并不是特别复杂，但应该可以发现第二种（业务分层包含mvc）的包结构，才是一种高内聚的包结构。第一种分层会让人有一种将各个业务模块（如mail和sms）的service和controller隔离开了的感觉，当模块更多，每个模块的内容更多，这个“隔得很远”的不适感会逐渐侵蚀你的开发速度。一种更加低内聚的反例是不用包分层，仅仅依赖前缀区分，由于在项目开发中真的发现同事写出了这样的代码，我觉得还是有必要拿出来说一说：12345678910111213141516171819//反例sinosoftgz.message.admin config CommonConfig.java MailConfig.java Smsconfig.java service CommonService.java MailTemplateService.java MailMessageService.java SmsTemplateService.java SmsMessageService.java web IndexController.java MailTemplateController.java MailMessageController.java SmsTemplateController.java SmsMessageController.java MessageAdminApp.java 这样的设计会导致web包越来越庞大，逐渐变得臃肿，是什么使项目僵化，项目经理为何一看到代码就头疼，规范的高内聚的包结构，遵循业务&gt;mvc的原则，可以知道我们的项目庞大却有条理。 其他模式《领域驱动设计》这本书介绍了众多的模式，上面只是介绍了一部分重要的模式，后续我会结合各个模式，尽量采用最佳实践+浅析设计的方式来解读。 微服务之于领域驱动设计的一点思考技术架构诚然重要，但不可忽视领域拆解和业务架构，《领域驱动设计》中的诸多失败，成功案例的总结，是支撑其理论知识的基础，最终汇聚成众多的模式。在火爆的微服务架构潮流下，我也逐渐意识到微服务不仅仅是技术的堆砌，更是一种设计，一门艺术。我的本科论文本想就微服务架构进行论述，奈何功底不够，最后只能改写成一篇分布式网站设计相关的文章，虽然是一个失败的过程，但让我加深了对微服务的认识。如今结合领域驱动设计，更加让我确定，技术方案始终有代替方案，决定微服务的不是框架的选择，不仅仅是restful或者rpc的接口设计风格的抉择，而更应该关注拆解，领域，限界上下文，聚合根等等一系列事物，这便是我所理解的领域驱动设计对微服务架构的指导意义。 参考文章多研究些架构，少谈些框架—-曹祖鹏 DDD领域驱动设计基本理论知识总结 - netfocus","categories":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"http://lexburner.github.io/categories/领域驱动设计/"}],"tags":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"http://lexburner.github.io/tags/领域驱动设计/"}]},{"title":"spring中的懒加载与事务--排坑记录","slug":"spring-transation-1","date":"2017-06-23T05:37:41.000Z","updated":"2017-09-05T09:35:32.017Z","comments":true,"path":"2017/06/23/spring-transation-1/","link":"","permalink":"http://lexburner.github.io/2017/06/23/spring-transation-1/","excerpt":"案例描述本文主要描述了开发中常见的几个与spring懒加载和事务相关的案例，描述常见的使用场景，以及如何规避他们，给出具体的代码。 在新的线程中，访问某个持久化对象的懒加载属性。 在quartz定时任务中，访问某个持久化对象的懒加载属性。 在dubbo，motan一类rpc框架中，远程调用时服务端session关闭的问题。 上面三个案例，其实核心都是一个问题，就是牵扯到spring对事务的管理，而懒加载这个技术，只是比较容易体现出事务出错的一个实践，主要用它来引发问题，进而对问题进行思考。","text":"案例描述本文主要描述了开发中常见的几个与spring懒加载和事务相关的案例，描述常见的使用场景，以及如何规避他们，给出具体的代码。 在新的线程中，访问某个持久化对象的懒加载属性。 在quartz定时任务中，访问某个持久化对象的懒加载属性。 在dubbo，motan一类rpc框架中，远程调用时服务端session关闭的问题。 上面三个案例，其实核心都是一个问题，就是牵扯到spring对事务的管理，而懒加载这个技术，只是比较容易体现出事务出错的一个实践，主要用它来引发问题，进而对问题进行思考。 前期准备为了能直观的暴露出第一个案例的问题，我新建了一个项目，采用传统的mvc分层，一个student.java实体类，一个studentDao.java持久层，一个studentService.java业务层，一个studentController控制层。 12345678910@Entity@Table(name = \"student\")public class Student &#123; @Id @GeneratedValue(strategy = GenerationType.AUTO) private Integer id; private String name; getter..setter..&#125; 持久层使用springdata，框架自动扩展出CURD方法12public interface StudentDao extends JpaRepository&lt;Student, Integer&gt;&#123;&#125; service层，先给出普通的调用方法。用于错误演示。1234567891011@Servicepublic class StudentService &#123; @Autowired StudentDao studentDao; public void testNormalGetOne()&#123; Student student = studentDao.getOne(1); System.out.println(student.getName()); &#125;&#125; 注意：getOne和findOne都是springdata提供的根据id查找单个实体的方法，区别是前者是懒加载，后者是立即加载。我们使用getOne来进行懒加载的实验，就不用大费周章去写懒加载属性，设置多个实体类了。 controller层，不是简简单单的调用，而是在新的线程中调用。使用controller层来代替单元测试（实际项目中，通常使用controller调用service，然后在浏览器或者http工具中调用触发，较为方便）1234567891011@RequestMapping(\"/testNormalGetOne\")@ResponseBodypublic String testNormalGetOne() &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; studentService.testNormalGetOne(); &#125; &#125;).start(); return \"testNormalGetOne\";&#125; 启动项目后，访问localhost:8080/testNormalGetOne报错如下：1Exception in thread \"Thread-6\" org.hibernate.LazyInitializationException: could not initialize proxy - no Session 问题分析no session说明了什么？道理很简单，因为spring的session是和线程绑定的，在整个model-&gt;dao-&gt;service-&gt;controller的调用链中，这种事务和线程绑定的机制非常契合。而我们出现的问题正式由于新开启了一个线程，这个线程与调用链的线程不是同一个。 问题解决我们先使用一种不太优雅的方式解决这个问题。在新的线程中，手动打开session。 12345678910public void testNormalGetOne() &#123; EntityManagerFactory entityManagerFactory = ApplicationContextProvider.getApplicationContext().getBean(EntityManagerFactory.class); EntityManager entityManager = entityManagerFactory.createEntityManager(); EntityManagerHolder entityManagerHolder = new EntityManagerHolder(entityManager); TransactionSynchronizationManager.bindResource(entityManagerFactory, entityManagerHolder); Student student = studentDao.getOne(1); System.out.println(student.getName()); TransactionSynchronizationManager.unbindResource(entityManagerFactory); EntityManagerFactoryUtils.closeEntityManager(entityManager);&#125; 由于我们使用了JPA，所以事务是由EntityManagerFactory这个工厂类生成的EntityManager来管理的。TransactionSynchronizationManager.bindResource(entityManagerFactory, entityManagerHolder);这个方法使用事务管理器绑定session。而ApplicationContextProvider这个工具类是用来获取spring容器中的EntityManagerFactory的，为什么不用注入的方式，下文讲解。它的代码如下：12345678910111213public class ApplicationContextProvider implements ApplicationContextAware &#123; private static ApplicationContext context = null; public static ApplicationContext getApplicationContext() &#123; return context; &#125; @Override public void setApplicationContext(ApplicationContext ac) throws BeansException &#123; context = ac; &#125;&#125; 问题暂时得到了解决。 问题再思考我们一般情况下使用懒加载属性，为什么没有出现no session的问题呢？相信大家都知道@Transactional这个注解，他会帮我们进行事务包裹，当然也会绑定session；以及大家熟知的hiberbate中的OpenSessionInterceptor和OpenSessionInViewFilter以及jpa中的OpenEntityManagerInViewInterceptor都是在没有session的情况下，打开session的过滤器。这种方法开始前依赖事务开启，方法结束后回收资源的操作，非常适合用过滤器拦截器处理，后续的两个未讲解的案例，其实都是使用了特殊的过滤器。 看一下官方文档如何描述这个jpa中的过滤器的： 29.3.4 Open EntityManager in View If you are running a web application, Spring Boot will by default register OpenEntityManagerInViewInterceptor to apply the “Open EntityManager in View” pattern, i.e. to allow for lazy loading in web views. If you don’t want this behavior you should set spring.jpa.open-in-view to false in your application.properties. 我们尝试着关闭这个过滤器：配置application.properties/application.yml文件1spring.jpa.open-in-view=false 再使用正常的方式访问懒加载属性（而不是在一个新的线程中）： 1234567891011 @RequestMapping(\"/testNormalGetOne\") @ResponseBody public String testNormalGetOne() &#123;// new Thread(new Runnable() &#123;// @Override// public void run() &#123; studentService.testNormalGetOne();// &#125;// &#125;).start(); return \"testNormalGetOne\"; &#125; 报错如下： 1&#123;\"timestamp\":1498194914012,\"status\":500,\"error\":\"Internal Server Error\",\"exception\":\"org.hibernate.LazyInitializationException\",\"message\":\"could not initialize proxy - no Session\",\"path\":\"/testNormalGetOne\"&#125; 是的，我们使用spring的controller作为单元测试时，以及我们平时在直接使用jpa的懒加载属性时没有太关注这个jpa的特性，因为springboot帮我们默认开启了这个过滤器。这也解释了，为什么在新的线程中，定时任务线程中，rpc远程调用时session没有打开的原因，因为这些流程没有经过springboot的web调用链。 另外两个实战案例上文已经阐释了，为什么quartz定时任务中访问懒加载属性，rpc框架服务端访问懒加载属性（注意不是客户端，客户端访问懒加载属性那是一种作死的行为，因为是代理对象）为出现问题。我们仿照spring打开session的思路（这取决于你使用hibernate还是jpa，抑或是mybatis），来编写我们的过滤器。 quartz中打开session：使用quartz提供的JobListenerSupport支持，编写一个任务过滤器，用于在每次任务执行时打开session1234567891011121314151617181920212223242526272829303132public class OpenEntityManagerJobListener extends JobListenerSupport implements ApplicationContextAware &#123; @Override public String getName() &#123; return \"OpenEntityManagerJobListener\"; &#125; EntityManagerFactory entityManagerFactory; @Override public void jobToBeExecuted(JobExecutionContext context) &#123; entityManagerFactory = applicationContext.getBean(EntityManagerFactory.class); EntityManager entityManager = entityManagerFactory.createEntityManager(); EntityManagerHolder emHolder = new EntityManagerHolder(entityManager); TransactionSynchronizationManager.bindResource(entityManagerFactory, emHolder); &#125; @Override public void jobWasExecuted(JobExecutionContext context, JobExecutionException jobException) &#123; EntityManagerHolder emHolder = (EntityManagerHolder) TransactionSynchronizationManager.unbindResource(entityManagerFactory); EntityManagerFactoryUtils.closeEntityManager(emHolder.getEntityManager()); &#125; ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; if(this.applicationContext ==null) throw new RuntimeException(\"applicationContext is null\"); &#125;&#125; 配置调度工厂： 12345678//调度工厂 @Bean public SchedulerFactoryBean schedulerFactoryBean() &#123; SchedulerFactoryBean factoryBean = new SchedulerFactoryBean(); factoryBean.setTriggers(triggerFactoryBeans().getObject()); factoryBean.setGlobalJobListeners(openEntityManagerJobListener()); return factoryBean; &#125; 也可以参考我的另一篇描述更为细致的文章(解决Quartz定时器中查询懒加载数据no session的问题)，那是我还是刚刚参加工作，可能有些许疏漏之处，不过参考是够了。 Motan（我现在使用的rpc框架）服务端打开session利用了motan对spi扩展的支持，编写了一个Filter，主要参考了motan的spi过滤器写法和springdata打开session/entityManager的思路。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158@SpiMeta(name = \"openjpasession\")@Activation(sequence = 100)public class OpenEntityManagerInMotanFilter implements Filter &#123; private Logger logger = LoggerFactory.getLogger(OpenEntityManagerInMotanFilter.class); /** * Default EntityManagerFactory bean name: \"entityManagerFactory\". * Only applies when no \"persistenceUnitName\" param has been specified. * * @see #setEntityManagerFactoryBeanName * @see #setPersistenceUnitName */ public static final String DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME = \"entityManagerFactory\"; private String entityManagerFactoryBeanName; private String persistenceUnitName; private volatile EntityManagerFactory entityManagerFactory; /** * Set the bean name of the EntityManagerFactory to fetch from Spring's * root application context. * &lt;p&gt;Default is \"entityManagerFactory\". Note that this default only applies * when no \"persistenceUnitName\" param has been specified. * * @see #setPersistenceUnitName * @see #DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME */ public void setEntityManagerFactoryBeanName(String entityManagerFactoryBeanName) &#123; this.entityManagerFactoryBeanName = entityManagerFactoryBeanName; &#125; /** * Return the bean name of the EntityManagerFactory to fetch from Spring's * root application context. */ protected String getEntityManagerFactoryBeanName() &#123; return this.entityManagerFactoryBeanName; &#125; /** * Set the name of the persistence unit to access the EntityManagerFactory for. * &lt;p&gt;This is an alternative to specifying the EntityManagerFactory by bean name, * resolving it by its persistence unit name instead. If no bean name and no persistence * unit name have been specified, we'll check whether a bean exists for the default * bean name \"entityManagerFactory\"; if not, a default EntityManagerFactory will * be retrieved through finding a single unique bean of type EntityManagerFactory. * * @see #setEntityManagerFactoryBeanName * @see #DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME */ public void setPersistenceUnitName(String persistenceUnitName) &#123; this.persistenceUnitName = persistenceUnitName; &#125; /** * Return the name of the persistence unit to access the EntityManagerFactory for, if any. */ protected String getPersistenceUnitName() &#123; return this.persistenceUnitName; &#125; /** * Look up the EntityManagerFactory that this filter should use. * &lt;p&gt;The default implementation looks for a bean with the specified name * in Spring's root application context. * * @return the EntityManagerFactory to use * @see #getEntityManagerFactoryBeanName */ protected EntityManagerFactory lookupEntityManagerFactory() &#123; String emfBeanName = getEntityManagerFactoryBeanName(); String puName = getPersistenceUnitName(); if (StringUtils.hasLength(emfBeanName)) &#123; return ApplicationContextProvider.getApplicationContext().getBean(emfBeanName, EntityManagerFactory.class); &#125; else if (!StringUtils.hasLength(puName) &amp;&amp; ApplicationContextProvider.getApplicationContext().containsBean(DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME)) &#123; return ApplicationContextProvider.getApplicationContext().getBean(DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME, EntityManagerFactory.class); &#125; else &#123; // Includes fallback search for single EntityManagerFactory bean by type. return EntityManagerFactoryUtils.findEntityManagerFactory(ApplicationContextProvider.getApplicationContext(), puName); &#125; &#125; /** * Create a JPA EntityManager to be bound to a request. * &lt;p&gt;Can be overridden in subclasses. * * @param emf the EntityManagerFactory to use * @see javax.persistence.EntityManagerFactory#createEntityManager() */ protected EntityManager createEntityManager(EntityManagerFactory emf) &#123; return emf.createEntityManager(); &#125; @Override public Response filter(Caller&lt;?&gt; caller, Request request) &#123; if (!(caller instanceof Provider)) &#123; return caller.call(request); &#125; EntityManagerFactory emf = null; try &#123; emf = lookupEntityManagerFactory(); &#125; catch (Exception e) &#123; logger.error(e.getMessage(), e); &#125; //可能没有启用openjpa if (emf == null) &#123; return caller.call(request); &#125; try &#123; //如果没有绑定，绑定到当前线程 if (TransactionSynchronizationManager.getResource(emf) == null) &#123; EntityManager em = createEntityManager(emf); EntityManagerHolder emHolder = new EntityManagerHolder(em); TransactionSynchronizationManager.bindResource(emf, emHolder); &#125; &#125; catch (Exception e) &#123; logger.error(e.getLocalizedMessage(), e); &#125; try &#123; return caller.call(request); &#125; finally &#123; //解除绑定 closeManager(emf); &#125; &#125; /** * 关闭 emf * * @param emf */ private void closeManager(EntityManagerFactory emf) &#123; if (emf == null || TransactionSynchronizationManager.getResource(emf) == null) &#123; return; &#125; EntityManagerHolder emHolder = null; try &#123; emHolder = (EntityManagerHolder) TransactionSynchronizationManager.unbindResource(emf); &#125; catch (IllegalStateException e) &#123; logger.error(e.getLocalizedMessage(), e); &#125; try &#123; if (emHolder != null) &#123; EntityManagerFactoryUtils.closeEntityManager(emHolder.getEntityManager()); &#125; &#125; catch (Exception e) &#123; logger.error(e.getLocalizedMessage(), e); &#125; &#125;&#125; 总结springboot中的事务管理做的永远比我们想的多，事务管理器的使用场景，@Transactional究竟起了哪些作用，以及spring-data这个对DDD最佳的阐释，以及mybatis一类的非j2ee规范在微服务的地位中是否高于jpa，各个层次之间的实体传输，消息传递…都是值得思考的。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"},{"name":"事务","slug":"事务","permalink":"http://lexburner.github.io/tags/事务/"}]},{"title":"使用zipkin做分布式链路监控","slug":"使用zipkin做分布式链路监控","date":"2017-06-11T18:51:51.000Z","updated":"2017-08-22T05:53:51.547Z","comments":true,"path":"2017/06/12/使用zipkin做分布式链路监控/","link":"","permalink":"http://lexburner.github.io/2017/06/12/使用zipkin做分布式链路监控/","excerpt":"介绍 Zipkin 为一个分布式的调用链跟踪系统( distributed tracing system ) ,设计来源于 google dapper paper 官方网站 快速入门 安装方式一：使用zipkin官方提供的jar启动服务zipkin官方提供了一个现成的使用springboot写的zipkin服务端，客户端的链路监控报告可以通过多种方式（下文会讲解具体的方式）向服务端发送报告。 系统需要安装java8 下载地址 配置详解查看源码可知其有4种持久化方式，本文选择使用最熟悉的mysql持久化链路调用信息。 首先建立数据库：默认情况下 zipkin 运行时数据保存在内存中，重启数据会丢失数据库脚本下载 查看与mysql storage相关的配置12345678910111213@ConfigurationProperties(\"zipkin.storage.mysql\")public class ZipkinMySQLStorageProperties implements Serializable &#123; // for Spark jobs private static final long serialVersionUID = 0L; private String host = \"localhost\"; private int port = 3306; private String username; private String password; private String db = \"zipkin\"; private int maxActive = 10; private boolean useSsl; ...&#125; 所以，我们使用mysql作为持久化策略，启动服务端的脚本也就有了1java -server -jar zipkin-server-1.26.0-exec.jar --zipkin.storage.type=mysql --zipkin.storage.mysql.host=localhost --zipkin.storage.mysql.port=3306 --zipkin.storage.mysql.username=root --zipkin.storage.mysql.password=root --zipkin.storage.mysql.db=zipkin 安装方式二springcloud官方按照传输方式分成了三种启动服务端的方式：Sleuth with Zipkin via HTTP，Sleuth with Zipkin via Spring Cloud Stream，Spring Cloud Sleuth Stream Zipkin Collector。只需要添加相应的依赖，之后配置相应的注解，如@EnableZipkinStreamServer即可。具体配置参考Spring Cloud官方文档 项目中，我们使用第一种作为服务端的启动方式，使用mysql作为持久化方案","text":"介绍 Zipkin 为一个分布式的调用链跟踪系统( distributed tracing system ) ,设计来源于 google dapper paper 官方网站 快速入门 安装方式一：使用zipkin官方提供的jar启动服务zipkin官方提供了一个现成的使用springboot写的zipkin服务端，客户端的链路监控报告可以通过多种方式（下文会讲解具体的方式）向服务端发送报告。 系统需要安装java8 下载地址 配置详解查看源码可知其有4种持久化方式，本文选择使用最熟悉的mysql持久化链路调用信息。 首先建立数据库：默认情况下 zipkin 运行时数据保存在内存中，重启数据会丢失数据库脚本下载 查看与mysql storage相关的配置12345678910111213@ConfigurationProperties(\"zipkin.storage.mysql\")public class ZipkinMySQLStorageProperties implements Serializable &#123; // for Spark jobs private static final long serialVersionUID = 0L; private String host = \"localhost\"; private int port = 3306; private String username; private String password; private String db = \"zipkin\"; private int maxActive = 10; private boolean useSsl; ...&#125; 所以，我们使用mysql作为持久化策略，启动服务端的脚本也就有了1java -server -jar zipkin-server-1.26.0-exec.jar --zipkin.storage.type=mysql --zipkin.storage.mysql.host=localhost --zipkin.storage.mysql.port=3306 --zipkin.storage.mysql.username=root --zipkin.storage.mysql.password=root --zipkin.storage.mysql.db=zipkin 安装方式二springcloud官方按照传输方式分成了三种启动服务端的方式：Sleuth with Zipkin via HTTP，Sleuth with Zipkin via Spring Cloud Stream，Spring Cloud Sleuth Stream Zipkin Collector。只需要添加相应的依赖，之后配置相应的注解，如@EnableZipkinStreamServer即可。具体配置参考Spring Cloud官方文档 项目中，我们使用第一种作为服务端的启动方式，使用mysql作为持久化方案 被监控项目配置application.yml 12345678910111213spring: zipkin: #服务端地址 base-url: http://10.19.52.11:9411 #本项目服务名 service: name: $&#123;spring.application.name&#125; sleuth: #监控开关 enabled: true #采样率 sampler: percentage: 1 springboot对zipkin的自动配置可以使得所有RequestMapping匹配到的endpoints得到监控，以及强化了restTemplate，对其加了一层拦截器，使得由他发起的http请求也同样被监控。 motan rpc调用监控Motan通过filter的SPI扩展机制支持OpenTracing，可以支持任何实现了OpenTracing标准的trace实现。使用OpenTracing需要以下步骤。 1.引入filter-opentracing扩展12345&lt;dependency&gt; &lt;groupId&gt;com.weibo&lt;/groupId&gt; &lt;artifactId&gt;filter-opentracing&lt;/artifactId&gt; &lt;version&gt;release&lt;/version&gt;&lt;/dependency&gt; 2.如果第三方trace工具声明了io.opentracing.Tracer的SPI扩展，直接引入第三方trace的jar包即可。如果第三方没有声明，则转第三步。 3.自定义一个TracerFactory实现TracerFactory接口，通过getTracer()来获取不同tracer实现。设置OpenTracingContext的tracerFactory为自定义的TracerFactory即可。 项目中的具体配置MotanConfig.java：12345678910111213141516171819202122232425@Bean(name = \"motanServerBasicConfig\") public BasicServiceConfigBean baseServiceConfig(@Value(\"$&#123;spring.sleuth.enabled:false&#125;\") Boolean tracing ) &#123; BasicServiceConfigBean config = new BasicServiceConfigBean(); ... if(tracing)&#123; config.setFilter(\"sleuth-tracing\"); &#125; ... return config; &#125;@BeanSleuthTracingContext sleuthTracingContext(@Autowired(required = false) org.springframework.cloud.sleuth.Tracer tracer)&#123; SleuthTracingContext context = new SleuthTracingContext(); context.setTracerFactory(new SleuthTracerFactory() &#123; @Override public org.springframework.cloud.sleuth.Tracer getTracer() &#123; return tracer; &#125; &#125;); return context; &#125; 数据查询具体的服务就不列出来了，为了演示依赖关系，service1使用restTemplate调用了service2,service2调用了service3，service4。还有一些现成的motan调用 find a trace当应用正常启动后，可以通过 http://10.19.52.11:9411 查看管理端项目已经成功被监控 Dependencies motan依赖树： http依赖树：","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://lexburner.github.io/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"http://lexburner.github.io/tags/DevOps/"},{"name":"Zipkin","slug":"Zipkin","permalink":"http://lexburner.github.io/tags/Zipkin/"}]},{"title":"JAVA程序员分级，你属于哪一种？","slug":"java-level","date":"2017-05-02T11:21:03.000Z","updated":"2017-08-22T05:56:22.455Z","comments":true,"path":"2017/05/02/java-level/","link":"","permalink":"http://lexburner.github.io/2017/05/02/java-level/","excerpt":"","text":"初级—初 掌握java基础，熟悉常用类库。理解java web中的servlet，jsp，并了解常用的框架对java web的封装原理，能够借助框架完成增删改查功能。理解数据库在web开发中的地位。 初级—中 理解java中较为高级的特性，如反射，动态代理，JVM，内存模型，多线程等等。熟练使用框架，对框架中遇到的bug，能够借助日志和搜索引擎分析出问题的原因。在团队中，能够独立完成普通后台业务功能的开发。了解数据库的高级特性，如索引，存储引擎等等。 初级—高 理解java分布式架构，微服务架构，了解其与集中式架构的区别，并能保证分布式代码质量。熟练使用各个中间件如redis，mq，zookeeper等等，并了解其工作原理和使用场景。能够在中级或高级程序员的带领之下，完成非核心功能的研发。能够关注开源，并且具有阅读源码的能力。 中级 具备一定的项目开发经验（3年之上一线互联网产品研发经验），拥有线上bug的处理能力，JVM调优能力，以及完成核心业务功能的开发。并且带领团队的新人，能够按能力分配任务。 高级 团队的核心人物，把控整个项目的质量，包括代码漏洞和规范问题。具有5年以上项目开发经验，2年以上架构搭建的经验，能够根据业务选择不同的架构类型；根据团队组成，分配不同的任务。具有将自己的知识分享出去的能力，带领初级程序员走向中级，中级程序员走向高级的能力。","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/categories/技术杂谈/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"},{"name":"杂谈","slug":"杂谈","permalink":"http://lexburner.github.io/tags/杂谈/"}]},{"title":"drools用户指南----Cross Products","slug":"drools-4","date":"2017-04-11T05:44:54.000Z","updated":"2017-10-25T10:29:04.312Z","comments":true,"path":"2017/04/11/drools-4/","link":"","permalink":"http://lexburner.github.io/2017/04/11/drools-4/","excerpt":"","text":"Cross Products之前提到“Cross Products”一词，其实就是一个join操作（译者注：可以理解为笛卡尔积）。想象一下，火灾报警示例的数据与以下规则结合使用，其中没有字段约束： 1234567rule \"Show Sprinklers\" when $room : Room() $sprinkler : Sprinkler()then System.out.println( \"room:\" + $room.getName() + \" sprinkler:\" + $sprinkler.getRoom().getName() );end 在SQL术语中，这就像是执行了select * from Room, Sprinkler，Sprinkler 表中的每一行将与Room表中的每一行相连接，从而产生以下输出： 12345678910111213141516room:office sprinkler:officeroom:office sprinkler:kitchenroom:office sprinkler:livingroomroom:office sprinkler:bedroomroom:kitchen sprinkler:officeroom:kitchen sprinkler:kitchenroom:kitchen sprinkler:livingroomroom:kitchen sprinkler:bedroomroom:livingroom sprinkler:officeroom:livingroom sprinkler:kitchenroom:livingroom sprinkler:livingroomroom:livingroom sprinkler:bedroomroom:bedroom sprinkler:officeroom:bedroom sprinkler:kitchenroom:bedroom sprinkler:livingroomroom:bedroom sprinkler:bedroom 这些连接结果显然会变得巨大，它们必然包含冗余数据。 cross products的大小通常是新规则引擎产品性能问题的根源。 从这可以看出，我们希望约束cross products，这便是用可变约束（the variable constraint）完成的。 12345678rulewhen $room : Room() $sprinkler : Sprinkler( room == $room )then System.out.println( \"room:\" + $room.getName() + \" sprinkler:\" + $sprinkler.getRoom().getName() );end 这就使得筛选结果只有寥寥几行, 这就为每一个Room筛选出了正确的Sprinkler. 在sql中(实际上是HQL) 这样的查询约等于select * from Room, Sprinkler where Room == Sprinkler.room.","categories":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/categories/规则引擎/"}],"tags":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/tags/规则引擎/"},{"name":"drools","slug":"drools","permalink":"http://lexburner.github.io/tags/drools/"}]},{"title":"drools用户指南----Methods vs Rules","slug":"drools-3","date":"2017-04-11T05:28:44.000Z","updated":"2017-10-25T10:29:04.309Z","comments":true,"path":"2017/04/11/drools-3/","link":"","permalink":"http://lexburner.github.io/2017/04/11/drools-3/","excerpt":"","text":"Methods vs Rules人们经常混淆方法和规则，初学者经常会问：“我如何理解规则的含义？“ 在最后一节之后，你会对规则的使用得心应手，答案也变得显而易见的，但在这之前，先让我们总结一下方法判断和规则的差异。 12345public void helloWorld(Person person) &#123; if ( person.getName().equals( \"Chuck\" ) ) &#123; System.out.println( \"Hello Chuck\" ); &#125;&#125; 方法是被直接调用的 需要传递具体的实例 一个调用导致一次执行（One call results in a single execution）。 12345rule \"Hello World\" when Person( name == \"Chuck\" )then System.out.println( \"Hello Chuck\" );end 只要将其插入引擎，就可以通过匹配任何数据执行规则。 规则永远无法被直接调用，而只能触发 无法将特定的实例传递给规则 根据匹配，一个规则可能会触发一次或多次，或根本不被触发。","categories":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/categories/规则引擎/"}],"tags":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/tags/规则引擎/"},{"name":"drools","slug":"drools","permalink":"http://lexburner.github.io/tags/drools/"}]},{"title":"drools用户指南----stateless session（无状态会话）的使用","slug":"drools-1","date":"2017-04-11T04:51:59.000Z","updated":"2017-10-25T10:29:04.304Z","comments":true,"path":"2017/04/11/drools-1/","link":"","permalink":"http://lexburner.github.io/2017/04/11/drools-1/","excerpt":"stateless session 无状态会话Drools规则引擎中有如此多的用例和诸多功能，它变得令人难以置信。不过不用担心，复杂性是分层的，你可以用简单的用例来逐步了解drools。 无状态会话，不使用推理，形成最简单的用例。无状态会话可以被称为函数传递一些数据，然后再接收一些结果。无状态会话的一些常见用例有以下但不限于： 验证这个人有资格获得抵押吗？ 计算计算抵押保费。 路由和过滤将传入的邮件（如电子邮件）过滤到文件夹中。将传入的邮件发送到目的地。 所以让我们从使用驾驶执照应用程序的一个非常简单的例子开始吧。 123456public class Applicant &#123; private String name; private int age; private boolean valid; // getter and setter methods here&#125; 现在我们有了我们的数据模型，我们可以写出我们的第一个规则。我们假设应用程序使用规则来拒绝不符合规则的申请。由于这是一个简单的验证用例，我们将添加一条规则来取消任何18岁以下的申请人的资格。 12345678package com.company.licenserule \"Is of valid age\"when $a : Applicant( age &lt; 18 )then $a.setValid( false );end","text":"stateless session 无状态会话Drools规则引擎中有如此多的用例和诸多功能，它变得令人难以置信。不过不用担心，复杂性是分层的，你可以用简单的用例来逐步了解drools。 无状态会话，不使用推理，形成最简单的用例。无状态会话可以被称为函数传递一些数据，然后再接收一些结果。无状态会话的一些常见用例有以下但不限于： 验证这个人有资格获得抵押吗？ 计算计算抵押保费。 路由和过滤将传入的邮件（如电子邮件）过滤到文件夹中。将传入的邮件发送到目的地。 所以让我们从使用驾驶执照应用程序的一个非常简单的例子开始吧。 123456public class Applicant &#123; private String name; private int age; private boolean valid; // getter and setter methods here&#125; 现在我们有了我们的数据模型，我们可以写出我们的第一个规则。我们假设应用程序使用规则来拒绝不符合规则的申请。由于这是一个简单的验证用例，我们将添加一条规则来取消任何18岁以下的申请人的资格。 12345678package com.company.licenserule \"Is of valid age\"when $a : Applicant( age &lt; 18 )then $a.setValid( false );end 为了使引擎了解数据，所以可以根据规则进行处理，我们必须插入数据，就像数据库一样。当申请人实例插入到引擎中时，将根据规则的约束进行评估，在这种情况下，这只是一个规则的两个约束条件。我们说两个，因为申请人类型是第一个对象类型约束，而age &lt;18是第二个字段约束。对象类型约束及其零个或多个字段约束被称为模式。当插入的实例同时满足对象类型约束和所有字段约束时，它被称为匹配。$a是一个绑定变量，它允许我们引用匹配的对象。其属性可以更新。美元字符（’$’）是可选的，但它有助于区分变量名称和字段名称。匹配模式与插入数据的过程并不奇怪，通常被称为模式匹配。 要使用这个规则，有必要把它放在一个Drools文件中，只是一个带有.drl扩展名的纯文本文件，简称为“Drools Rule Language”。我们来调用licenseApplication.drl这个文件，并将其存储在Kie Project中。 Kie项目具有正常的Maven项目的结构，并附加一个可以创建的KieBase和KieSession文件（kmodule.xml）。该文件必须放在Maven项目的resources/META-INF文件夹中，而所有其他Drools工件（如包含前一规则的licenseApplication.drl）必须存储在资源文件夹或其下的任何其他子文件夹中。 由于为所有配置方面提供了有意义的默认值，所以最简单的kmodule.xml文件只能包含一个空的kmodule标签，如下所示： 12&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;kmodule xmlns=\"http://www.drools.org/xsd/kmodule\"/&gt; 此时，可以从类路径创建一个KieContainer来读取要构建的文件。 12KieServices kieServices = KieServices.Factory.get();KieContainer kContainer = kieServices.getKieClasspathContainer(); 上面的代码段编译了类路径中找到的所有DRL文件，并将该编译结果KieModule放在KieContainer中。如果没有错误，我们现在可以从KieContainer创建我们的会话并执行一些数据： 12345StatelessKieSession kSession = kContainer.newStatelessKieSession();Applicant applicant = new Applicant( \"Mr John Smith\", 16 );assertTrue( applicant.isValid() );ksession.execute( applicant );assertFalse( applicant.isValid() ); 上述代码根据规则执行数据。由于申请人年龄未满18岁，申请被标记为无效。 到目前为止，我们只使用了一个实例，但是如果我们想要使用多个实例呢？我们可以执行任何实现Iterable的对象，如集合。我们再添加一个名为Application的类，它有应用程序的日期，我们还将布尔有效字段移到Application类。 1234567891011public class Applicant &#123; private String name; private int age; // getter and setter methods here&#125;public class Application &#123; private Date dateApplied; private boolean valid; // getter and setter methods here&#125; 我们还将添加另一条规则来验证申请是否在一段时间内进行。 12345678910111213141516package com.company.licenserule \"Is of valid age\"when Applicant( age &lt; 18 ) $a : Application() then $a.setValid( false );endrule \"Application was made this year\"when $a : Application( dateApplied &gt; \"01-jan-2009\" ) then $a.setValid( false );end 不幸的是，Java数组不实现Iterable接口，所以我们必须使用JDK转换器方法Arrays.asList（…）。下面显示的代码针对一个可迭代列表执行，其中在触发任何匹配的规则之前插入所有集合元素。 123456StatelessKieSession kSession = kContainer.newStatelessKieSession();Applicant applicant = new Applicant( \"Mr John Smith\", 16 );Application application = new Application();assertTrue( application.isValid() );ksession.execute( Arrays.asList( new Object[] &#123; application, applicant &#125; ) );assertFalse( application.isValid() ); 执行的两个执行方法（Object object）和execute（Iterable对象）实际上是接口BatchExecutor的方法execute（Command命令）的便利方法。 KieCommands命令工厂可以像KIE A​​PI的所有其他工厂一样从KieServices获取，用于创建命令，以便以下操作相当于执行（Iterable it）： 1ksession.execute( kieServices.getCommands().newInsertElements( Arrays.asList( new Object[] &#123; application, applicant &#125; ) ); 批处理执行器和命令工厂在使用多个命令和输出标识符以获取结果时特别有用。123456KieCommands kieCommands = kieServices.getCommands();List&lt;Command&gt; cmds = new ArrayList&lt;Command&gt;();cmds.add( kieCommands.newInsert( new Person( \"Mr John Smith\" ), \"mrSmith\", true, null ) );cmds.add( kieCommands.newInsert( new Person( \"Mr John Doe\" ), \"mrDoe\", true, null ) );BatchExecutionResults results = ksession.execute( kieCommands.newBatchExecution( cmds ) );assertEquals( new Person( \"Mr John Smith\" ), results.getValue( \"mrSmith\" ) );","categories":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/categories/规则引擎/"}],"tags":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/tags/规则引擎/"},{"name":"drools","slug":"drools","permalink":"http://lexburner.github.io/tags/drools/"}]},{"title":"drools用户指南----stateful session（有状态会话）的使用","slug":"drools-2","date":"2017-04-11T04:37:22.000Z","updated":"2017-10-25T10:29:04.307Z","comments":true,"path":"2017/04/11/drools-2/","link":"","permalink":"http://lexburner.github.io/2017/04/11/drools-2/","excerpt":"stateful session 有状态会话有状态会话长期存在，并允许随着时间的推移进行迭代更改。 有状态会话的一些常见用例包括但不限于： 监测半自动买入股票市场监控与分析。 诊断故障查找，医疗诊断 物流包裹跟踪和送货配置 合规验证市场交易的合法性。 与无状态会话相反，必须先调用dispose()方法，以确保没有内存泄漏，因为KieBase包含创建状态知识会话时的引用。 由于状态知识会话是最常用的会话类型，所以它只是在KIE API中命名为KieSession。 KieSession还支持BatchExecutor接口，如StatelessKieSession，唯一的区别是FireAllRules命令在有状态会话结束时不被自动调用。 我们举例说明了用于提高火灾报警器的监控用例。 只使用四个类，我们假设Room代表房子里的房间，每个Room都有一个喷头Sprinkler。 如果在房间里发生火灾，我们用一个Fire实例来表示,用Alarm代表警报 。 123456789101112131415161718public class Room &#123; private String name // getter and setter methods here&#125;public class Sprinkler &#123; private Room room; private boolean on; // getter and setter methods here&#125;public class Fire &#123; private Room room; // getter and setter methods here&#125;public class Alarm &#123;&#125; 在上一节无状态会话中介绍了插入和匹配数据的概念。 这个例子假设每个对象类型的都是单个实例被插入的，因此只使用了字面约束。 然而，房子有许多房间，因此rules必须表达实体类之间的关系，例如在某个房间内的喷洒器。 这最好通过使用绑定变量作为模式中的约束来完成。 这种“加入”过程产生了所谓的“cross products”，这在下一节中将会介绍。","text":"stateful session 有状态会话有状态会话长期存在，并允许随着时间的推移进行迭代更改。 有状态会话的一些常见用例包括但不限于： 监测半自动买入股票市场监控与分析。 诊断故障查找，医疗诊断 物流包裹跟踪和送货配置 合规验证市场交易的合法性。 与无状态会话相反，必须先调用dispose()方法，以确保没有内存泄漏，因为KieBase包含创建状态知识会话时的引用。 由于状态知识会话是最常用的会话类型，所以它只是在KIE API中命名为KieSession。 KieSession还支持BatchExecutor接口，如StatelessKieSession，唯一的区别是FireAllRules命令在有状态会话结束时不被自动调用。 我们举例说明了用于提高火灾报警器的监控用例。 只使用四个类，我们假设Room代表房子里的房间，每个Room都有一个喷头Sprinkler。 如果在房间里发生火灾，我们用一个Fire实例来表示,用Alarm代表警报 。 123456789101112131415161718public class Room &#123; private String name // getter and setter methods here&#125;public class Sprinkler &#123; private Room room; private boolean on; // getter and setter methods here&#125;public class Fire &#123; private Room room; // getter and setter methods here&#125;public class Alarm &#123;&#125; 在上一节无状态会话中介绍了插入和匹配数据的概念。 这个例子假设每个对象类型的都是单个实例被插入的，因此只使用了字面约束。 然而，房子有许多房间，因此rules必须表达实体类之间的关系，例如在某个房间内的喷洒器。 这最好通过使用绑定变量作为模式中的约束来完成。 这种“加入”过程产生了所谓的“cross products”，这在下一节中将会介绍。 当发生火灾时，会为该类别创建Fire类的实例，并将其插入到会话中。 该规则使用Fire对象的房间字段上的绑定来约束与当前关闭的房间的喷水灭火器的匹配。 当此规则触发并且执行结果时，喷头被打开。 12345678rule \"When there is a fire turn on the sprinkler\"when Fire($room : room) $sprinkler : Sprinkler( room == $room, on == false )then modify( $sprinkler ) &#123; setOn( true ) &#125;; System.out.println( \"Turn on the sprinkler for room \" + $room.getName() );end 而无状态会话使用标准Java语法来修改字段，在上述规则中，我们使用modify语句，它作为一种“with”语句。 它可以包含一系列逗号分隔的Java表达式，即对由modify语句的控制表达式选择的对象的setter的调用。 这将修改数据，并使引擎意识到这些更改，以便它可以再次对其进行推理。 这个过程被称为推理，对于有状态会话的工作至关重要。 无状态会话通常不使用推理，因此引擎不需要意识到数据的更改。 也可以通过使用顺序模式显式地关闭推理。 到目前为止，我们有规则告诉我们匹配数据是否存在，但是当它不存在时呢？ 我们如何确定火已经熄灭了，即没有Fire对象呢？ 以前的约束是根据命题逻辑的句子，其中引擎限制个别的实例。 Drools还支持First Order Logic，允许您查看数据集。 当某个不存在时，关键字下的模式不匹配。 一旦这个房间的火灾消失，下面给出的规则会使喷水灭火。 123456789rule \"When the fire is gone turn off the sprinkler\"when $room : Room( ) $sprinkler : Sprinkler( room == $room, on == true ) not Fire( room == $room )then modify( $sprinkler ) &#123; setOn( false ) &#125;; System.out.println( \"Turn off the sprinkler for room \" + $room.getName() );end 每个room有一个喷水灭火器，house只有一个警报。 当发生火灾时，会创建一个alrm对象，而不管发生多少火灾，整个建筑物都只需要一个警报alrm。 1234567rule \"Raise the alarm when we have one or more fires\"when exists Fire()then insert( new Alarm() ); System.out.println( \"Raise the alarm\" );end 同样，当没有火灾时，我们想要删除警报，所以可以再次使用not关键字。 12345678rule \"Cancel the alarm when all the fires have gone\"when not Fire() $alarm : Alarm()then delete( $alarm ); System.out.println( \"Cancel the alarm\" );end 最后，当应用程序首次启动并且在报警消除并且所有喷头已关闭后，都会打印Everything is ok。 1234567rule \"Status output when things are ok\"when not Alarm() not Sprinkler( on == true ) then System.out.println( \"Everything is ok\" );end 正如我们在无状态会话示例中所做的那样，上述规则应放在单个DRL文件中，并保存到Maven项目或其任何子文件夹的资源文件夹中。 如前所述，我们可以从KieContainer获得KieSession。 唯一的区别是，这次我们创建一个有状态会话，而之前我们创建的是一个无状态会话。 123KieServices kieServices = KieServices.Factory.get();KieContainer kContainer = kieServices.getKieClasspathContainer();KieSession ksession = kContainer.newKieSession(); 创建会话后，现在可以随着时间的推移迭代地使用它。 创建和插入四个房间对象，每个房间的对应一个Sprinkler对象。 此时，规则引擎已经完成了所有的匹配，但并没有触发。 调用ksession.fireAllRules（）使得匹配的规则触发，但因为没有火灾，所以输出结果是Everything is ok。 1234567891011String[] names = new String[]&#123;\"kitchen\", \"bedroom\", \"office\", \"livingroom\"&#125;;Map&lt;String,Room&gt; name2room = new HashMap&lt;String,Room&gt;();for( String name: names )&#123; Room room = new Room( name ); name2room.put( name, room ); ksession.insert( room ); Sprinkler sprinkler = new Sprinkler( room ); ksession.insert( sprinkler );&#125;ksession.fireAllRules(); Everything is ok 我们现在创造两个Fire并插入它们， 随着发动机内部的火灾，一旦调用了fireAllRules（），报警器就会升高，并且相应的喷水灭火器打开。 123456Fire kitchenFire = new Fire( name2room.get( \"kitchen\" ) );Fire officeFire = new Fire( name2room.get( \"office\" ) );FactHandle kitchenFireHandle = ksession.insert( kitchenFire );FactHandle officeFireHandle = ksession.insert( officeFire );ksession.fireAllRules(); Raise the alarmTurn on the sprinkler for room kitchenTurn on the sprinkler for room office 一段时间之后，火灾将熄灭，并且Fire实例被撤回。 这导致喷头关闭，报警被取消，最后再次打印Everything is ok。 1234ksession.delete( kitchenFireHandle );ksession.delete( officeFireHandle );ksession.fireAllRules(); Cancel the alarmTurn off the sprinkler for room officeTurn off the sprinkler for room kitchenEverything is ok","categories":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/categories/规则引擎/"}],"tags":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/tags/规则引擎/"},{"name":"drools","slug":"drools","permalink":"http://lexburner.github.io/tags/drools/"}]},{"title":"Zuul性能测试","slug":"zuul-test","date":"2017-04-08T07:27:52.000Z","updated":"2017-08-22T06:26:32.079Z","comments":true,"path":"2017/04/08/zuul-test/","link":"","permalink":"http://lexburner.github.io/2017/04/08/zuul-test/","excerpt":"环境准备采用三台阿里云服务器作为测试10.19.52.8 部署网关应用-gateway10.19.52.9, 10.19.52.10 部署用于测试的业务系统 压测工具准备选用ab作为压力测试的工具，为了方便起见，直接将ab工具安装在10.19.52.8这台机测试命令如下：1ab -n 10000 -c 100 http://10.19.52.8:8080/hello/testOK?access_token=e0345712-c30d-4bf8-ae61-8cae1ec38c52 其中－n表示请求数，－c表示并发数,上面一条命令也就意味着，100个用户并发对http://10.19.52.8/hello/testOK累计发送了10000次请求。 服务器,网关配置由于我们使用的tomcat容器，关于tomcat的一点知识总结如下：","text":"环境准备采用三台阿里云服务器作为测试10.19.52.8 部署网关应用-gateway10.19.52.9, 10.19.52.10 部署用于测试的业务系统 压测工具准备选用ab作为压力测试的工具，为了方便起见，直接将ab工具安装在10.19.52.8这台机测试命令如下：1ab -n 10000 -c 100 http://10.19.52.8:8080/hello/testOK?access_token=e0345712-c30d-4bf8-ae61-8cae1ec38c52 其中－n表示请求数，－c表示并发数,上面一条命令也就意味着，100个用户并发对http://10.19.52.8/hello/testOK累计发送了10000次请求。 服务器,网关配置由于我们使用的tomcat容器，关于tomcat的一点知识总结如下： Tomcat的最大并发数是可以配置的，实际运用中，最大并发数与硬件性能和CPU数量都有很大关系的。更好的硬件，更多的处理器都会使Tomcat支持更多的并发。​Tomcat 默认的HTTP实现是采用阻塞式的Socket通信，每个请求都需要创建一个线程处理，当一个进程有500个线程在跑的话，那性能已经是很低很低了。Tomcat默认配置的最大请求数是150，也就是说同时支持150个并发。具体能承载多少并发，需要看硬件的配置，CPU越多性能越高，分配给JVM的内存越多性能也就越高，但也会加重GC的负担。当某个应用拥有 250个以上并发的时候，应考虑应用服务器的集群。操作系统对于进程中的线程数有一定的限制： Windows 每个进程中的线程数不允许超过 2000Linux 每个进程中的线程数不允许超过 1000在Java中每开启一个线程需要耗用1MB的JVM内存空间用于作为线程栈之用，此处也应考虑。 所以我们修改配置tomcat的默认配置，如下：12345server: tomcat: accept-count: 1000 max-threads: 1000 max-connections: 2000 无论是网关应用，还是用于测试的业务系统的tomcat，我们都需要如上配置，否则会引起木桶效应，整个调用流程会受到配置最差的应用的干扰。zuul内部路由可以理解为使用一个线程池去发送路由请求，所以我们也需要扩大这个线程池的容量，配置如下：1234zuul: host: max-per-route-connections: 1000 max-total-connections: 1000 监控工具为了确保上述配置真正起作用，我们使用Java VisualVM这个工具监控这几台服务器上部署的tomcat的线程以及内存使用情况。启动脚本加上如下参数，之后通过工具连接2099端口即可监控1-Dcom.sun.management.jmxremote.port=2099 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Djava.rmi.server.hostname=10.19.52.8 开始测试 测试一1.通过访问网关，由网关转发，应用端接口延迟200ms后返回一个字符串，模拟真实接口的业务处理延迟2.300个线程并发请求，共计100000 次1ab -n 100000 -c 300 http://10.19.52.8:8080/hello/testOK?access_token=e0345712-c30d-4bf8-ae61-8cae1ec38c52 1234567891011121314151617181920212223242526272829303132Document Path: /hello/testOK?access_token=e0345712-c30d-4bf8-ae61-8cae1ec38c52Document Length: 2 bytesConcurrency Level: 300Time taken for tests: 151.026 secondsComplete requests: 100000Failed requests: 0Write errors: 0Total transferred: 42200844 bytesHTML transferred: 200004 bytes**Requests per second: 662.14 [#/sec] (mean)**Time per request: 453.078 [ms] (mean)Time per request: 1.510 [ms] (mean, across all concurrent requests)Transfer rate: 272.88 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 5 7.0 2 98Processing: 206 447 478.7 230 3171Waiting: 197 445 478.7 227 3165Total: 206 451 478.8 236 3177Percentage of the requests served within a certain time (ms) 50% 236 66% 250 75% 273 80% 322 90% 1408 95% 1506 98% 1684 99% 1764 100% 3177 (longest request) 测试二：1.直接访问应用，应用端接口延迟200ms后返回一个字符串，模拟真实接口的业务处理延迟2.300个线程并发请求，共计100000 次 1ab -n 100000 -c 300 http://10.19.52.9:9091/testOK 1234567891011121314151617181920212223242526272829303132333435Server Hostname: 10.19.52.9Server Port: 9091Document Path: /testOKDocument Length: 2 bytesConcurrency Level: 300Time taken for tests: 69.003 secondsComplete requests: 100000Failed requests: 0Write errors: 0Total transferred: 13400000 bytesHTML transferred: 200000 bytes**Requests per second: 1449.21 [#/sec] (mean)**Time per request: 207.009 [ms] (mean)Time per request: 0.690 [ms] (mean, across all concurrent requests)Transfer rate: 189.64 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 0 0.8 0 10Processing: 200 206 7.7 202 286Waiting: 200 205 7.7 202 286Total: 201 206 7.9 203 295Percentage of the requests served within a certain time (ms) 50% 203 66% 205 75% 207 80% 209 90% 215 95% 220 98% 229 99% 240 100% 295 (longest request) 经过网关路由之后的性能下降是不可避免的，在测试过程中，查看监控端的线程变化，如下图： 我们的配置的确产生了作用。 我们再来分析一下上面测试结果的一个重要指标：Requests per second，我们的网关经过了鉴权之后，性能仍然可以达到600+每秒的响应，是完全可以接受的，峰值时内存情况，使用top指令，如下所示：ab测试命令也占用了一定的cpu使用率，总应用接近70%的cpu使用率，这估计也是单个tomcat实例的瓶颈了。因为我们的应用服务器会单独部署网关，并且可以在多个服务器上部署多个实例，所以这个结果可以接受。 为了避免单次响应带来的偶然因素，我们重复进行测试一（更改为10000次请求，并发量200），看看Requests per second的变化。 123451. 799.452. 818.863. 838.674. 833.905. 973.65 总结有一些其他的数据没有整理到博客中，但是也顺便把结论写一下。 这次的测试有几个注意点： 是在应用服务器端模拟200ms的延时，因为实际请求不可能不伴随着耗时的业务操作，实际发现对ab的测试影响还是较大的，毕竟线程阻塞着，不延迟时request per second能达到2000，加了200ms延迟之后下降到1000+。 模拟总请求数和线程数的变化会引起QPS/TPS的抖动，即使是在多核CPU的承受范围之内，也并不是说线程越多，QPS/TPS就越高，因为启动线程的开销，以及线程上下文切换的耗时，开辟线程带来的内存损耗都会影响性能。钱总说单个tomcat实例的并发度理论值200就可以接受了，经过参数调优后的tomcat使用zuul做网关能达到如上的测试结果，完全可以投入生产环境使用了。而tomcat默认的150线程，如果使用200的并发度测试就显然是“不公平的”。 测试注意点有几个，例如ab部署在了api-gateway本机会影响性能，tomcat参数以及zuul参数应当尽可能放开，不让其默认配置影响测试。 本文还有些遗漏的数据，后续会补上…","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud Zuul","slug":"Spring-Cloud-Zuul","permalink":"http://lexburner.github.io/tags/Spring-Cloud-Zuul/"}]},{"title":"springcloud----Zuul动态路由","slug":"springcloud-zuul-dynamic-route","date":"2017-04-01T06:11:52.000Z","updated":"2017-08-22T06:34:10.169Z","comments":true,"path":"2017/04/01/springcloud-zuul-dynamic-route/","link":"","permalink":"http://lexburner.github.io/2017/04/01/springcloud-zuul-dynamic-route/","excerpt":"前言Zuul 是Netflix 提供的一个开源组件,致力于在云平台上提供动态路由，监控，弹性，安全等边缘服务的框架。也有很多公司使用它来作为网关的重要组成部分，碰巧今年公司的架构组决定自研一个网关产品，集动态路由，动态权限，限流配额等功能为一体，为其他部门的项目提供统一的外网调用管理，最终形成产品(这方面阿里其实已经有成熟的网关产品了，但是不太适用于个性化的配置，也没有集成权限和限流降级)。 不过这里并不想介绍整个网关的架构，而是想着重于讨论其中的一个关键点，并且也是经常在交流群中听人说起的：动态路由怎么做？ 再阐释什么是动态路由之前，需要介绍一下架构的设计。 传统互联网架构图上图是没有网关参与的一个最典型的互联网架构(本文中统一使用book代表应用实例，即真正提供服务的一个业务系统) 加入eureka的架构图book注册到eureka注册中心中，zuul本身也连接着同一个eureka，可以拉取book众多实例的列表。服务中心的注册发现一直是值得推崇的一种方式，但是不适用与网关产品。因为我们的网关是面向众多的其他部门的已有或是异构架构的系统，不应该强求其他系统都使用eureka，这样是有侵入性的设计。 最终架构图要强调的一点是，gateway最终也会部署多个实例，达到分布式的效果，在架构图中没有画出，请大家自行脑补。 本博客的示例使用最后一章架构图为例，带来动态路由的实现方式，会有具体的代码。","text":"前言Zuul 是Netflix 提供的一个开源组件,致力于在云平台上提供动态路由，监控，弹性，安全等边缘服务的框架。也有很多公司使用它来作为网关的重要组成部分，碰巧今年公司的架构组决定自研一个网关产品，集动态路由，动态权限，限流配额等功能为一体，为其他部门的项目提供统一的外网调用管理，最终形成产品(这方面阿里其实已经有成熟的网关产品了，但是不太适用于个性化的配置，也没有集成权限和限流降级)。 不过这里并不想介绍整个网关的架构，而是想着重于讨论其中的一个关键点，并且也是经常在交流群中听人说起的：动态路由怎么做？ 再阐释什么是动态路由之前，需要介绍一下架构的设计。 传统互联网架构图上图是没有网关参与的一个最典型的互联网架构(本文中统一使用book代表应用实例，即真正提供服务的一个业务系统) 加入eureka的架构图book注册到eureka注册中心中，zuul本身也连接着同一个eureka，可以拉取book众多实例的列表。服务中心的注册发现一直是值得推崇的一种方式，但是不适用与网关产品。因为我们的网关是面向众多的其他部门的已有或是异构架构的系统，不应该强求其他系统都使用eureka，这样是有侵入性的设计。 最终架构图要强调的一点是，gateway最终也会部署多个实例，达到分布式的效果，在架构图中没有画出，请大家自行脑补。 本博客的示例使用最后一章架构图为例，带来动态路由的实现方式，会有具体的代码。 动态路由动态路由需要达到可持久化配置，动态刷新的效果。如架构图所示，不仅要能满足从spring的配置文件properties加载路由信息，还需要从数据库加载我们的配置。另外一点是，路由信息在容器启动时就已经加载进入了内存，我们希望配置完成后，实施发布，动态刷新内存中的路由信息，达到不停机维护路由信息的效果。 zuul–HelloWorldDemo项目结构123456789101112131415161718192021222324252627&lt;groupId&gt;com.sinosoft&lt;/groupId&gt; &lt;artifactId&gt;zuul-gateway-demo&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;modules&gt; &lt;module&gt;gateway&lt;/module&gt; &lt;module&gt;book&lt;/module&gt; &lt;/modules&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Camden.SR6&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; tip：springboot-1.5.2对应的springcloud的版本需要使用Camden.SR6，一开始想专门写这个demo时，只替换了springboot的版本1.4.0-&gt;1.5.2，结果启动就报错了，最后发现是版本不兼容的锅。 gateway项目：启动类：GatewayApplication.java123456789@EnableZuulProxy@SpringBootApplicationpublic class GatewayApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GatewayApplication.class, args); &#125;&#125; 配置：application.properties 1234567#配置在配置文件中的路由信息zuul.routes.books.url=http://localhost:8090zuul.routes.books.path=/books/**#不使用注册中心,会带来侵入性ribbon.eureka.enabled=false#网关端口server.port=8080 book项目：启动类：BookApplication.java 12345678910111213141516171819@RestController@SpringBootApplicationpublic class BookApplication &#123; @RequestMapping(value = \"/available\") public String available() &#123; System.out.println(\"Spring in Action\"); return \"Spring in Action\"; &#125; @RequestMapping(value = \"/checked-out\") public String checkedOut() &#123; return \"Spring Boot in Action\"; &#125; public static void main(String[] args) &#123; SpringApplication.run(BookApplication.class, args); &#125;&#125; 配置类：application.properties 1server.port=8090 测试访问：http://localhost:8080/books/available 上述demo是一个简单的静态路由，简单看下源码，zuul是怎么做到转发，路由的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140@Configuration@EnableConfigurationProperties(&#123; ZuulProperties.class &#125;)@ConditionalOnClass(ZuulServlet.class)@Import(ServerPropertiesAutoConfiguration.class)public class ZuulConfiguration &#123; @Autowired //zuul的配置文件,对应了application.properties中的配置信息 protected ZuulProperties zuulProperties; @Autowired protected ServerProperties server; @Autowired(required = false) private ErrorController errorController; @Bean public HasFeatures zuulFeature() &#123; return HasFeatures.namedFeature(\"Zuul (Simple)\", ZuulConfiguration.class); &#125; //核心类，路由定位器，最最重要 @Bean @ConditionalOnMissingBean(RouteLocator.class) public RouteLocator routeLocator() &#123; //默认配置的实现是SimpleRouteLocator.class return new SimpleRouteLocator(this.server.getServletPrefix(), this.zuulProperties); &#125; //zuul的控制器，负责处理链路调用 @Bean public ZuulController zuulController() &#123; return new ZuulController(); &#125; //MVC HandlerMapping that maps incoming request paths to remote services. @Bean public ZuulHandlerMapping zuulHandlerMapping(RouteLocator routes) &#123; ZuulHandlerMapping mapping = new ZuulHandlerMapping(routes, zuulController()); mapping.setErrorController(this.errorController); return mapping; &#125; //注册了一个路由刷新监听器，默认实现是ZuulRefreshListener.class，这个是我们动态路由的关键 @Bean public ApplicationListener&lt;ApplicationEvent&gt; zuulRefreshRoutesListener() &#123; return new ZuulRefreshListener(); &#125; @Bean @ConditionalOnMissingBean(name = \"zuulServlet\") public ServletRegistrationBean zuulServlet() &#123; ServletRegistrationBean servlet = new ServletRegistrationBean(new ZuulServlet(), this.zuulProperties.getServletPattern()); // The whole point of exposing this servlet is to provide a route that doesn't // buffer requests. servlet.addInitParameter(\"buffer-requests\", \"false\"); return servlet; &#125; // pre filters @Bean public ServletDetectionFilter servletDetectionFilter() &#123; return new ServletDetectionFilter(); &#125; @Bean public FormBodyWrapperFilter formBodyWrapperFilter() &#123; return new FormBodyWrapperFilter(); &#125; @Bean public DebugFilter debugFilter() &#123; return new DebugFilter(); &#125; @Bean public Servlet30WrapperFilter servlet30WrapperFilter() &#123; return new Servlet30WrapperFilter(); &#125; // post filters @Bean public SendResponseFilter sendResponseFilter() &#123; return new SendResponseFilter(); &#125; @Bean public SendErrorFilter sendErrorFilter() &#123; return new SendErrorFilter(); &#125; @Bean public SendForwardFilter sendForwardFilter() &#123; return new SendForwardFilter(); &#125; @Configuration protected static class ZuulFilterConfiguration &#123; @Autowired private Map&lt;String, ZuulFilter&gt; filters; @Bean public ZuulFilterInitializer zuulFilterInitializer() &#123; return new ZuulFilterInitializer(this.filters); &#125; &#125; //上面提到的路由刷新监听器 private static class ZuulRefreshListener implements ApplicationListener&lt;ApplicationEvent&gt; &#123; @Autowired private ZuulHandlerMapping zuulHandlerMapping; private HeartbeatMonitor heartbeatMonitor = new HeartbeatMonitor(); @Override public void onApplicationEvent(ApplicationEvent event) &#123; if (event instanceof ContextRefreshedEvent || event instanceof RefreshScopeRefreshedEvent || event instanceof RoutesRefreshedEvent) &#123; //设置为脏,下一次匹配到路径时，如果发现为脏，则会去刷新路由信息 this.zuulHandlerMapping.setDirty(true); &#125; else if (event instanceof HeartbeatEvent) &#123; if (this.heartbeatMonitor.update(((HeartbeatEvent) event).getValue())) &#123; this.zuulHandlerMapping.setDirty(true); &#125; &#125; &#125; &#125;&#125; 我们要解决动态路由的难题，第一步就得理解路由定位器的作用。很失望，因为从接口关系来看，spring考虑到了路由刷新的需求，但是默认实现的SimpleRouteLocator没有实现RefreshableRouteLocator接口，看来我们只能借鉴DiscoveryClientRouteLocator去改造SimpleRouteLocator使其具备刷新能力。123public interface RefreshableRouteLocator extends RouteLocator &#123; void refresh();&#125; DiscoveryClientRouteLocator比SimpleRouteLocator多了两个功能，第一是从DiscoveryClient（如Eureka）发现路由信息，之前的架构图已经给大家解释清楚了，我们不想使用eureka这种侵入式的网关模块，所以忽略它，第二是实现了RefreshableRouteLocator接口，能够实现动态刷新。对SimpleRouteLocator.class的源码加一些注释，方便大家阅读： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165public class SimpleRouteLocator implements RouteLocator &#123; //配置文件中的路由信息配置 private ZuulProperties properties; //路径正则配置器,即作用于path:/books/** private PathMatcher pathMatcher = new AntPathMatcher(); private String dispatcherServletPath = \"/\"; private String zuulServletPath; private AtomicReference&lt;Map&lt;String, ZuulRoute&gt;&gt; routes = new AtomicReference&lt;&gt;(); public SimpleRouteLocator(String servletPath, ZuulProperties properties) &#123; this.properties = properties; if (servletPath != null &amp;&amp; StringUtils.hasText(servletPath)) &#123; this.dispatcherServletPath = servletPath; &#125; this.zuulServletPath = properties.getServletPath(); &#125; //路由定位器和其他组件的交互，是最终把定位的Routes以list的方式提供出去,核心实现 @Override public List&lt;Route&gt; getRoutes() &#123; if (this.routes.get() == null) &#123; this.routes.set(locateRoutes()); &#125; List&lt;Route&gt; values = new ArrayList&lt;&gt;(); for (String url : this.routes.get().keySet()) &#123; ZuulRoute route = this.routes.get().get(url); String path = route.getPath(); values.add(getRoute(route, path)); &#125; return values; &#125; @Override public Collection&lt;String&gt; getIgnoredPaths() &#123; return this.properties.getIgnoredPatterns(); &#125; //这个方法在网关产品中也很重要，可以根据实际路径匹配到Route来进行业务逻辑的操作，进行一些加工 @Override public Route getMatchingRoute(final String path) &#123; if (log.isDebugEnabled()) &#123; log.debug(\"Finding route for path: \" + path); &#125; if (this.routes.get() == null) &#123; this.routes.set(locateRoutes()); &#125; if (log.isDebugEnabled()) &#123; log.debug(\"servletPath=\" + this.dispatcherServletPath); log.debug(\"zuulServletPath=\" + this.zuulServletPath); log.debug(\"RequestUtils.isDispatcherServletRequest()=\" + RequestUtils.isDispatcherServletRequest()); log.debug(\"RequestUtils.isZuulServletRequest()=\" + RequestUtils.isZuulServletRequest()); &#125; String adjustedPath = adjustPath(path); ZuulRoute route = null; if (!matchesIgnoredPatterns(adjustedPath)) &#123; for (Entry&lt;String, ZuulRoute&gt; entry : this.routes.get().entrySet()) &#123; String pattern = entry.getKey(); log.debug(\"Matching pattern:\" + pattern); if (this.pathMatcher.match(pattern, adjustedPath)) &#123; route = entry.getValue(); break; &#125; &#125; &#125; if (log.isDebugEnabled()) &#123; log.debug(\"route matched=\" + route); &#125; return getRoute(route, adjustedPath); &#125; private Route getRoute(ZuulRoute route, String path) &#123; if (route == null) &#123; return null; &#125; String targetPath = path; String prefix = this.properties.getPrefix(); if (path.startsWith(prefix) &amp;&amp; this.properties.isStripPrefix()) &#123; targetPath = path.substring(prefix.length()); &#125; if (route.isStripPrefix()) &#123; int index = route.getPath().indexOf(\"*\") - 1; if (index &gt; 0) &#123; String routePrefix = route.getPath().substring(0, index); targetPath = targetPath.replaceFirst(routePrefix, \"\"); prefix = prefix + routePrefix; &#125; &#125; Boolean retryable = this.properties.getRetryable(); if (route.getRetryable() != null) &#123; retryable = route.getRetryable(); &#125; return new Route(route.getId(), targetPath, route.getLocation(), prefix, retryable, route.isCustomSensitiveHeaders() ? route.getSensitiveHeaders() : null); &#125; //注意这个类并没有实现refresh接口，但是却提供了一个protected级别的方法,旨在让子类不需要重复维护一个private AtomicReference&lt;Map&lt;String, ZuulRoute&gt;&gt; routes = new AtomicReference&lt;&gt;();也可以达到刷新的效果 protected void doRefresh() &#123; this.routes.set(locateRoutes()); &#125; //具体就是在这儿定位路由信息的，我们之后从数据库加载路由信息，主要也是从这儿改写 /** * Compute a map of path pattern to route. The default is just a static map from the * &#123;@link ZuulProperties&#125;, but subclasses can add dynamic calculations. */ protected Map&lt;String, ZuulRoute&gt; locateRoutes() &#123; LinkedHashMap&lt;String, ZuulRoute&gt; routesMap = new LinkedHashMap&lt;String, ZuulRoute&gt;(); for (ZuulRoute route : this.properties.getRoutes().values()) &#123; routesMap.put(route.getPath(), route); &#125; return routesMap; &#125; protected boolean matchesIgnoredPatterns(String path) &#123; for (String pattern : this.properties.getIgnoredPatterns()) &#123; log.debug(\"Matching ignored pattern:\" + pattern); if (this.pathMatcher.match(pattern, path)) &#123; log.debug(\"Path \" + path + \" matches ignored pattern \" + pattern); return true; &#125; &#125; return false; &#125; private String adjustPath(final String path) &#123; String adjustedPath = path; if (RequestUtils.isDispatcherServletRequest() &amp;&amp; StringUtils.hasText(this.dispatcherServletPath)) &#123; if (!this.dispatcherServletPath.equals(\"/\")) &#123; adjustedPath = path.substring(this.dispatcherServletPath.length()); log.debug(\"Stripped dispatcherServletPath\"); &#125; &#125; else if (RequestUtils.isZuulServletRequest()) &#123; if (StringUtils.hasText(this.zuulServletPath) &amp;&amp; !this.zuulServletPath.equals(\"/\")) &#123; adjustedPath = path.substring(this.zuulServletPath.length()); log.debug(\"Stripped zuulServletPath\"); &#125; &#125; else &#123; // do nothing &#125; log.debug(\"adjustedPath=\" + path); return adjustedPath; &#125;&#125; 重写过后的自定义路由定位器如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169public class CustomRouteLocator extends SimpleRouteLocator implements RefreshableRouteLocator&#123; public final static Logger logger = LoggerFactory.getLogger(CustomRouteLocator.class); private JdbcTemplate jdbcTemplate; private ZuulProperties properties; public void setJdbcTemplate(JdbcTemplate jdbcTemplate)&#123; this.jdbcTemplate = jdbcTemplate; &#125; public CustomRouteLocator(String servletPath, ZuulProperties properties) &#123; super(servletPath, properties); this.properties = properties; logger.info(\"servletPath:&#123;&#125;\",servletPath); &#125; //父类已经提供了这个方法，这里写出来只是为了说明这一个方法很重要！！！// @Override// protected void doRefresh() &#123;// super.doRefresh();// &#125; @Override public void refresh() &#123; doRefresh(); &#125; @Override protected Map&lt;String, ZuulRoute&gt; locateRoutes() &#123; LinkedHashMap&lt;String, ZuulRoute&gt; routesMap = new LinkedHashMap&lt;String, ZuulRoute&gt;(); //从application.properties中加载路由信息 routesMap.putAll(super.locateRoutes()); //从db中加载路由信息 routesMap.putAll(locateRoutesFromDB()); //优化一下配置 LinkedHashMap&lt;String, ZuulRoute&gt; values = new LinkedHashMap&lt;&gt;(); for (Map.Entry&lt;String, ZuulRoute&gt; entry : routesMap.entrySet()) &#123; String path = entry.getKey(); // Prepend with slash if not already present. if (!path.startsWith(\"/\")) &#123; path = \"/\" + path; &#125; if (StringUtils.hasText(this.properties.getPrefix())) &#123; path = this.properties.getPrefix() + path; if (!path.startsWith(\"/\")) &#123; path = \"/\" + path; &#125; &#125; values.put(path, entry.getValue()); &#125; return values; &#125; private Map&lt;String, ZuulRoute&gt; locateRoutesFromDB()&#123; Map&lt;String, ZuulRoute&gt; routes = new LinkedHashMap&lt;&gt;(); List&lt;ZuulRouteVO&gt; results = jdbcTemplate.query(\"select * from gateway_api_define where enabled = true \",new BeanPropertyRowMapper&lt;&gt;(ZuulRouteVO.class)); for (ZuulRouteVO result : results) &#123; if(org.apache.commons.lang3.StringUtils.isBlank(result.getPath()) || org.apache.commons.lang3.StringUtils.isBlank(result.getUrl()) )&#123; continue; &#125; ZuulRoute zuulRoute = new ZuulRoute(); try &#123; org.springframework.beans.BeanUtils.copyProperties(result,zuulRoute); &#125; catch (Exception e) &#123; logger.error(\"=============load zuul route info from db with error==============\",e); &#125; routes.put(zuulRoute.getPath(),zuulRoute); &#125; return routes; &#125; public static class ZuulRouteVO &#123; /** * The ID of the route (the same as its map key by default). */ private String id; /** * The path (pattern) for the route, e.g. /foo/**. */ private String path; /** * The service ID (if any) to map to this route. You can specify a physical URL or * a service, but not both. */ private String serviceId; /** * A full physical URL to map to the route. An alternative is to use a service ID * and service discovery to find the physical address. */ private String url; /** * Flag to determine whether the prefix for this route (the path, minus pattern * patcher) should be stripped before forwarding. */ private boolean stripPrefix = true; /** * Flag to indicate that this route should be retryable (if supported). Generally * retry requires a service ID and ribbon. */ private Boolean retryable; private Boolean enabled; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public String getPath() &#123; return path; &#125; public void setPath(String path) &#123; this.path = path; &#125; public String getServiceId() &#123; return serviceId; &#125; public void setServiceId(String serviceId) &#123; this.serviceId = serviceId; &#125; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; public boolean isStripPrefix() &#123; return stripPrefix; &#125; public void setStripPrefix(boolean stripPrefix) &#123; this.stripPrefix = stripPrefix; &#125; public Boolean getRetryable() &#123; return retryable; &#125; public void setRetryable(Boolean retryable) &#123; this.retryable = retryable; &#125; public Boolean getEnabled() &#123; return enabled; &#125; public void setEnabled(Boolean enabled) &#123; this.enabled = enabled; &#125; &#125;&#125; 配置这个自定义的路由定位器： 123456789101112131415161718@Configurationpublic class CustomZuulConfig &#123; @Autowired ZuulProperties zuulProperties; @Autowired ServerProperties server; @Autowired JdbcTemplate jdbcTemplate; @Bean public CustomRouteLocator routeLocator() &#123; CustomRouteLocator routeLocator = new CustomRouteLocator(this.server.getServletPrefix(), this.zuulProperties); routeLocator.setJdbcTemplate(jdbcTemplate); return routeLocator; &#125;&#125; 现在容器启动时，就可以从数据库和配置文件中一起加载路由信息了，离动态路由还差最后一步，就是实时刷新，前面已经说过了，默认的ZuulConfigure已经配置了事件监听器，我们只需要发送一个事件就可以实现刷新了。 1234567891011121314public class RefreshRouteService &#123; @Autowired ApplicationEventPublisher publisher; @Autowired RouteLocator routeLocator; public void refreshRoute() &#123; RoutesRefreshedEvent routesRefreshedEvent = new RoutesRefreshedEvent(routeLocator); publisher.publishEvent(routesRefreshedEvent); &#125;&#125; 具体的刷新流程其实就是从数据库重新加载了一遍，有人可能会问，为什么不自己是手动重新加载Locator.dorefresh？非要用事件去刷新。这牵扯到内部的zuul内部组件的工作流程，不仅仅是Locator本身的一个变量，具体想要了解的还得去看源码。 到这儿我们就实现了动态路由了，所以的实例代码和建表语句我会放到github上，下载的时候记得给我star QAQ github地址","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud Zuul","slug":"Spring-Cloud-Zuul","permalink":"http://lexburner.github.io/tags/Spring-Cloud-Zuul/"}]},{"title":"分布式限流","slug":"distribute-ratelimit","date":"2017-03-18T05:52:00.000Z","updated":"2017-08-22T06:40:13.761Z","comments":true,"path":"2017/03/18/distribute-ratelimit/","link":"","permalink":"http://lexburner.github.io/2017/03/18/distribute-ratelimit/","excerpt":"","text":"前言最近正在为本科论文的事感到心烦，一方面是在调研期间，发现大部分的本科论文都是以MVC为架构，如果是使用了java作为开发语言则又是千篇一律的在使用SSH，二方面是自己想就微服务，分布式方面写一篇论文，讲述一些技术点的实现，和一些中间件的使用，看到如八股文般的模板格式..不免让人望文生怯。退一步，投入模板化ssh-web项目的怀抱，落入俗套，可以省去自己不少时间，因为在外实习，琐事并不少；进一步，需要投入大量时间精力去研究，而且不成体系，没有论文参考。 突然觉得写博客，比写论文爽多了，可以写自己想写的，记录自己最真实的想法。可能会逐渐将之前博客维护的自己的一些想法，纳入到本科论文中去。 经典限流算法说回正题，补上之前分布式限流的实现。先介绍一些现有的限流方案。 核心的算法主要就是四种：A类：计数器法，滑动窗口法B类：令牌桶法，漏桶法 这里的四种算法通常都是在应用级别讨论的，这里不重复介绍这四种算法的实现思路了，只不过我人为的将他们分成了A，B两类。 A类算法，是否决式限流。即如果系统设定限流方案是1分钟允许100次调用，那么真实请求1分钟调用200次的话，意味着超出的100次调用，得到的是空结果或者调用频繁异常。 B类算法，是阻塞式限流。即如果系统设定限流方案是1分钟允许100次调用，那么真实请求1分钟调用200次的话，意味着超出的100次调用，会均匀安排到下一分钟返回。（当然B类算法，也可以立即返回失败，也可以达到否决式限流的效果） B类算法，如Guava包提供的RateLimiter，内部其实就是一个阻塞队列，达到阻塞限流的效果。然后分布式场景下，有一些思路悄悄的发生了变化。多个模块之间不能保证相互阻塞，共享的变量也不在一片内存空间中。为了使用阻塞限流的算法，我们不得不将统计流量放到redis一类的共享内存中，如果操作是一系列复合的操作，我们还不能使用redis自带的CAS操作(CAS操作只能保证单个操作的原子性)或者使用中间件级别的队列来阻塞操作，显示加分布式锁的开销又是非常的巨大。最终选择放弃阻塞式限流，而在分布式场景下，仅仅使用redis+lua脚本的方式来达到分布式-否决式限流的效果。redis执行lua脚本是一个单线程的行为，所以不需要显示加锁，这可以说避免了加锁导致的线程切换开销。 锁的演变下面记录一下这个设计的演变过程。 单体式应用中显示加锁首先还是回到单体应用中对共享变量进行+1的例子。12345678910111213141516171819Integer count = 0;//sychronized锁public synchronized void synchronizedIncrement()&#123; count++; &#125;//juc中的lockLock lock = new ReentrantLock(); public void incrementByLock()&#123; lock.lock(); try&#123; count++; &#125;finally &#123; lock.unlock(); &#125; &#125; 用synchronized或者lock同步的方式进行统计，当单位时间内到达限定次数后否决执行。限制：单体应用下有效，分布式场景失效，显示加锁，开销大。 单体式应用中CAS操作 12345public AtomicInteger atomicInteger = new AtomicInteger(0);public increamt()&#123; atomicInteger.incrementAndGet();&#125; 虽然没有显示加锁，但是CAS操作有一定的局限性，限流中不仅要对计数器进行+1，而且还要记录时间段，所以复合操作，还是无法避免加锁。 分布式应用中显示加锁 1234567891011RedisDistributeLock lock = new RedisDistributeLock();public void incrementByLock()&#123; lock.lock(); try&#123; count++; &#125;finally &#123; lock.unlock(); &#125;&#125; 分布式阻塞锁的实现，可以参考我之前的博客。虽然能达到多个模块之间的同步，但还是开销过大。不得已时才会考虑使用。 redis+lua脚本限流（最终方案） 12345678910111213local key = KEYS[1] --限流KEY（一秒一个）local limit = tonumber(ARGV[1]) --限流大小local current = tonumber(redis.call(&apos;get&apos;, key) or &quot;0&quot;)if current + 1 &gt; limit then --如果超出限流大小 redis.call(&quot;INCRBY&quot;, key,&quot;1&quot;) -- 如果不需要统计真是访问量可以不加这行 return 0else --请求数+1，并设置2秒过期 redis.call(&quot;INCRBY&quot;, key,&quot;1&quot;) if tonumber(ARGV[2]) &gt; -1 then redis.call(&quot;expire&quot;, key,tonumber(ARGV[2])) --时间窗口最大时间后销毁键 end return 1end lua脚本返回值比较奇怪，用java客户端接受返回值，只能使用Long，没有去深究。这个脚本只需要传入key（url+时间戳/预设时间窗口大小），便可以实现限流。这里也贴下java中配套的工具类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package sinosoftgz.apiGateway.utils;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.core.script.RedisScript;import org.springframework.util.Assert;import java.util.Arrays;/** * Created by xujingfeng on 2017/3/13. * &lt;p&gt; * 基于redis lua脚本的线程安全的计数器限流方案 * &lt;/p&gt; */public class RedisRateLimiter &#123; /** * 限流访问的url */ private String url; /** * 单位时间的大小,最大值为 Long.MAX_VALUE - 1,以秒为单位 */ final Long timeUnit; /** * 单位时间窗口内允许的访问次数 */ final Integer limit; /** * 需要传入一个lua script,莫名其妙redisTemplate返回值永远是个Long */ private RedisScript&lt;Long&gt; redisScript; private RedisTemplate redisTemplate; /** * 配置键是否会过期， * true：可以用来做接口流量统计，用定时器去删除 * false：过期自动删除，时间窗口过小的话会导致键过多 */ private boolean isDurable = false; public void setRedisScript(RedisScript&lt;Long&gt; redisScript) &#123; this.redisScript = redisScript; &#125; public void setRedisTemplate(RedisTemplate redisTemplate) &#123; this.redisTemplate = redisTemplate; &#125; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; public boolean isDurable() &#123; return isDurable; &#125; public void setDurable(boolean durable) &#123; isDurable = durable; &#125; public RedisRateLimiter(Integer limit, Long timeUnit) &#123; this.timeUnit = timeUnit; Assert.isTrue(timeUnit &lt; Long.MAX_VALUE - 1); this.limit = limit; &#125; public RedisRateLimiter(Integer limit, Long timeUnit, boolean isDurable) &#123; this(limit, timeUnit); this.isDurable = isDurable; &#125; public boolean acquire() &#123; return this.acquire(this.url); &#125; public boolean acquire(String url) &#123; StringBuffer key = new StringBuffer(); key.append(&quot;rateLimiter&quot;).append(&quot;:&quot;) .append(url).append(&quot;:&quot;) .append(System.currentTimeMillis() / 1000 / timeUnit); Integer expire = limit + 1; String convertExpire = isDurable ? &quot;-1&quot; : expire.toString(); return redisTemplate.execute(redisScript, Arrays.asList(key.toString()), limit.toString(), convertExpire).equals(1l); &#125;&#125; 由此可以见，分布式场景下，一个小小的统计次数的需求，如果真想在分布式下做到最完善，需要花很大的精力。","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/categories/架构设计/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://lexburner.github.io/tags/redis/"},{"name":"lua","slug":"lua","permalink":"http://lexburner.github.io/tags/lua/"}]},{"title":"DevOps的八荣八耻","slug":"devops-1","date":"2017-03-13T16:43:52.000Z","updated":"2017-08-22T06:46:24.382Z","comments":true,"path":"2017/03/14/devops-1/","link":"","permalink":"http://lexburner.github.io/2017/03/14/devops-1/","excerpt":"","text":"前言被群里的好友安利了一发，周日跑去参加了一个技术讲座《云上开发与运维最佳实践》，听完两个人的演讲之后才发现主题竟然是讲运维，好在有一个人干货不少，在此记录下所得。简单追溯了一下这个DevOps才发现并不是一个新的概念，早在2010年就能看到有相关的人在追捧这个概念了。DevOps 就是开发（Development）和运维（Operations）这两个领域的合并。（如果没错的话，DevOps还包括产品管理、QA、winces 甚至销售等领域）。这种理念和现如今流行的微服务架构以及分布式特性的相关理念不谋而合。这篇文章主要就是转载记录了当时又拍云运维总监的演讲稿。 DevOps的八荣八耻DevOps这个思想提出来已经五六年了，一直都是呼声很高，落地很难，为什么呢？这可能与各个公司的业务情况和技术发展路线有或多或少的关系，比如说创业的最早技术合伙人是运维出身或者技术出身，但是水平不高，为了公司持续发展，引入新鲜血液时，就会存在技术的先进性跟解决遗留烂摊子的矛盾。又或者业务本身偏向于用户，导致技术被边缘化，产品又没有好的架构，限制了快速发展等；所以，DevOps的推进一定要自上而下，凭借挑战自我，颠覆传统的勇气才能去落实。 以可配置为荣，以硬编码为耻 △ 以可配置为荣，以硬编码为耻 hardcoding一时爽，真正要做改动时，需要定位代码，做出调整，甚至可能会破坏功能。以下可以说是配置的一个进化史 • 本地配置,程序⽣生成 (txt/ini/cfg)• 集中配置, 动态⽣生成(Yaml/Json)• 环境变量量(代码⽆无侵⼊入&amp;语⾔言⽆无关性)• 服务⾃自动发现,⾃自动注册(zookeeper/consul) 以互备为荣，以单点为耻 △ 以互备为荣，以单点为耻 互容互备一直是优良架构的设计重点。 又拍云早期做架构设计，使用了LVS+Keeplived+VRRP做转换，这样可以方便负载均衡，动态升级，隔离故障。现在的又拍云第二代，已经在部分大节点使用OSPF和Quagga做等价路由的负载均衡和冗余保障。 Nginx可以加Haproxy或LVS做负载均衡。MySQL可以做主从切换，或者是MMM的高可用成熟解决方案。我们的消息队列之前用rabbitmq做，现在主要是redis和kafka集群化，其中kafka已经迁到了Mesos容器平台里。 服务的自动发现、注册，我们可以使用consul、etcd、doozer（Heroku公司产品），还有zookeeper。主要区别是算法不一样，zookeeper用的是paxos算法，而consul用的是raft算法。目前看来consul比较流行，因为consul的自动发现和自动注册更加容易使用。etcd主要是CoreOS在主推，CoreOS本身就是一个滚动发布的针对分布式部署的操作系统，大家可以去关注一下它。还有一个是hadoop和elk，大数据平台的可扩展性是标配，很容易互备。 上面是举了一些常见互备的软件组件的造型，那我们如何是设计一个无单点的架构呢？主要掌握以下几点： 1.无状态 无状态意味着没有竞争，很容易做负载均衡，负载均衡的方式有很多种，F5，LVS，Haproxy，总能找到一种适合你的方式。 2.无共享 以前我们很喜欢用内存来保持临时信息，如进程间的交换，这种方式虽然效率很高，但是对程序的扩展性没什么好处，尤其是现在的互联网体量，光靠单机或者高性能机器是明显玩不转的。所以我们现在就需要使用类似消息队列的组件，把数据共享出去，利用多台机器把负载给承担下来。 3.松耦合/异步处理 以前我们用Gearman这样的任务框架。大家可以把任务丢进任务池里，生成多个消费者去取任务。当我的消费不够用时，可以平滑增加我的work资源，让他从更快的去拿任务。运维平台这边以python/celery的组合使用更多。 4.分布式/集群协作 像Hadoop这样的天生大数据/数据仓库解决方案，由于先前设计比较成熟，一般都是通过很多台机器扩容来实现map/reduce的扩展计算能力。 以随时重启为荣，以不能迁移为耻 △ 以随时重启为荣，以不能迁移为耻 关于这个点，我们讲三个方面： 1.Pet到Cow观念的转变 以前我们说机器是pet，也就是宠物模式，然后花了几万块钱去买的服务器，当宝一般供奉。但事实上却并不是这样，任何电子设备、服务器只要一上线，便开始了一个衰老的过程，你根本不知道在运行过程中会发生什么事，比如说质量差的电容会老化爆浆，电子元器件在机房的恶劣环境里会加速损坏，这些变化都是我们无法参与控制的，所以无论我们怎么努力，都无法保障机器有多么的牢靠。 谷歌指出的Cow模式就是指农场模式。就是要把机器发生故障当做常态，打个比方，比如说这头牛死了，那我就不要了，因为我有很多这样的牛，或者是再拉一头新的牛。这就是我们软件开发和运维需要做的转变，去适应这种变化。 2.OpenStack虚拟机的编排 虚拟化是个好东西，通过OpenStack我们很容易就可以做出一些存储或者迁移的操作，但是在实施的过程中，也是一波三折的。 又拍云从2014年开始在内部推动OpenStack，当然我们也踩过OpenStack网络的坑，那时候我们用双千兆的卡做内网通讯，因为使用OpenStack实现虚拟化后，一切都变成了文件，在网络上传输的话，对网络的压力会非常大，结果就导致部分服务响应缓慢（因为本身就是实验性质，所以在硬件上没有足够投入，内测时也没有推广，所以影响不大）。 2015年又拍云再上的OpenStack，全部都用双万兆的网卡做bonding，交换机也是做了端口聚合和堆叠。目前来说，只有云存储没有上线，其它云处理，云网络的使用还是能够满足要求。 3.Docker的导入导出 Docker是更轻量级的资源隔离和复用技术，从2016年开始，又拍云同时也在尝试使用Mesos/Docker来实现云处理的业务迁移。 以整体交付为荣，以部分交付为耻 △ 以整体交付为荣，以部分交付为耻 以往开发运维要安装一个机器，首先要去申请采购，购买完了还要等待运输，在运输中要花去一天的时间，之后还需要配交换机和网络。在这个过程中你会发现，简单的给开发配台机器，光上架就涉及到运维的很多环节，更不要说系统安装，优化，软件配置等剩余工作了，所以大多数情况下你只能做到部分交付。 要如何解决这些问题？通过OpenStack可以做到云计算、云网络、云存储这三块搭建完成之后，进行整体交付。 根据一些经验总结，在整个云平台当中，云存储的坑最多，云计算、云网络相对来说比较成熟。现在云计算的硬件基本上是基于英特尔CPU的虚拟化技术来硬件指令穿透的，损耗大概2%～5%，这是可以接受的。至于云网络，刚才胡凯（B站运维总监）提到内网包转发效率，我做过一个测试，在OpenStack的内网中，如果MTU默认是1500，万兆网卡的转发率大概为6.7xxGbps。后来我在优化的过程中，也翻查一些文档，看到的数据是可以达到9.5xxGbps，通过不断的摸索，对比测试后发现，如果把内网的MTU搞成大包，如9000时，万兆网卡的存储量直接达到了9.72Gbps左右的。不过，这个MTU需要提前在宿主机上调整好，需要重启生效。所以，这个问题发现得越早越好，这样就可以做到统一调度，分配资源。 Docker的好处是可以做到Build、Shipand Run，一气呵成。无论是对开发，测试，还是运维来说，Docker都是同一份Dockerfile清单，所以使用Docker在公司里的推动就很顺畅。虽然OpenStack也可以一站式交付，整体交付，使用时非常方便。但是对开发来说，他还是拿到一台机器，还是需要去安装软件环境，配置，上线，运行，除了得到机器快一些，对上线服务没有什么大的帮助，所以又拍云现在的Openstack集群一般对内申请开发测试用，外网生产环境还是以Docker容器化部署为主，这也是大家都喜闻乐见的方式，但前提是开发那边能够适应编写Dockerfile（目前是我在内部推动这种变革，如新的项目就强制要求用docker）。 以无状态为荣，以有状态为耻 △ 以无状态为荣，以有状态为耻 有状态的服务真的很麻烦，无论是存在数据库、磁盘开销，还有各种锁等资源的竞争，横向扩展也很差，不能重启，也不能互备。所以，有姿态的服务对于扩展原则来说，就是一场恶梦。如果是说我们解决这个问题，那就要使用解耦和负载均衡的方法去解决问题。 1.使用可靠的中间件 中间件其实最早出现在金融公司、证券公司，后来随着互联网行业不断壮大以后，就用一些高可靠性的号称工业级的消息队列出现，如RabbitMQ，一出来以后，就把中间件拉下神坛。随着中间件民用化，互联网蓬勃发展，是可以把一些服务变成无状态，方便扩展。 2.公共资源池 我们可以通过各种云，容器云、弹性云，做计算单元的弹性扩展。 3.能够被计算 如果你不想存状态，那也可以被计算，比如说Ceph存储，它的创新在于每个数据块都是可计算出来的，这就类似无状态的，每次都算，反正现在的cpu都这么强悍了，所以，无状态是一个命题，在做架构的时候，你脑海里一定要有这个意念，然后再看你用什么样的方式开动脑筋，预先的跟开发，运维沟通好，把应用拆分成一种无状态的最佳组合。 以标准化为荣，以特殊化为耻 △ 以标准化为荣，以特殊化为耻 在标准化方面，我们在这几个方面改良： 1.统一输入输出 统一入口是我加入又拍云后做的第一件事情，我们用一个统一的文本，到现在也在用，然后推送到所有的边缘，服务器上面的组件，要用到的参数，都能从配置里读出来。代码管理方面我们也使用git，git wiki，批量部署我们用ansible（早在2012年，我做了一些比较后，就在公司里推行ansible，看来还是很明智的决定）。 2.统一的流程管理 运维中使用python最多，所以我们使用了yaml和playbook。又拍云有自己的跳板机，通过VPN登陆，目前我们也在试用一个带有审计功能的堡垒机，可以把每个人的操作录制下来，然后再去回放观察，改进我们的工作流程。 3.抽象底层设计和复用组件 如果是开发者的话，就会写很多的复用函数，对于优秀的运维人员来说，也要有优秀的抽象业务的能力，也要去做一些重复工作的复用准备，如频繁的，繁琐易出错的手工操作抽象成若干运维的脚本化。 最后是巧妙的利用虚拟化、容器服务、server-less微服务，这些服务是可以被备份，还原的，可以保持一个相对稳定的状态，我们要拒绝多的特殊管理操作。香农-信息熵理论里说，变量的不确定性越大，熵就越大，把它搞清楚所需要的信息量也就越大。理论上来说，如果是一个孤立的系统，他就会变得越来越乱。 以自动化工具为荣，以手动和人肉为耻 △ 以自动化工具为荣，以手动和人肉为耻 又拍云早期，用的是bash、sed、awk，因为我之前有搞嵌入式的背景和经验，对一个十几兆的嵌入式系统来说，上面是不可能有python/perl/nodejs等环境。所以我们把服务器批量安装，部署，上线，做成了嵌入式的系统后，只要点亮以后，运行一个硬件检测的程序，会把机器的CPU、内存、硬盘大小等都打印出来，供货商截图给我看，这个机器是否合格。合格的机器可以直接发到机房去，在机器到了机房通上网线以后会有一个ansibleplaybook的推动。 自从用了这种方法以后，我们在公司里面基本上没有见到服务器，一般直接产线上检测通过后发到机房。然后又拍云的运维人员就可以连上去远程管理，在过去的三年里我们服务器平均每年翻了三倍，节点翻了六倍多，但是人手并没有增加。 关于tgz、rpm、pkg的打包部署，我们用的是tgz的打包及docker镜像。优势在于，又拍云自有CDN网络，软件通过推动到CDN网络下可以加速下发。 关于集成测试、自动测试的发布，像ELK集中日志的分析、大数据的分析，我们现在使用ELK以后，只要有基础的运维技术知识便可看懂，不需要高深的运维知识和脚本编辑知识，大多数人都可以完成这份工作，好处就是你多了好多眼睛帮你一起来发现问题，定位问题。 最后是不要图形，不要交互，不要终端。一旦有了图形以后，很难实现自动化。原则就是，不要手工hack，最好是用程序生成程序的方式去完成这个步骤。 以无人值守为荣，以人工介入为耻 △ 以无人值守为荣，以人工介入为耻 运维部门要做的事情有三件： 1.运维自动化 要有一定的业务抽象能力，要有标准化的流程。没有好的自动化，就很难把运维的工作效率提升了，只要做好这些，就可以节省时间，从容应对业务增长。而且运维自动化的另一个好处就是运维不会因为人的喜怒哀乐而受到影响稳定性，比如说我今天心情不好，你让我装一台机器我还可以忍，你让我装十台一百台就不行了。但如果公司有了运维自动化的流程，这个事情就可以避免，因为谁做都一样。 2.监控要常态 2016年年初，又拍云特别成立大数据分析部门，我们把日志做了采样收集和过滤，通过大数据平台做日志的同构数据分析，重点关注4xx/5xx/2xx比例，响应时间分析如100毫秒、200毫秒、500毫秒，还有区域性的速率分布，讲真，这真是一个好东西。 3.性能可视化 数据的有效展示。现在ELK对我们的帮助很大，从监控图上来看相关的数据指标，一目了然。这里就不反复赘述了。 DevOps的本质最后，我们谈一谈DevOps的本质。 弹性 像亚马逊推云时，那个单词叫elastic，意思是，你要能够扩展，如横向扩展；你要能负载均衡，如果你是基于openstack/docker资源池，你的资源就可以复用，可以编排回滚。比如说OpenStack有模板，我打一个镜像包，稍微重了一点，Docker的就轻一点，Docker可以做一个滚动发布，可以保留原来的程序、原来的容器，你可以做快速切换，这也是一种变化的弹性。 无关性 如果是虚拟化资源，一切都可以在模板里面设置，可以把底层的硬件、系统、网络抚平差异，比如说不管物理磁盘是1T(市面上缺货)/4T/6T的盘，都可以划分100G容量，所以当把一切变成按需申请的服务，无论是开发还是运维，工作都会比较简单，因为它的无关性。 不可变的基础设施 这个对传统运维可能是一种打击，因为基础镜像可能已经做的足够安全，足够完美，足够精干，不需要基础运维过多的人工参与。但我认为恰恰能帮助传统运维减轻工作量，反而有更多的精力去迎接虚拟化、容器化，SDN的挑战，掌握了新技能后，就可以随取随用。","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://lexburner.github.io/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"http://lexburner.github.io/tags/DevOps/"}]},{"title":"java并发实践--ConcurrentHashMap与CAS","slug":"java-ConcurrentHashMap-CAS","date":"2017-03-11T16:02:00.000Z","updated":"2017-08-22T06:56:00.534Z","comments":true,"path":"2017/03/12/java-ConcurrentHashMap-CAS/","link":"","permalink":"http://lexburner.github.io/2017/03/12/java-ConcurrentHashMap-CAS/","excerpt":"前言最近在做接口限流时涉及到了一个有意思问题，牵扯出了关于concurrentHashMap的一些用法，以及CAS的一些概念。限流算法很多，我主要就以最简单的计数器法来做引。先抽象化一下需求：统计每个接口访问的次数。一个接口对应一个url，也就是一个字符串，每调用一次对其进行加一处理。可能出现的问题主要有三个： 多线程访问，需要选择合适的并发容器 分布式下多个实例统计接口流量需要共享内存 流量统计应该尽可能不损耗服务器性能 但这次的博客并不是想描述怎么去实现接口限流，而是主要想描述一下遇到的问题，所以，第二点暂时不考虑，即不使用redis。 说到并发的字符串统计，立即让人联想到的数据结构便是ConcurrentHashpMap&lt;String,Long&gt; urlCounter;","text":"前言最近在做接口限流时涉及到了一个有意思问题，牵扯出了关于concurrentHashMap的一些用法，以及CAS的一些概念。限流算法很多，我主要就以最简单的计数器法来做引。先抽象化一下需求：统计每个接口访问的次数。一个接口对应一个url，也就是一个字符串，每调用一次对其进行加一处理。可能出现的问题主要有三个： 多线程访问，需要选择合适的并发容器 分布式下多个实例统计接口流量需要共享内存 流量统计应该尽可能不损耗服务器性能 但这次的博客并不是想描述怎么去实现接口限流，而是主要想描述一下遇到的问题，所以，第二点暂时不考虑，即不使用redis。 说到并发的字符串统计，立即让人联想到的数据结构便是ConcurrentHashpMap&lt;String,Long&gt; urlCounter; 如果你刚刚接触并发可能会写出如代码清单1的代码 代码清单112345678910111213141516171819202122232425262728293031323334353637383940414243444546public class CounterDemo1 &#123; private final Map&lt;String, Long&gt; urlCounter = new ConcurrentHashMap&lt;&gt;(); //接口调用次数+1 public long increase(String url) &#123; Long oldValue = urlCounter.get(url); Long newValue = (oldValue == null) ? 1L : oldValue + 1; urlCounter.put(url, newValue); return newValue; &#125; //获取调用次数 public Long getCount(String url)&#123; return urlCounter.get(url); &#125; public static void main(String[] args) &#123; ExecutorService executor = Executors.newFixedThreadPool(10); final CounterDemo1 counterDemo = new CounterDemo1(); int callTime = 100000; final String url = \"http://localhost:8080/hello\"; CountDownLatch countDownLatch = new CountDownLatch(callTime); //模拟并发情况下的接口调用统计 for(int i=0;i&lt;callTime;i++)&#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; counterDemo.increase(url); countDownLatch.countDown(); &#125; &#125;); &#125; try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; executor.shutdown(); //等待所有线程统计完成后输出调用次数 System.out.println(\"调用次数：\"+counterDemo.getCount(url)); &#125;&#125;console output：调用次数：96526 都说concurrentHashMap是个线程安全的并发容器，所以没有显示加同步，实际效果呢并不如所愿。 问题就出在increase方法，concurrentHashMap能保证的是每一个操作（put，get,delete…）本身是线程安全的，但是我们的increase方法，对concurrentHashMap的操作是一个组合，先get再put，所以多个线程的操作出现了覆盖。如果对整个increase方法加锁，那么又违背了我们使用并发容器的初衷，因为锁的开销很大。我们有没有方法改善统计方法呢？代码清单2罗列了concurrentHashMap父接口concurrentMap的一个非常有用但是又常常被忽略的方法。 代码清单21234567891011121314/** * Replaces the entry for a key only if currently mapped to a given value. * This is equivalent to * &lt;pre&gt; &#123;@code * if (map.containsKey(key) &amp;&amp; Objects.equals(map.get(key), oldValue)) &#123; * map.put(key, newValue); * return true; * &#125; else * return false; * &#125;&lt;/pre&gt; * * except that the action is performed atomically. */ boolean replace(K key, V oldValue, V newValue); 这其实就是一个最典型的CAS操作，except that the action is performed atomically.这句话真是帮了大忙，我们可以保证比较和设置是一个原子操作，当A线程尝试在increase时，旧值被修改的话就回导致replace失效，而我们只需要用一个循环，不断获取最新值，直到成功replace一次，即可完成统计。 改进后的increase方法如下 代码清单31234567891011121314151617181920212223public long increase2(String url) &#123; Long oldValue, newValue; while (true) &#123; oldValue = urlCounter.get(url); if (oldValue == null) &#123; newValue = 1l; //初始化成功，退出循环 if (urlCounter.putIfAbsent(url, 1l) == null) break; //如果初始化失败，说明其他线程已经初始化过了 &#125; else &#123; newValue = oldValue + 1; //+1成功，退出循环 if (urlCounter.replace(url, oldValue, newValue)) break; //如果+1失败，说明其他线程已经修改过了旧值 &#125; &#125; return newValue;&#125;console output：调用次数：100000 再次调用后获得了正确的结果，上述方案看上去比较繁琐，因为第一次调用时需要进行一次初始化，所以多了一个判断，也用到了另一个CAS操作putIfAbsent，他的源代码描述如下： 代码清单412345678910111213141516171819202122232425/** * If the specified key is not already associated * with a value, associate it with the given value. * This is equivalent to * &lt;pre&gt; &#123;@code * if (!map.containsKey(key)) * return map.put(key, value); * else * return map.get(key); * &#125;&lt;/pre&gt; * * except that the action is performed atomically. * * @implNote This implementation intentionally re-abstracts the * inappropriate default provided in &#123;@code Map&#125;. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with the specified key, or * &#123;@code null&#125; if there was no mapping for the key. * (A &#123;@code null&#125; return can also indicate that the map * previously associated &#123;@code null&#125; with the key, * if the implementation supports null values.) */ V putIfAbsent(K key, V value); 简单翻译如下：“如果（调用该方法时）key-value 已经存在，则返回那个 value 值。如果调用时 map 里没有找到 key 的 mapping，返回一个 null 值”。值得注意点的一点就是concurrentHashMap的value是不能存在null值的。实际上呢，上述的方案也可以把Long替换成AtomicLong，可以简化实现， ConcurrentHashMap。 juc包下的各类Atomic类也提供了大量的CAS操作，可以不用加锁，也可以实现原子操作，以后看到其他类库有类似比较后设值，不存在即设值，加一并获取返回值等等一系列的组合操作合并成了一个接口的，都应该意识到很有可能是CAS操作。如redis的IncreamtAndGet，setIfAbsent，Atomic类的一系列api，以及上述描述的concurrentHashMap中相关的api（不同api的CAS组合接口可能名称类似，但是返回值含义不大相同，我们使用CAS的api很大程度需要获取其返回值来进行分支处理，所以一定要搞清楚每个接口的特性。如redistemplate提供的setIfAbsent，当设置成功时返回的是true，而与之名称类似的ConcurrentHashMap的putIfAbsent在设置成功后返回的是null，要足够小心，加以区分）。凡事没有绝对，但是一个大体上正确的编程建议便是能使用编程类库并发容器（线程安全的类）完成的操作，尽量不要显示加锁同步。 再扯一句关于CAS的知识点，CAS不能代替同步，由它引出了一个经典的ABA问题，即修改过一次之后，第二次修改又变为了原值，可能会在一些逻辑中出现问题。不过对于计数这个逻辑而言，只是单调的增，不会受到影响。 最后介绍一个和主题非常贴切的并发容器：Guava包中AtomicLongMap，使用他来做计数器非常容易。 代码清单51234567891011private AtomicLongMap&lt;String&gt; urlCounter3 = AtomicLongMap.create();public long increase3(String url) &#123; long newValue = urlCounter3.incrementAndGet(url); return newValue;&#125;public Long getCount3(String url) &#123; return urlCounter3.get(url);&#125; 看一下他的源码就会发现，其实和代码清单3思路差不多，只不过功能更完善了一点。 和CAS很像的操作，我之前的博客中提到过数据库的乐观锁，用version字段来进行并发控制，其实也是一种compare and swap的思想。 杂谈：网上很多对ConcurrentHashMap的介绍，众所周知，这是一个用分段锁实现的一个线程安全的map容器，但是真正对他的使用场景有介绍的少之又少。面试中能知道这个容器的人也确实不少，问出去，也就回答一个分段锁就没有下文了，但我觉得吧，有时候一知半解反而会比不知道更可怕。 参考 https://my.oschina.net/mononite/blog/144329 http://www.tuicool.com/articles/zuui6z","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"},{"name":"多线程","slug":"多线程","permalink":"http://lexburner.github.io/tags/多线程/"}]},{"title":"volatile疑问记录","slug":"volatile-question","date":"2017-03-07T11:26:52.000Z","updated":"2017-08-22T07:12:44.354Z","comments":true,"path":"2017/03/07/volatile-question/","link":"","permalink":"http://lexburner.github.io/2017/03/07/volatile-question/","excerpt":"","text":"对java中volatile关键字的描述，主要是可见性和有序性两方面。 一个很广泛的应用就是使得多个线程对共享资源的改动变得互相可见，如下： 123456789101112131415161718192021222324252627282930313233343536public class TestVolatile extends Thread &#123; /*A*/// public volatile boolean runFlag = true; public boolean runFlag = true; public boolean isRunFlag() &#123; return runFlag; &#125; public void setRunFlag(boolean runFlag) &#123; this.runFlag = runFlag; &#125; @Override public void run() &#123; System.out.println(\"进入run\"); while (isRunFlag()) &#123; /*B*/// System.out.println(\"running\"); &#125; System.out.println(\"退出run\"); &#125; public static void main(String[] args) throws InterruptedException &#123; TestVolatile testVolatile = new TestVolatile(); testVolatile.start(); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; testVolatile.setRunFlag(false); System.out.println(\"main already set runflag to false\"); new CountDownLatch(1).await(); &#125;&#125; 在A处如果不将运行标记（runflag）设置成volatile，那么main线程对runflag的修改对于testVolatile线程将不可见。导致其一直不打印“退出run”这句。 但是如果在testVolatile线程的while()增加一句：B处打印语句，程序却达到了不使用volatile，修改也变得可见，不知道到底是什么原理。 只能大概估计是while()的执行过程中线程上下文进行了切换，使得重新去主存获取了runflag的最新值，从而退出了循环，暂时记录… 2017/3/8日更新和群里面的朋友讨论了一下，发现同一份代码，不同的机器运行出了不一样的效果。又仔细翻阅了一下《effective java》，依稀记得当时好像遇到过这个问题，果然，在并发的第一张就对这个现象做出了解释。关键就在于HotSpot Server VM对编译进行了优化，这种优化称之为提升(hoisting)，结果导致了活性失败（liveness failure） 1while (isRunFlag()) &#123;&#125; 会被优化成 123if(isRunFlag())&#123; while(true)...&#125; 引用effective java这一节的原话： 简而言之，当多个线程共享可变数据的时候，每个读或者写数据的线程都必须执行同步如果没有同步，就无法保证一个线程所做的修改可以被另一个线程获知。未能同步共享可变数据会造成程序的活性失败和安全性失败。这样的失败是难以调式的。他们可能是间歇性的，且与时间相关，程序的行为在不同的VM上可能根本不同，如果只需要线程之间的交互通信，而不需要互斥，volatile修饰符就是一种可以接受的同步形式，但是正确的使用它可能需要一些技巧。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"},{"name":"多线程","slug":"多线程","permalink":"http://lexburner.github.io/tags/多线程/"}]},{"title":"浅析java内存模型（JMM）","slug":"浅析java内存模型（JMM）","date":"2017-02-24T05:07:52.000Z","updated":"2017-08-22T07:16:41.143Z","comments":true,"path":"2017/02/24/浅析java内存模型（JMM）/","link":"","permalink":"http://lexburner.github.io/2017/02/24/浅析java内存模型（JMM）/","excerpt":"","text":"并发编程模型的分类在并发编程中，我们需要处理两个关键问题：线程之间如何通信及线程之间如何同步（这里的线程是指并发执行的活动实体）。通信是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。 在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信。 同步是指程序用于控制不同线程之间操作发生相对顺序的机制。在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。如果编写多线程程序的Java程序员不理解隐式进行的线程之间通信的工作机制，很可能会遇到各种奇怪的内存可见性问题。 Java内存模型的抽象在java中，所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享（本文使用“共享变量”这个术语代指实例域，静态域和数组元素）。局部变量（Local variables），方法定义参数（java语言规范称之为formal method parameters）和异常处理器参数（exception handler parameters）不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。 Java线程之间的通信由Java内存模型（本文简称为JMM）控制，JMM决定一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。Java内存模型的抽象示意图如下： 从上图来看，线程A与线程B之间如要通信的话，必须要经历下面2个步骤： 首先，线程A把本地内存A中更新过的共享变量刷新到主内存中去。 然后，线程B到主内存中去读取线程A之前已更新过的共享变量。 下面通过示意图来说明这两个步骤： 如上图所示，本地内存A和B有主内存中共享变量x的副本。假设初始时，这三个内存中的x值都为0。线程A在执行时，把更新后的x值（假设值为1）临时存放在自己的本地内存A中。当线程A和线程B需要通信时，线程A首先会把自己本地内存中修改后的x值刷新到主内存中，此时主内存中的x值变为了1。随后，线程B到主内存中去读取线程A更新后的x值，此时线程B的本地内存的x值也变为了1。 从整体来看，这两个步骤实质上是线程A在向线程B发送消息，而且这个通信过程必须要经过主内存。JMM通过控制主内存与每个线程的本地内存之间的交互，来为java程序员提供内存可见性保证。 重排序在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型： 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从java源代码到最终实际执行的指令序列，会分别经历下面三种重排序： 上述的1属于编译器重排序，2和3属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM的处理器重排序规则会要求java编译器在生成指令序列时，插入特定类型的内存屏障（memory barriers，intel称之为memory fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。 JMM属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 处理器重排序与内存屏障指令现代的处理器使用写缓冲区来临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，可以减少对内存总线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读/写操作的执行顺序，不一定与内存实际发生的读/写操作顺序一致！为了具体说明，请看下面示例： Processor A Processor B a = 1; //A1x = b; //A2 b = 2; //B1y = a; //B2 初始状态：a = b = 0处理器允许执行后得到结果：x = y = 0 假设处理器A和处理器B按程序的顺序并行执行内存访问，最终却可能得到x = y = 0的结果。具体的原因如下图所示： 这里处理器A和处理器B可以同时把共享变量写入自己的写缓冲区（A1，B1），然后从内存中读取另一个共享变量（A2，B2），最后才把自己写缓存区中保存的脏数据刷新到内存中（A3，B3）。当以这种时序执行时，程序就可以得到x = y = 0的结果。 从内存操作实际发生的顺序来看，直到处理器A执行A3来刷新自己的写缓存区，写操作A1才算真正执行了。虽然处理器A执行内存操作的顺序为：A1-&gt;A2，但内存操作实际发生的顺序却是：A2-&gt;A1。此时，处理器A的内存操作顺序被重排序了（处理器B的情况和处理器A一样，这里就不赘述了）。 这里的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操做重排序。 下面是常见处理器允许的重排序类型的列表： Load-Load Load-Store Store-Store Store-Load 数据依赖 sparc-TSO N N N Y N x86 N N N Y N ia64 Y Y Y Y N PowerPC Y Y Y Y N 上表单元格中的“N”表示处理器不允许两个操作重排序，“Y”表示允许重排序。 从上表我们可以看出：常见的处理器都允许Store-Load重排序；常见的处理器都不允许对存在数据依赖的操作做重排序。sparc-TSO和x86拥有相对较强的处理器内存模型，它们仅允许对写-读操作做重排序（因为它们都使用了写缓冲区）。 ※注1：sparc-TSO是指以TSO(Total Store Order)内存模型运行时，sparc处理器的特性。 ※注2：上表中的x86包括x64及AMD64。 ※注3：由于ARM处理器的内存模型与PowerPC处理器的内存模型非常类似，本文将忽略它。 ※注4：数据依赖性后文会专门说明。 为了保证内存可见性，java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM把内存屏障指令分为下列四类： 屏障类型 指令示例 说明 LoadLoad Barriers Load1; LoadLoad; Load2 确保Load1数据的装载，之前于Load2及所有后续装载指令的装载。 StoreStore Barriers Store1; StoreStore; Store2 确保Store1数据对其他处理器可见（刷新到内存），之前于Store2及所有后续存储指令的存储。 LoadStore Barriers Load1; LoadStore; Store2 确保Load1数据装载，之前于Store2及所有后续的存储指令刷新到内存。 StoreLoad Barriers Store1; StoreLoad; Load2 确保Store1数据对其他处理器变得可见（指刷新到内存），之前于Load2及所有后续装载指令的装载。StoreLoad Barriers会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。 StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他三个屏障的效果。现代的多处理器大都支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（buffer fully flush）。 happens-before从JDK5开始，java使用新的JSR -133内存模型（本文除非特别说明，针对的都是JSR- 133内存模型）。JSR-133提出了happens-before的概念，通过这个概念来阐述操作之间的内存可见性。如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在happens-before关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 与程序员密切相关的happens-before规则如下： 程序顺序规则：一个线程中的每个操作，happens- before 于该线程中的任意后续操作。 监视器锁规则：对一个监视器锁的解锁，happens- before 于随后对这个监视器锁的加锁。 volatile变量规则：对一个volatile域的写，happens- before 于任意后续对这个volatile域的读。 传递性：如果A happens- before B，且B happens- before C，那么A happens- before C。 注意，两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。happens- before的定义很微妙，后文会具体说明happens-before为什么要这么定义。 happens-before与JMM的关系如下图所示： 如上图所示，一个happens-before规则通常对应于多个编译器重排序规则和处理器重排序规则。对于java程序员来说，happens-before规则简单易懂，它避免程序员为了理解JMM提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现。 原文地址","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"},{"name":"JMM","slug":"JMM","permalink":"http://lexburner.github.io/tags/JMM/"}]},{"title":"浅析项目中的并发","slug":"浅析项目中的并发","date":"2017-02-22T03:31:52.000Z","updated":"2017-08-22T07:46:51.708Z","comments":true,"path":"2017/02/22/浅析项目中的并发/","link":"","permalink":"http://lexburner.github.io/2017/02/22/浅析项目中的并发/","excerpt":"前言控制并发的方法很多，我之前的两篇博客都有过介绍，从最基础的synchronized，juc中的lock，到数据库的行级锁，乐观锁，悲观锁，再到中间件级别的redis，zookeeper分布式锁。今天主要想讲的主题是“根据并发出现的具体业务场景，使用合理的控制并发手段”。 什么是并发由一个大家都了解的例子引入我们今天的主题：并发 123456789101112131415161718192021222324252627public class Demo1 &#123; public Integer count = 0; public static void main(String[] args) &#123; final Demo1 demo1 = new Demo1(); Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++)&#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; demo1.count++; &#125; &#125;); &#125; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"final count value:\"+demo1.count); &#125;&#125;console:final count value:973 这个过程中，类变量count就是共享资源，而++操作并不是线程安全的，而多个线程去对count执行++操作，并没有happens-before原则保障执行的先后顺序，导致了最终结果并不是想要的1000","text":"前言控制并发的方法很多，我之前的两篇博客都有过介绍，从最基础的synchronized，juc中的lock，到数据库的行级锁，乐观锁，悲观锁，再到中间件级别的redis，zookeeper分布式锁。今天主要想讲的主题是“根据并发出现的具体业务场景，使用合理的控制并发手段”。 什么是并发由一个大家都了解的例子引入我们今天的主题：并发 123456789101112131415161718192021222324252627public class Demo1 &#123; public Integer count = 0; public static void main(String[] args) &#123; final Demo1 demo1 = new Demo1(); Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++)&#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; demo1.count++; &#125; &#125;); &#125; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"final count value:\"+demo1.count); &#125;&#125;console:final count value:973 这个过程中，类变量count就是共享资源，而++操作并不是线程安全的，而多个线程去对count执行++操作，并没有happens-before原则保障执行的先后顺序，导致了最终结果并不是想要的1000 下面，我们把并发中的共享资源从类变量转移到数据库中。先来看看使用框架的情况，以JPA为例（充血模型） 1234567891011121314151617181920212223242526272829303132@Componentpublic class Demo2 &#123; @Autowired TestNumDao testNumDao; @Transactional public void test()&#123; TestNum testNum = testNumDao.findOne(\"1\"); testNum.setCount(testNum.getCount()+1); testNumDao.save(testNum); &#125;&#125;controller: @Autowired Demo2 demo2; @RequestMapping(\"test\") @ResponseBody public String test()&#123; Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++)&#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; demo2.test(); &#125; &#125;); &#125; return \"test\"; &#125; 数据库的记录 id count 1 344 初窥门径的程序员会认为事务最基本的ACID中便包含了原子性，但是事务的原子性和今天所讲的并发中的原子操作仅仅是名词上有点类似。而有点经验的程序员都能知道这中间发生了什么（下面细说），这只是暴露了项目中并发问题的冰山一角。 改成直接用sql如何呢（贫血模型）？ 1234567891011121314151617181920@RequestMapping(\"testSql\") @ResponseBody public String testSql() throws InterruptedException &#123; final CountDownLatch countDownLatch = new CountDownLatch(1000); long start = System.currentTimeMillis(); Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++)&#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; jdbcTemplate.execute(\"update test_num set count = count + 1 where id = '1'\"); countDownLatch.countDown(); &#125; &#125;); &#125; countDownLatch.await(); long costTime =System.currentTimeMillis() - start; System.out.println(\"共花费：\"+costTime+\" s\"); return \"testSql\"; &#125; 数据库结果： count ： 1000 达到了预期效果这个例子我顺便记录了耗时,控制台打印:共花费：113 ms简单对比一下二，三两个例子，都是想对数据库的count进行+1操作，唯一的区别就是，后者的+1计算发生在数据库，而前者的计算依赖于事先查出来的值，并且计算发生在程序的内存中。而现在大部分的ORM框架的兴起，导致了写第二种代码的程序员变多，不注意并发的话，就会出现问题。下面我们来看看具体的业务场景。 业务场景 修改个人信息 修改商品信息 扣除账户余额，扣减库存 业务场景分析第一个场景，互联网如此众多的用户修改个人信息，这算不算并发？答案是：算也不算。算，从程序员角度来看，每一个用户请求进来，都是调用的同一个修改入口，具体一点，就是映射到controller层的同一个requestMapping，所以一定是并发的。不算，虽然程序是并发的，但是从用户角度来分析，每个人只可以修改自己的信息，所以，不同用户的操作其实是隔离的，所以不算“并发”。这也是为什么很多开发者，在日常开发中一直不注意并发控制，却也没有发生太大问题的原因，大多数初级程序员开发的还都是CRM，OA，CMS系统。 回到我们的并发，第一种业务场景，是可以使用如上模式的，对于一条用户数据的修改，我们允许程序员读取数据到内存中，内存计算修改（耗时操作），提交更改，提交事务。 1234567//Transaction startUser user = userDao.findById(\"1\");user.setName(\"newName\");user.setAge(user.getAge()+1);...//其他耗时操作userDao.save(user);//Transaction commit 这个场景变现为：几乎不存在并发，不需要控制，场景乐观。 为了严谨，也可以选择控制并发，但我觉得这需要交给写这段代码的同事，让他自由发挥。第二个场景已经有所不同了，同样是修改一个记录，但是系统中可能有多个操作员来维护，此时，商品数据表现为一个共享数据，所以存在微弱的并发，通常表现为数据的脏读，例如操作员A，B同时对一个商品信息维护，我们希望只能有一个操作员修改成功，另外一个操作员得到错误提示（该商品信息已经发生变化），否则，两个人都以为自己修改成功了，但是其实只有一个人完成了操作，另一个人的操作被覆盖了。 这个场景表现为：存在并发，需要控制，允许失败，场景乐观。 通常我建议这种场景使用乐观锁，即在商品属性添加一个version字段标记修改的版本，这样两个操作员拿到同一个版本号，第一个操作员修改成功后版本号变化，另一个操作员的修改就会失败了。 1234567891011121314151617class Goods&#123; @Version int version;&#125;//Transaction starttry&#123; Goods goods = goodsDao.findById(\"1\"); goods.setName(\"newName\"); goods.setPrice(goods.getPrice()+100.00); ...//其他耗时操作 goodsDao.save(goods);&#125;catch(org.hibernate.StaleObjectStateException e)&#123; //返回给前台&#125;//Transaction commit springdata配合jpa可以自动捕获version异常，也可以自动手动对比。 第三个场景这个场景表现为：存在频繁的并发，需要控制，不允许失败，场景悲观。 强调一下，本例不应该使用在项目中，只是为了举例而设置的一个场景，因为这种贫血模型无法满足复杂的业务场景，而且依靠单机事务来保证一致性，并发性能和可扩展性能不好。 一个秒杀场景，大量请求在短时间涌入，是不可能像第二种场景一样，100个并发请求，一个成功，其他99个全部异常的。 设计方案应该达到的效果是：有足够库存时，允许并发，库存到0时，之后的请求全部失败；有足够金额时，允许并发，金额不够支付时立刻告知余额不足。 可以利用数据库的行级锁，update set balance = balance - money where userId = ? and balance &gt;= money;update stock = stock - number where goodsId = ? and stock &gt;= number ; 然后在后台 查看返回值是否影响行数为1，判断请求是否成功，利用数据库保证并发。 需要补充一点，我这里所讲的秒杀，并不是指双11那种级别的秒杀，那需要多层架构去控制并发，前端拦截，负载均衡….不能仅仅依赖于数据库的，会导致严重的性能问题。为了留一下一个直观的感受，这里对比一下oracle，mysql的两个主流存储引擎：innodb，myisam的性能问题。123456oracle:10000个线程共计1000000次并发请求：共花费：101017 ms =&gt;101sinnodb:10000个线程共计1000000次并发请求：共花费：550330 ms =&gt;550smyisam:10000个线程共计1000000次并发请求：共花费：75802 ms =&gt;75s 可见，如果真正有大量请求到达数据库，光是依靠数据库解决并发是不现实的，所以仅仅只用数据库来做保障而不是完全依赖。需要根据业务场景选择合适的控制并发手段。 后续，待补充分布式锁控制并发…浅析队列在并发场景中的地位…","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/categories/架构设计/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"},{"name":"多线程","slug":"多线程","permalink":"http://lexburner.github.io/tags/多线程/"}]},{"title":"聊聊IT行业应届生求职","slug":"聊聊IT行业应届生求职","date":"2017-02-19T16:57:52.000Z","updated":"2017-08-22T07:50:12.532Z","comments":true,"path":"2017/02/20/聊聊IT行业应届生求职/","link":"","permalink":"http://lexburner.github.io/2017/02/20/聊聊IT行业应届生求职/","excerpt":"前言回首大三下的暑假，那时候刚开始出来找实习，如今已经即将进入大四下学期，恍惚间，已经过去了8，9个月。写这篇文章的初衷就是想结合自己的经验给即将要出来找工作的应届生一些建议，想当初自己刚出来时，也得到过热心学长的教导，权当一种传递吧。 个人经历坐标上海，目前在一家IT软件公司从事电子商务，金融保险类的网站开发，主要使用的语言是JAVA。从任职的3-4个月起，开始担任项目小组长协同项目经理进行开发。期间由于技术总监常驻广州的原因，我兼任了上海分部这一块的面试工作，主要负责技术部分的面试（TMD工资却没涨T__T）。所以对广大来面试者的水平，以及公司想要的人才都有了更深的了解；有了面试经验后，一些观念也有了转变。 面试杂谈大四肯定很多人想出来找实习，但是又完全没有任何经验，这就很尴尬了，我先来说一些一定要注意的点。 不要乱投简历，现在互联网上有很多培训机构，中介机构，打着招聘的牌子，背后却干着培训的勾当。通常是对一些基础不太好的同学进行技术面试，对他们的信心造成碾压，而后，提出培训后入职的建议。通常这类公司就是通过这种手段去拉人培训，招人根本不是初衷。所以，要问清楚公司的情况，有必要面试之前先去百度搜一搜公司的基本情况和评价。 紧接着上面那点，可以通过一些业界信誉比较高的app或者网站去筛选公司。如BOSS直聘，拉钩，51job，前程无忧…特别是前面两个，是专门给程序员招聘使用的，针对性很强，对自己能力有了解的同学也可以量力而行，挑选适合自己的岗位。 投简历之前搞清楚公司的性质。IT行业目前大方向就分为两类：软件公司，互联网公司。我当初刚进公司的时候甲方乙方都搞不清楚，大家可能一下子也不知道这两种公司性质有什么区别。可以参照知乎这个问题的讨论https://www.zhihu.com/question/20274106/answer/40996303，简单来说同样的能力：软件公司轻松，钱少；互联网公司累，钱多。软件公司中又有外企，民营，国资等划分，工作性质又分为外包，自营...外包又分为人力外包和项目外包...互联网公司一说，大家肯定都知道BAT，京东，谷歌...还有一个层面的划分就是，软件公司大多提供的是服务，互联网公司通常都有自己的产品，不过这么说不够严谨，权当个参考吧。","text":"前言回首大三下的暑假，那时候刚开始出来找实习，如今已经即将进入大四下学期，恍惚间，已经过去了8，9个月。写这篇文章的初衷就是想结合自己的经验给即将要出来找工作的应届生一些建议，想当初自己刚出来时，也得到过热心学长的教导，权当一种传递吧。 个人经历坐标上海，目前在一家IT软件公司从事电子商务，金融保险类的网站开发，主要使用的语言是JAVA。从任职的3-4个月起，开始担任项目小组长协同项目经理进行开发。期间由于技术总监常驻广州的原因，我兼任了上海分部这一块的面试工作，主要负责技术部分的面试（TMD工资却没涨T__T）。所以对广大来面试者的水平，以及公司想要的人才都有了更深的了解；有了面试经验后，一些观念也有了转变。 面试杂谈大四肯定很多人想出来找实习，但是又完全没有任何经验，这就很尴尬了，我先来说一些一定要注意的点。 不要乱投简历，现在互联网上有很多培训机构，中介机构，打着招聘的牌子，背后却干着培训的勾当。通常是对一些基础不太好的同学进行技术面试，对他们的信心造成碾压，而后，提出培训后入职的建议。通常这类公司就是通过这种手段去拉人培训，招人根本不是初衷。所以，要问清楚公司的情况，有必要面试之前先去百度搜一搜公司的基本情况和评价。 紧接着上面那点，可以通过一些业界信誉比较高的app或者网站去筛选公司。如BOSS直聘，拉钩，51job，前程无忧…特别是前面两个，是专门给程序员招聘使用的，针对性很强，对自己能力有了解的同学也可以量力而行，挑选适合自己的岗位。 投简历之前搞清楚公司的性质。IT行业目前大方向就分为两类：软件公司，互联网公司。我当初刚进公司的时候甲方乙方都搞不清楚，大家可能一下子也不知道这两种公司性质有什么区别。可以参照知乎这个问题的讨论https://www.zhihu.com/question/20274106/answer/40996303，简单来说同样的能力：软件公司轻松，钱少；互联网公司累，钱多。软件公司中又有外企，民营，国资等划分，工作性质又分为外包，自营...外包又分为人力外包和项目外包...互联网公司一说，大家肯定都知道BAT，京东，谷歌...还有一个层面的划分就是，软件公司大多提供的是服务，互联网公司通常都有自己的产品，不过这么说不够严谨，权当个参考吧。 下面说一说这么多公司，怎么挑选适合自己的岗位。有很多的参考项，个人的能力，期望的工作地点以及地域的工资水准，未来的职业规划，房价，对象，水土气候，人脉等等诸多因素。本人是干java的，所以就以java求职来做例子，其他职业，专业请结合自己的专业知识做好对比即可。全部以上海为准，上海的起薪大概是2.8K左右，这叫基本工资，其他城市，例如无锡，苏州，大概在2.3k左右，视经济发展程度而定，先有个大概了解。 下面来看看具体招聘需求A类： Java 6K-12K职位描述 人品过硬。愿意追随项目长期发展。有能力。 有阅历。 有学历。符合PSD原则，即出身贫寒、渴望成功、聪明机智。 不需要我吐槽了吧，这种明明是招技术岗，却对技术没有要求的，估计能骗一些小白去面试，只有技术一无所知，才会退而去要求人品，试想一下，你啥都不会，也只能要求你人品过关了。 B类： 职位描述 任职要求：1) 大专或以上学历，计算机相关专业，1-3年以上软件开发经验；2) 熟练掌握Java开发技术，j2ee平台的核心技术的原理：jsp、ajax、servlet，jdbc等；3) 熟练掌握一种主流数据库：MySQL/sql,server/oracle/DB2，熟悉一种应用服务器的配置：tomcat/jboss/weblogic/websphere；4) 熟悉和理解Java开发各层次框架，如struts、spring、hiberate等，掌握基本Web前台技术；5) 热爱开发工作，具备良好的程序开发驾驭能力，需求分析把握能力；6) 好的沟通和解能力，善于团队合作，逻辑思维强，能够独立思考。 此文我是想写给应届生的，1-3年的工作经验没那么恐怖，大多数情况下，你的能力够了，公司不会跟你较真，用年限压你，所以看到自己技术水平能够达到，资历却不符合的岗位，也可以尝试着投一投。这类公司其实已经算是对技术有了要求了，而且技术细节都明确了出来，但是，看到只对jsp，servlet这些技术有所要求，明眼人都知道，这是在招初级开发，了解一点框架，懂计算机基础，这样的新手，公司还是可以接受的，上海这边针对可以独立开发的应届生，或者培训班出来可以直接上手的非科班生：软件公司，实习开价大概在4-5k，转正开价大概在7-8k；互联网公司实习大概在5-6k，转正开价9-10k起步。985/211或者能力不错能够入职的高校生，在互联网名企的开价，就以阿里为例，我了解到的情况大概是12k14 or 1216。这里都是说一个上海地区价格，不适用与全国。北京的情况是IT非常发达，很多互联网公司都在北京，而上海，深圳，广州其次，注意，上海是金融之都，并非IT之都。 C类 ： Java工程师 13K-21K任职资格 1)大学本科或以上学历，计算机相关专业；2)熟练掌握core java以及主流java框架，3)熟悉HTML5、CSS3、JAVASCRIPT、JQUERY等前端技术；4)熟练掌握面向对象的设计原则，熟悉JAVA设计模式，具备一定的系统架构设计能力；5)熟悉常用的互联网相关技术产品和中间件，例如redis，elasticSearch，activeMq，Dubbo等；6)能够带领开发小组独立完成产品功能的模块设计和研发；7)熟悉面向服务的开发，有大型互联网项目的开发/设计经验优先；8)较强的上进心和求知欲，善于学习和运用新知识，善于沟通和逻辑表达，有强烈的团队意识和执行力。 没找到特别适合本科生的描述，简单概括下这类公司，按照招聘要求来说吧。对计算机专业做要求，说明希望应聘者的专业素质有所保障，懂得基本的操作系统原理，数据结构，编译原理…因为这些都是本科期间必学的。对core java有掌握，说明是要招java岗位，基础必须牢固。前端知识有所了解，说明要懂得如果跟前端人员交互，不是完全的服务端开发设计模式和架构，说明不是要招只能够写增删改查的业务人员，更希望是那种能驱动团队的人才一系列中间件的要求说明企业比较正规，跟的上互联网的步伐，通常这类公司的技术总监是比较厉害的，发展前景不错dubbo一出来，说明该公司还是搞得分布式框架，微服务架构，对程序员的要求更上了一个档次 综合来看，具备以上素质的人当然配得上高一点的工资。 简历简历不要弄虚作假，什么东西是自己做的，什么东西不是自己做的，面试官一句话就能问出来。我面试过的很多人把自己的项目技能写的天花乱坠，随便问一个东西，都不能说个所以然出来，你还写了干嘛，徒增尴尬。 简历不要写与应聘岗位相差太大的描述，如果写了，也要能自圆其说，为什么体现出了自己的才能。我看过一个应聘JAVA后端的“人才”写着有普通话证书，来，我现场让你说一段绕口令？还有诸如“参加XXX比赛，虽然没得奖，但是自己得到了锻炼”之类的话，真的有必要写在简历上面吗？ 真是没得写的，可以说一说自己大学里面参加的活动体现出怎么样的能力，自己的优异表现，学分绩点，专业课程知识等等。要是实在一无可写…算了，那还是写普通话证书吧。 有项目经验，比赛经历，专业技能证书，英语考级证书的务必要写上（排名分先后）。都是应届生吗，注意一些技巧，如果你其他方面很突出，但是英语不行，只过了4级，那就别写英语4级了，因为会暴露你没有过6级。用其他证书掩盖过去。这不是欺骗，而是扬长避短。 简历得体大方，模板到处有，关于应届生求职简历的事，可以到知乎好好看看。 公司的诉求普通公司找人，一是看人的基础水平符不符合岗位需求，二是看人的素质符不符合团队的理念，再者就是追求一个性价比。 不是说你能力够了我就要招你，有些时候，公司就是要招基础的业务人员，你技术太厉害，要价太高，完全没必要招你。一个公司的垂直分层，必然是金字塔结构。所以讲究一个对号入座，搞清楚自己的能力，搞清楚自己想要什么样的一份岗位，投简历之前好好看看岗位的描述，公司的诉求。 我面了前前后后也快30多个人了，有很多培训班出来的非科班生，很多应届或者一年经验的人，985/211也有，工作了12年的人也有，说实话，能力也就这样，能力很强的人要么出国了，要么内推进了名企，我就只能从我接触到的这些人，说出一些看法。资历在我看来不是很重要，仅仅作为一个参考的位面，好几个工作了3-4年的人我感觉好不如咱们应届生，不追求技术的突破，一直干着增删改查操作，问一些JAVA基础性的知识又一无所知，要价有得太低，体现出对自己的不自信，有得太高，不清楚自己的定位，入职率很低。再加上现在公司都是对分布式架构的开发，需要的从业者的素质越来越高。整个互联网的趋势也是如此，没有什么人是突然就变得很厉害的，我司技术总监拥有着这么厉害的技术，在我所知也是靠着毕业后依旧数年如一日的对技术的热忱追求。所以，特别是IT互联网行业，更希望找到的，是有一颗学习的心，具备终身学习能力的人，以应对日新月异的互联网技术变更。 最后大多数人还是需要有自己的思考，此文谨代表个人看法供大家参考。","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/categories/技术杂谈/"}],"tags":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/tags/技术杂谈/"},{"name":"求职","slug":"求职","permalink":"http://lexburner.github.io/tags/求职/"}]},{"title":"《微服务》九大特性笔记","slug":"ms-1","date":"2017-02-18T17:05:52.000Z","updated":"2017-08-22T07:52:39.139Z","comments":true,"path":"2017/02/19/ms-1/","link":"","permalink":"http://lexburner.github.io/2017/02/19/ms-1/","excerpt":"","text":"服务组件化组件，是一个可以独立更换和升级的单元。就像PC中的CPU、内存、显卡、硬盘一样，独立且可以更换升级而不影响其他单元。 在“微服务”架构中，需要我们对服务进行组件化分解。服务，是一种进程外的组件，它通过http等通信协议进行协作，而不是传统组件以嵌入的方式协同工作。服务都独立开发、部署，可以有效的避免一个服务的修改引起整个系统的重新部署。 打一个不恰当的比喻，如果我们的PC组件以服务的方式构建，我们只维护主板和一些必要外设之后，计算能力通过一组外部服务实现，我们只需要告诉PC我们从哪个地址来获得计算能力，通过服务定义的计算接口来实现我们使用过程中的计算需求，从而实现CPU组件的服务化。这样我们原本复杂的PC服务得到了更轻量化的实现，我们甚至只需要更换服务地址就能升级我们PC的计算能力。 按业务组织团队当我们开始决定如何划分“微服务”时，通常也意味着我们要开始对团队进行重新规划与组织。按以往的方式，我们往往会以技术的层面去划分多个不同的团队，比如：DBA团队、运维团队、后端团队、前端团队、设计师团队等等。若我们继续按这种方式组织团队来实施“微服务”架构开发时，当有一个有问题需要更改，可能是一个非常简单的变动，比如：对人物描述增加一个字段，这就需要从数据存储开始考虑一直到设计和前端，虽然大家的修改都非常小，但这会引起跨团队的时间和预算审批。 在实施“微服务”架构时，需要采用不同的团队分割方法。由于每一个微服务都是针对特定业务的宽栈或是全栈实现，既要负责数据的持久化存储，又要负责用户的接口定义等各种跨专业领域的职能。因此，面对大型项目时候，对于微服务团队拆分更加建议按业务线的方式进行拆分，一方面可以有效减少服务内部修改所产生的内耗；另一方面，团队边界可以变得更为清晰。 做“产品”的态度实施“微服务”架构的团队中，每个小团队都应该以做产品的方式，对其产品的整个生命周期负责。而不是以项目的模式，以完成开发与交付并将成果交接给维护者为最终目标。 开发团队通过了解服务在具体生产环境中的情况，可以增加他们对具体业务的理解，比如：很多时候一些业务中发生的特殊或异常情况，很可能产品经理都并不知晓，但细心的开发者很容易通过生产环境发现这些特殊的潜在问题或需求。 所以，我们需要用做“产品”的态度来对待每一个“微服务”，持续关注服务的运作情况，并不断地分析帮助用户来提升业务功能。 智能端点与哑管道在单体应用中，组件间直接通过函数调用的方式进行交互协作。而在“微服务”架构中，服务由于不在一个进程中，组件间的通信模式发生了改变，若仅仅将原本在进程内的方法调用改成RPC方式的调用，会导致微服务之间产生繁琐的通信，使得系统表现更为糟糕，所以，我们需要更粗粒度的通信协议。 在“微服务”架构中，通常会使用这两个服务调用方式： 第一种，使用HTTP协议的RESTful API或轻量级的消息发送协议，来实现信息传递与服务调用的触发。第二种，通过在轻量级消息总线上传递消息，类似RabbitMQ等一些提供可靠异步交换的结构。 在极度强调性能的情况下，有些团队会使用二进制的消息发送协议，例如：protobuf。即使是这样，这些系统仍然会呈现出“智能端点和哑管道”的特点，为了在易读性与高效性之间取得平衡。当然大多数Web应用或企业系统并不需要作出在这两者间做出选择，能够获得易读性就已经是一个极大的胜利了。——Martin Fowler 去中心化治理当我们采用集中化的架构治理方案时，通常在技术平台上都会做同一的标准，但是每一种技术平台都有其短板，这会导致在碰到短板时，不得不花费大力气去解决，并且可能还是因为其底层原因解决的不是很好。 在实施“微服务”架构时，通过采用轻量级的契约定义接口，使得我们对于服务本身的具体技术平台不再那么敏感，这样我们整个“微服务”架构的系统中的组件就能针对其不同的业务特点选择不同的技术平台，终于不会出现杀鸡用牛刀或是杀牛用指甲钳的尴尬处境了。 不是每一个问题都是钉子，不是每一个解决方案都是锤子 去中心化管理数据我们在实施“微服务”架构时，都希望可以让每一个服务来管理其自有的数据库，这就是数据管理的去中心化。 在去中心化过程中，我们除了将原数据库中的存储内容拆分到新的同平台的其他数据库实例中之外（如：把原本存储在MySQL中的表拆分后，存储多几个不同的MySQL实例中），也可以针对一些具有特殊结构或业务特性的数据存储到一些其他技术的数据库实例中（如：把日志信息存储到MongoDB中、把用户登录信息存储到Redis中）。 虽然，数据管理的去中心化可以让数据管理更加细致化，通过采用更合适的技术来让数据存储和性能达到最优。但是，由于数据存储于不同的数据库实例中后，数据一致性也成为“微服务”架构中急需解决的问题之一。分布式事务的实现，本身难度就非常大，所以在“微服务”架构中，我们更强调在各服务之间进行“无事务”的调用，而对于数据一致性，只要求数据在最后的处理状态是一致的效果；若在过程中发现错误，通过补偿机制来进行处理，使得错误数据能够达到最终的一致性。 基础设施自动化近年来云计算服务与容器化技术的不断成熟，运维基础设施的工作变得越来越不那么难了。但是，当我们实施“微服务”架构时，数据库、应用程序的个头虽然都变小了，但是因为拆分的原因，数量成倍的增长。这使得运维人员需要关注的内容也成倍的增长，并且操作性任务也会成倍的增长，这些问题若没有得到妥善的解决，必将成为运维人员的噩梦。 所以，在“微服务”架构中，请务必从一开始就构建起“持续交付”平台来支撑整个实施过程，该平台需要两大内容，不可或缺： 自动化测试：每次部署前的强心剂，尽可能的获得对正在运行软件的信心。自动化部署：解放繁琐枯燥的重复操作以及对多环境的配置管理。 容错设计在单体应用中，一般不存在单个组件故障而其他还在运行的情况，通常是一挂全挂。而在“微服务”架构中，由于服务都运行在独立的进程中，所以是存在部分服务出现故障，而其他服务都正常运行的情况，比如：当正常运作的服务B调用到故障服务A时，因故障服务A没有返回，线程挂起开始等待，直到超时才能释放，而此时若触发服务B调用服务A的请求来自服务C，而服务C频繁调用服务B时，由于其依赖服务A，大量线程被挂起等待，最后导致服务A也不能正常服务，这时就会出现故障的蔓延。 所以，在“微服务”架构中，快速的检测出故障源并尽可能的自动恢复服务是必须要被设计和考虑的。通常，我们都希望在每个服务中实现监控和日志记录的组件，比如：服务状态、断路器状态、吞吐量、网络延迟等关键数据的仪表盘等。 演进式设计通过上面的几点特征，我们已经能够体会到，要实施一个完美的“微服务”架构，需要考虑的设计与成本并不小，对于没有足够经验的团队来说，甚至要比单体应用发付出更多的代价。 所以，很多情况下，架构师们都会以演进的方式进行系统的构建，在初期系统以单体系统的方式来设计和实施，一方面系统体量初期并不会很大，构建和维护成本都不高。另一方面，初期的核心业务在后期通常也不会发生巨大的改变。随着系统的发展或者业务的需要，架构师们会将一些经常变动或是有一定时间效应的内容进行“微服务”处理，并逐渐地将原来在单体系统中多变的模块逐步拆分出来，而稳定不太变化的就形成了一个核心“微服务”存在于整个架构之中。 原文由 程序猿DD-翟永超 创作转载自《微服务》九大特性笔记","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/categories/架构设计/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://lexburner.github.io/tags/微服务/"}]},{"title":"ThreadLocal的最佳实践","slug":"threadLocal","date":"2017-02-14T09:38:52.000Z","updated":"2017-08-22T07:56:03.082Z","comments":true,"path":"2017/02/14/threadLocal/","link":"","permalink":"http://lexburner.github.io/2017/02/14/threadLocal/","excerpt":"","text":"SimpleDateFormat众所周知是线程不安全的，多线程中如何保证线程安全又同时兼顾性能问题呢？那就是使用ThreadLocal维护SimpleDateFormat 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class SimpleDateFormatThreadTest &#123; static volatile AtomicInteger n = new AtomicInteger(-1); static ThreadLocal&lt;DateFormat&gt; sdf ; static &#123; sdf =new ThreadLocal&lt;DateFormat&gt;() &#123; @Override protected DateFormat initialValue() &#123; return new SimpleDateFormat(\"yyyy-MM-dd\"); &#125; &#125;; &#125; public static void main(String[] args) throws ParseException, InterruptedException &#123; Set&lt;String&gt; dateSet = new ConcurrentHashSet&lt;&gt;(); Set&lt;Integer&gt; numberSet = new ConcurrentHashSet&lt;&gt;(); Date[] dates = new Date[1000]; for (int i = 0; i &lt; 1000; i++) &#123; dates[i] = sdf.get().parse(i + 1000 + \"-11-22\"); &#125; ExecutorService executorService = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++)&#123; executorService.execute(new Runnable() &#123; @Override public void run() &#123; int number = n.incrementAndGet(); String date = sdf.get().format(dates[number]); numberSet.add(number); dateSet.add(date); System.out.println(number+\" \"+date); &#125; &#125;); &#125; executorService.shutdown(); Thread.sleep(5000); System.out.println(dateSet.size()); System.out.println(numberSet.size()); &#125;&#125; 实践证明sdf的parse（String to Date）有严重的线程安全问题，format（Date to String）有轻微的线程安全问题，虽然不太明显，但还是会出现问题，这和内部的实现有关。 简单分析下使用ThreadLocal的好处，1000次转换操作，10个线程争抢执行，如果每次都去new 一个sdf，可见其效率之低，而使用ThreadLocal，是对每个线程维护一个sdf，所以最多就只会出现10个sdf，真正项目中，由于操作系统线程分片执行，所以线程不会非常的多，使用ThreadLocal的好处也就立竿见影了。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"},{"name":"多线程","slug":"多线程","permalink":"http://lexburner.github.io/tags/多线程/"}]},{"title":"Transactional注解使用注意点","slug":"transactional-tips","date":"2017-02-14T08:51:52.000Z","updated":"2017-12-27T07:47:18.864Z","comments":true,"path":"2017/02/14/transactional-tips/","link":"","permalink":"http://lexburner.github.io/2017/02/14/transactional-tips/","excerpt":"","text":"@Transactional可以说是spring中最常用的注解之一了，通常情况下我们在需要对一个service方法添加事务时，加上这个注解，如果发生unchecked exception，就会发生rollback，最典型的例子如下。 1234567891011121314@Servicepublic class StudentService &#123; @Autowired StudentDao studentDao; @Transactional public void innerSave(int i) &#123; Student student = new Student(); student.setName(&quot;test&quot; + i); studentDao.save(student); //i=5 会出现异常 int a = 1 / (i - 5); &#125;&#125; 在调用innerSave(5)时会发运算异常，导致保存操作回滚，不在此赘述了。 新的需求：循环保存10个学生，发生异常时要求回滚。我们理所当然的写出了下面的代码，在StudentService.java添加如下方法 123456789public void outerLooper1() &#123; for (int i = 1; i &lt;= 10; i++) &#123; try&#123; innerSave(i); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; &#125; 先考虑一下test5这个学生有没有保存呢？结果：依然出现了，考虑下问题出在哪儿了？ 其实也好理解，spring中@Transactional 的事务开启 ，是基于接口 或者是类的代理被创建的。所以在同一个类中一个普通方法outerLooper1()调用另一个有事务的方法innerSave()，事务是不会起作用的。要解决这个问题，一般我的做法是写一个帮助类，注入到当前类中，来完成事务操作。 12345678@AutowiredUtilService utilService;public void outerLooper2() &#123; for (int i = 1; i &lt;= 10; i++) &#123; utilService.innerSave(i); &#125;&#125; 在spring中使用事务需要遵守一些规范和了解一些坑点，别想当然。列举一下一些注意点。 在需要事务管理的地方加@Transactional 注解。@Transactional 注解可以被应用于接口定义和接口方法、类定义和类的public 方法上。 @Transactional 注解只能应用到 public 可见度的方法上。如果你在 protected、private 或者package-visible 的方法上使用@Transactional 注解，它也不会报错，但是这个被注解的方法将不会展示已配置的事务设置。 Spring团队建议在具体的类（或类的方法）上使用 @Transactional 注解，而不要使用在类所要实现的任何接口上。在接口上使用@Transactional 注解，只能当你设置了基于接口的代理时它才生效。因为注解是 不能继承的，这就意味着如果正在使用基于类的代理时，那么事务的设置将不能被基于类的代理所识别，而且对象也将不会被事务代理所包装。 @Transactional 的事务开启 ，或者是基于接口的或者是基于类的代理被创建。所以在同一个类中一个方法调用另一个方法有事务的方法，事务是不会起作用的。 了解事务的隔离级别，各个数据库默认的隔离级别是不一样的，在spring中用的是isolation = Isolation.READ_COMMITTED来设置；了解事务的传播机制，当发生事务嵌套时，按照业务选择对应的传播机制，用propagation= Propagation.REQUIRED来设置。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"},{"name":"事务","slug":"事务","permalink":"http://lexburner.github.io/tags/事务/"}]},{"title":"简单了解RPC实现原理","slug":"easy-know-rpc","date":"2017-02-10T07:11:52.000Z","updated":"2017-12-18T02:34:33.147Z","comments":true,"path":"2017/02/10/easy-know-rpc/","link":"","permalink":"http://lexburner.github.io/2017/02/10/easy-know-rpc/","excerpt":"时下很多企业应用更新换代到分布式，一篇文章了解什么是RPC。原作者梁飞，在此记录下他非常简洁的rpc实现思路。","text":"时下很多企业应用更新换代到分布式，一篇文章了解什么是RPC。原作者梁飞，在此记录下他非常简洁的rpc实现思路。 核心框架类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (\"Confidential * Information\"). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.framework;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.net.ServerSocket;import java.net.Socket;/** * RpcFramework * * @author william.liangf */public class RpcFramework &#123; /** * 暴露服务 * * @param service 服务实现 * @param port 服务端口 * @throws Exception */ public static void export(final Object service, int port) throws Exception &#123; if (service == null) throw new IllegalArgumentException(\"service instance == null\"); if (port &lt;= 0 || port &gt; 65535) throw new IllegalArgumentException(\"Invalid port \" + port); System.out.println(\"Export service \" + service.getClass().getName() + \" on port \" + port); ServerSocket server = new ServerSocket(port); for(;;) &#123; try &#123; final Socket socket = server.accept(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; try &#123; ObjectInputStream input = new ObjectInputStream(socket.getInputStream()); try &#123; String methodName = input.readUTF(); Class&lt;?&gt;[] parameterTypes = (Class&lt;?&gt;[])input.readObject(); Object[] arguments = (Object[])input.readObject(); ObjectOutputStream output = new ObjectOutputStream(socket.getOutputStream()); try &#123; Method method = service.getClass().getMethod(methodName, parameterTypes); Object result = method.invoke(service, arguments); output.writeObject(result); &#125; catch (Throwable t) &#123; output.writeObject(t); &#125; finally &#123; output.close(); &#125; &#125; finally &#123; input.close(); &#125; &#125; finally &#123; socket.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 引用服务 * * @param &lt;T&gt; 接口泛型 * @param interfaceClass 接口类型 * @param host 服务器主机名 * @param port 服务器端口 * @return 远程服务 * @throws Exception */ @SuppressWarnings(\"unchecked\") public static &lt;T&gt; T refer(final Class&lt;T&gt; interfaceClass, final String host, final int port) throws Exception &#123; if (interfaceClass == null) throw new IllegalArgumentException(\"Interface class == null\"); if (! interfaceClass.isInterface()) throw new IllegalArgumentException(\"The \" + interfaceClass.getName() + \" must be interface class!\"); if (host == null || host.length() == 0) throw new IllegalArgumentException(\"Host == null!\"); if (port &lt;= 0 || port &gt; 65535) throw new IllegalArgumentException(\"Invalid port \" + port); System.out.println(\"Get remote service \" + interfaceClass.getName() + \" from server \" + host + \":\" + port); return (T) Proxy.newProxyInstance(interfaceClass.getClassLoader(), new Class&lt;?&gt;[] &#123;interfaceClass&#125;, new InvocationHandler() &#123; public Object invoke(Object proxy, Method method, Object[] arguments) throws Throwable &#123; Socket socket = new Socket(host, port); try &#123; ObjectOutputStream output = new ObjectOutputStream(socket.getOutputStream()); try &#123; output.writeUTF(method.getName()); output.writeObject(method.getParameterTypes()); output.writeObject(arguments); ObjectInputStream input = new ObjectInputStream(socket.getInputStream()); try &#123; Object result = input.readObject(); if (result instanceof Throwable) &#123; throw (Throwable) result; &#125; return result; &#125; finally &#123; input.close(); &#125; &#125; finally &#123; output.close(); &#125; &#125; finally &#123; socket.close(); &#125; &#125; &#125;); &#125;&#125; 定义服务接口12345678910111213141516171819/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (\"Confidential * Information\"). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.test;/** * HelloService * * @author william.liangf */public interface HelloService &#123; String hello(String name);&#125; 实现服务123456789101112131415161718192021/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (\"Confidential * Information\"). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.test;/** * HelloServiceImpl * * @author william.liangf */public class HelloServiceImpl implements HelloService &#123; public String hello(String name) &#123; return \"Hello \" + name; &#125;&#125; 暴露服务123456789101112131415161718192021222324/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (\"Confidential * Information\"). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.test;import com.alibaba.study.rpc.framework.RpcFramework;/** * RpcProvider * * @author william.liangf */public class RpcProvider &#123; public static void main(String[] args) throws Exception &#123; HelloService service = new HelloServiceImpl(); RpcFramework.export(service, 1234); &#125;&#125; 引用服务12345678910111213141516171819202122232425262728/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (\"Confidential * Information\"). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.test;import com.alibaba.study.rpc.framework.RpcFramework;/** * RpcConsumer * * @author william.liangf */public class RpcConsumer &#123; public static void main(String[] args) throws Exception &#123; HelloService service = RpcFramework.refer(HelloService.class, \"127.0.0.1\", 1234); for (int i = 0; i &lt; Integer.MAX_VALUE; i ++) &#123; String hello = service.hello(\"World\" + i); System.out.println(hello); Thread.sleep(1000); &#125; &#125; &#125; 总结这个简单的例子的实现思路是使用阻塞的socket IO流来进行server和client的通信，也就是rpc应用中服务提供方和服务消费方。并且是端对端的，用端口号来直接进行通信。方法的远程调用使用的是jdk的动态代理，参数的序列化也是使用的最简单的objectStream。 真实的rpc框架会对上面的实现方式进行替换，采用更快更稳定，更高可用易扩展，更适宜分布式场景的中间件，技术来替换。例如使用netty的nio特性达到非阻塞的通信，使用zookeeper统一管理服务注册与发现，解决了端对端不灵活的劣势。代理方式有cglib字节码技术。序列化方式有hession2，fastjson等等。不过梁飞大大的博客使用原生的jdk api就展现给各位读者一个生动形象的rpc demo，实在是强。rpc框架解决的不仅仅是技术层面的实现，还考虑到了rpc调用中的诸多问题，重试机制，超时配置…这些就需要去了解成熟的rpc框架是如果考虑这些问题的了。 推荐一个轻量级的rpc框架：motan。weibo团队在github开源的一个rpc框架，有相应的文档，用起来感觉比dubbo要轻量级，易上手。","categories":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/categories/RPC/"}],"tags":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/tags/RPC/"}]},{"title":"java trick--String.intern()","slug":"java trick--String.intern()","date":"2016-11-07T15:16:52.000Z","updated":"2017-08-22T08:24:18.638Z","comments":true,"path":"2016/11/07/java trick--String.intern()/","link":"","permalink":"http://lexburner.github.io/2016/11/07/java trick--String.intern()/","excerpt":"","text":"《深入理解java虚拟机》第二版中对String.intern()方法的讲解中所举的例子非常有意思 不了解String.intern()的朋友要理解他其实也很容易，它返回的是一个字符串在字符串常亮池中的引用。直接看下面的demo 123456789public class Main &#123; public static void main(String[] args) &#123; String str1 = new StringBuilder(\"计算机\").append(\"软件\").toString(); System.out.println(str1.intern() == str1); String str2 = new StringBuilder(\"ja\").append(\"va\").toString(); System.out.println(str2.intern() == str2); &#125;&#125; 两者输出的结果如下： 12truefalse 我用的jdk版本为Oracle JDK7u45。简单来说，就是一个很奇怪的现象，为什么java这个字符串在类加载之前就已经加载到常量池了？ 我在知乎找到了具体的说明，如下： 1234567891011package sun.misc;import java.io.PrintStream;public class Version &#123; private static final String launcher_name = \"java\"; private static final String java_version = \"1.7.0_79\"; private static final String java_runtime_name = \"Java(TM) SE Runtime Environment\"; private static final String java_runtime_version = \"1.7.0_79-b15\"; ...&#125; 而HotSpot JVM的实现会在类加载时先调用： 123456789public final class System&#123; ... private static void initializeSystemClass() &#123; ... sun.misc.Version.init(); ... &#125; ...&#125; 原来是sun.misc.Version这个类在起作用。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"}]},{"title":"java trick--String.intern()","slug":"java trick--intergerCache","date":"2016-11-07T15:00:52.000Z","updated":"2017-08-22T08:27:03.061Z","comments":true,"path":"2016/11/07/java trick--intergerCache/","link":"","permalink":"http://lexburner.github.io/2016/11/07/java trick--intergerCache/","excerpt":"","text":"看一段代码： 1234567public class Main &#123; public static void main(String[] args) &#123; Integer a=100,b=100,c=150,d=150; System.out.println(a==b); System.out.println(c==d); &#125;&#125; 这段代码会输出什么？ 不加留意的人可能会理所当然的认为两个答案会是一致的，但结果却是： 12truefalse 下面一个很好解释，因为自动拆装箱机制，比较的是两者的引用，而不是值，所以为false，那么为什么前者是同一个引用呢？ 来看看Integer这个类，首先是自动拆装箱会调用valueOf()方法 123456public static Integer valueOf(int i) &#123; assert IntegerCache.high &gt;= 127; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; 这里并不是简单的返回new Integer(i) 而是判断了一下int的数值，Integer的存在一个缓存机制，默认用一个IntegerCache缓存了[IntegerCache.low,IntegerCache.high]的引用,其中IntegerCache这个内部类真正在做缓存 1234567891011121314151617181920212223242526private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(\"java.lang.Integer.IntegerCache.high\"); if (integerCacheHighPropValue != null) &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); &#125; private IntegerCache() &#123;&#125; &#125; 所以就出现了最开始的一个小trick","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"}]},{"title":"java trick--String.intern()","slug":"java trick--system.out.println","date":"2016-11-07T14:03:52.000Z","updated":"2017-08-22T08:28:34.717Z","comments":true,"path":"2016/11/07/java trick--system.out.println/","link":"","permalink":"http://lexburner.github.io/2016/11/07/java trick--system.out.println/","excerpt":"","text":"多线程在使用system.out.println时要留一个有意思的地方 123456789101112131415161718192021public class Main &#123; public static void main(String[] args) &#123; Thread thread = new MyThread(); thread.start(); System.out.println(\"end\"); &#125;&#125;class MyThread extends Thread &#123; private int i = 0; @Override public void run() &#123; while (true) &#123; i++; System.out.println(i); &#125; &#125;&#125; 主线程另起一个线程，然后在主线程最后打印一个end，猜猜看结果是什么？end会不会打印？主线程一直被Mythread占用原因就在于system.out.println是一个同步方法 12345678910111213/** * Prints an integer and then terminate the line. This method behaves as * though it invokes &lt;code&gt;&#123;@link #print(int)&#125;&lt;/code&gt; and then * &lt;code&gt;&#123;@link #println()&#125;&lt;/code&gt;. * * @param x The &lt;code&gt;int&lt;/code&gt; to be printed. */ public void println(int x) &#123; synchronized (this) &#123; print(x); newLine(); &#125; &#125;","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"}]},{"title":"使用zkclient操作zookeeper的学习过程记录","slug":"使用zkclient操作zookeeper的学习过程记录","date":"2016-08-16T07:52:52.000Z","updated":"2017-08-22T08:19:15.173Z","comments":true,"path":"2016/08/16/使用zkclient操作zookeeper的学习过程记录/","link":"","permalink":"http://lexburner.github.io/2016/08/16/使用zkclient操作zookeeper的学习过程记录/","excerpt":"前言最近开发的分布式(使用motan)项目中使用zookeeper作为服务中心来提供注册服务(@MotanService)和发现服务(@MotanRefer),虽然motan这个rpc框架对服务模块进行了很好的封装，但是以防以后会出现定制化的需求，以及对服务更好的监控，所以有必要了解一下zookeeper的基本知识和使用方法。关于zookeeper的知识点，网上很多的博客都已经介绍的很详尽了，我写这篇的博客的用意其实也就是将一些零散的却很精妙的博客整理出来，方便以后查阅。短篇以cp的方式，长篇的以url的方式。 zookeeper是什么？ ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。ZooKeeper包含一个简单的原语集，提供Java和C的接口。 ZooKeeper代码版本中，提供了分布式独享锁、选举、队列的接口。 —-百度百科 一开始看的云里雾里的，幸好我之前搞过一点hadoop，对他的生态体系有所了解，这才大概知道他想说什么。提炼几个关键词，并且加入我后面学习的理解，总结一下就是– zookeeper是一个组件，需要安装客户端和服务端，一般用于解决分布式开发下的一些问题。化抽象为具体，你可以把整个zookeeper理解成一个树形数据结构，也可以理解为一个文件系统的结构，每个叶子节点都会携带一些信息(data)，并且也可能会携带一些操作(op)。分布式场景中，每一个客户端都可以访问到这些叶子节点，并且进行一些操作。我们所有使用zookeeper的场景几乎都是在CRUD某一个或者某些叶子节点，然后会触发对应的操作…即zookeeper本身可以理解为一个shareData。—-来自于博主的口胡 zookeeper怎么学？学一个新的中间件的最好方法是先在脑子里面有一个想法：我为什么要学他，是想解决什么问题，他大概是个什么东西，我觉得打开思路的最好方式是看几篇博客(大多数情况你一开始看不懂，但是混个眼熟)，然后看视频，这里我自己是了解过了zookeeper原生的api之后看了极客学院的视频","text":"前言最近开发的分布式(使用motan)项目中使用zookeeper作为服务中心来提供注册服务(@MotanService)和发现服务(@MotanRefer),虽然motan这个rpc框架对服务模块进行了很好的封装，但是以防以后会出现定制化的需求，以及对服务更好的监控，所以有必要了解一下zookeeper的基本知识和使用方法。关于zookeeper的知识点，网上很多的博客都已经介绍的很详尽了，我写这篇的博客的用意其实也就是将一些零散的却很精妙的博客整理出来，方便以后查阅。短篇以cp的方式，长篇的以url的方式。 zookeeper是什么？ ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。ZooKeeper包含一个简单的原语集，提供Java和C的接口。 ZooKeeper代码版本中，提供了分布式独享锁、选举、队列的接口。 —-百度百科 一开始看的云里雾里的，幸好我之前搞过一点hadoop，对他的生态体系有所了解，这才大概知道他想说什么。提炼几个关键词，并且加入我后面学习的理解，总结一下就是– zookeeper是一个组件，需要安装客户端和服务端，一般用于解决分布式开发下的一些问题。化抽象为具体，你可以把整个zookeeper理解成一个树形数据结构，也可以理解为一个文件系统的结构，每个叶子节点都会携带一些信息(data)，并且也可能会携带一些操作(op)。分布式场景中，每一个客户端都可以访问到这些叶子节点，并且进行一些操作。我们所有使用zookeeper的场景几乎都是在CRUD某一个或者某些叶子节点，然后会触发对应的操作…即zookeeper本身可以理解为一个shareData。—-来自于博主的口胡 zookeeper怎么学？学一个新的中间件的最好方法是先在脑子里面有一个想法：我为什么要学他，是想解决什么问题，他大概是个什么东西，我觉得打开思路的最好方式是看几篇博客(大多数情况你一开始看不懂，但是混个眼熟)，然后看视频，这里我自己是了解过了zookeeper原生的api之后看了极客学院的视频 zkclient的使用学完原生api之后一般我们不直接使用，类比redis的客户端jedis，再到spring提供的redisTemplate;类比jdbc到dbutils，再到orm框架。所以作为小白，我建议使用这个比较简单的客户端zkclient，当后期需求需要一些定制化需求时使用原生的api自己重写，或者使用更高级一点的其他客户端。 zkclient我学完之后觉得非常轻量级，设计也很规范，大概可以参考以下的博客。博客园-房继诺原作者非常用心，里面给出了一张zkclient的uml类图，如下顺便也复习一下uml类图的知识，理解清楚图中用到的聚合，组合，关联，泛化，实现的箭头含义。uml建模没有学好的同学的移步这个链接，里面对应了java讲解，还算详细。掌握这个客户端之后，还需要补充一些注意点 1. create方法:创建节点时,如果节点已经存在,仍然抛出NodeExistException,可是我期望它不在抛出此异常. 2. retryUtilConnected: 如果向zookeeper请求数据时(create,delete,setData等),此时链接不可用,那么调用者将会被阻塞直到链接建立成功;不过我仍然需要一些方法是非阻塞的,如果链接不可用,则抛出异常,或者直接返回. 3. create方法: 创建节点时,如果节点的父节点不存在,我期望同时也要创建父节点,而不是抛出异常. 4. data监测: 我需要提供一个额外的功能来补充watch的不足,开启一个线程,间歇性的去zk server获取指定的path的data,并缓存起来..归因与watch可能丢失,以及它不能持续的反应znode数据的每一次变化,所以只能手动去同步获取. 回到开始这个时候看看你当初为啥要学习zookeeper，看看能不能解决你当时遇到的问题。如果你有兴趣，可以自己去试试zookeeper前面提到的那些可以实现的功能：分布式锁、选举、队列等等","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://lexburner.github.io/tags/zookeeper/"}]},{"title":"Hello World","slug":"hello-world","date":"2016-08-16T07:52:52.000Z","updated":"2017-08-22T04:31:48.389Z","comments":true,"path":"2016/08/16/hello-world/","link":"","permalink":"http://lexburner.github.io/2016/08/16/hello-world/","excerpt":"","text":"","categories":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"},{"name":"Validation","slug":"Validation","permalink":"http://lexburner.github.io/tags/Validation/"}]},{"title":"使用JPA实现乐观锁","slug":"使用JPA实现乐观锁","date":"2016-08-16T07:52:52.000Z","updated":"2017-08-22T08:18:33.986Z","comments":true,"path":"2016/08/16/使用JPA实现乐观锁/","link":"","permalink":"http://lexburner.github.io/2016/08/16/使用JPA实现乐观锁/","excerpt":"乐观锁的概念就不再赘述了，不了解的朋友请自行百度谷歌之，今天主要说的是在项目中如何使用乐观锁，做成一个小demo。 持久层使用jpa时，默认提供了一个注解@Version先看看源码怎么描述这个注解的 1234@Target(&#123; METHOD, FIELD &#125;)@Retention(RUNTIME)public @interface Version &#123;&#125; 简单来说就是用一个version字段来充当乐观锁的作用。先来设计实体类 123456789101112131415161718192021/** * Created by xujingfeng on 2017/1/30. */@Entity@Table(name = \"t_student\")public class Student &#123; @Id @GenericGenerator(name = \"PKUUID\", strategy = \"uuid2\") @GeneratedValue(generator = \"PKUUID\") @Column(length = 36) private String id; @Version private int version; private String name; //getter()... //setter()...&#125;","text":"乐观锁的概念就不再赘述了，不了解的朋友请自行百度谷歌之，今天主要说的是在项目中如何使用乐观锁，做成一个小demo。 持久层使用jpa时，默认提供了一个注解@Version先看看源码怎么描述这个注解的 1234@Target(&#123; METHOD, FIELD &#125;)@Retention(RUNTIME)public @interface Version &#123;&#125; 简单来说就是用一个version字段来充当乐观锁的作用。先来设计实体类 123456789101112131415161718192021/** * Created by xujingfeng on 2017/1/30. */@Entity@Table(name = \"t_student\")public class Student &#123; @Id @GenericGenerator(name = \"PKUUID\", strategy = \"uuid2\") @GeneratedValue(generator = \"PKUUID\") @Column(length = 36) private String id; @Version private int version; private String name; //getter()... //setter()...&#125; Dao层 12345678910/** * Created by xujingfeng on 2017/1/30. */public interface StudentDao extends JpaRepository&lt;Student,String&gt;&#123; @Query(\"update Student set name=?1 where id=?2\") @Modifying @Transactional int updateNameById(String name,String id);&#125; Controller层充当单元测试的作用，通过访问一个requestMapping来触发我们想要测试的方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Created by xujingfeng on 2017/1/30. */@Controllerpublic class StudentController &#123; @Autowired StudentDao studentDao; @RequestMapping(\"student.html\") @ResponseBody public String student()&#123; Student student = new Student(); student.setName(\"xujingfeng\"); studentDao.save(student); return \"student\"; &#125; @RequestMapping(\"testVersion.html\") @ResponseBody public String testVersion() throws InterruptedException &#123; Student student = studentDao.findOne(\"6ed16acc-61df-4a66-add9-d17c88b69755\"); student.setName(\"xuxuan\"); new Thread(new Runnable() &#123; @Override public void run() &#123; studentDao.findOne(\"6ed16acc-61df-4a66-add9-d17c88b69755\"); student.setName(\"xuxuanInThread\"); studentDao.save(student); &#125; &#125;).start(); Thread.sleep(1000); studentDao.save(student); return \"testVersion\"; &#125; @RequestMapping(\"updateNameById.html\") @ResponseBody public String updateNameById()&#123; studentDao.updateNameById(\"xuxuan2\",\"6ed16acc-61df-4a66-add9-d17c88b69755\"); return \"updateNameById\"; &#125;&#125; 这里面三个方法，主要是我们想用来测试的三个注意点。第一个方法student.html我们想看看springdata如何对version字段进行增长的。就不贴图了，直接给结论，对于添加了@Version的注解，我们不需要手动去控制，每一次save操作会在原来的基础上+1，如果初始为null，则springdata自动设置其为0。第二个方法testVersion.html是乐观锁的核心，当多个线程并发访问同一行记录时，添加了@Version乐观锁之后，程序会进行怎么样的控制呢？ 1org.hibernate.StaleObjectStateException: Row was updated or deleted by another transaction (or unsaved-value mapping was incorrect) : [com.example.jpa.Student#6ed16acc-61df-4a66-add9-d17c88b69755] 异常信息如上，主线程和新线程获取了同一行记录，并且新线程优先提交了事务，版本号一致，修改成功。等到了主线程再想save提交事务时，便得到一个版本号不一致的异常，那么在项目开发中就应该自己捕获这个异常根据业务内容做对应处理，是重试还是放弃etc… 第三个方法，updateNameById.html是想强调一下，@Query中的update，delete操作是不会触发springdata的相关代理操作的，而是转化为原生sql的方式，所以在项目中使用时也要注意这点。 总结乐观锁，用在一些敏感业务数据上，而其本身的修饰：乐观，代表的含义便是相信大多数场景下version是一致的。但是从业务角度出发又要保证数据的严格一致性，避免脏读等问题，使用的场景需要斟酌。记得前面一片博文简单介绍了一下行级锁的概念，其实本质上和乐观锁都是想要再数据库层面加锁控制并发，那么什么时候该用乐观锁，行级锁，什么时候得在程序级别加同步锁，又要根据具体的业务场景去判断。找到能够满足自己项目需求的方案，找到性能和可靠性的平衡点，才是一个程序员的价值所在。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://lexburner.github.io/tags/多线程/"},{"name":"数据库","slug":"数据库","permalink":"http://lexburner.github.io/tags/数据库/"}]}]}