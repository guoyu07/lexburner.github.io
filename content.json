{"meta":{"title":"徐靖峰|个人博客","subtitle":null,"description":null,"author":"徐靖峰","url":"http://lexburner.github.io"},"pages":[{"title":"徐靖峰","date":"2017-08-22T04:47:32.980Z","updated":"2017-08-22T04:47:32.980Z","comments":true,"path":"about/index.html","permalink":"http://lexburner.github.io/about/index.html","excerpt":"","text":"生于1995年，江苏泰州人，毕业于常州大学，目前就职于中科软，地点位于上海浦东新区，从事业务开发和基础架构研发工作。 大一时有幸加入校ACM队，使用C/C++，对算法和编程产生了浓厚的兴趣。大四来上海中科软担任JAVA后端实习后，就职至今。对如今主流的互联网技术均有不同程度的掌握，尤其擅长后端技术，对软件设计有自己的理解，运维测试方面有过涉猎，前端为弱势项。平时喜欢分享知识，交流见闻，博客旧址：徐靖峰的CSDN。 coding之外，有如下的兴趣爱好： 看动漫(bilibili)，看电影，看直播(douyu) 游戏(毕业后已经很少接触) 写日记&amp;感想，思考 旅游摄影 最近在研究领域驱动设计，ELK，openresty。在项目不忙时，会经常更新博客，记录自己对开发设计的理解。如果你对博客中有任何的疑问&amp;建议，欢迎与我交流。"},{"title":"Categories","date":"2017-08-21T10:10:46.417Z","updated":"2017-08-21T06:03:05.876Z","comments":true,"path":"categories/index.html","permalink":"http://lexburner.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2017-08-21T06:03:05.883Z","updated":"2017-08-21T06:03:05.883Z","comments":true,"path":"tags/index.html","permalink":"http://lexburner.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"浅析分布式下的事件驱动机制（PubSub模式）","slug":"event-2","date":"2017-09-13T14:49:23.000Z","updated":"2017-09-14T01:13:30.473Z","comments":true,"path":"2017/09/13/event-2/","link":"","permalink":"http://lexburner.github.io/2017/09/13/event-2/","excerpt":"上一篇文章《浅析Spring中的事件驱动机制》简单介绍了Spring对事件的支持。Event的整个生命周期，从publisher发出，经过applicationContext容器通知到EventListener，都是发生在单个Spring容器中，而在分布式场景下，有些时候一个事件的产生，可能需要被多个实例响应，本文主要介绍分布式场景下的事件驱动机制，由于使用了Redis，ActiveMQ，也可以换一个名词来理解：分布式下的发布订阅模式。 JMS规范在日常项目开发中，我们或多或少的发现一些包一些类位于java或javax中，他们主要提供抽象类，接口，提供了一种规范，如JPA，JSR，JNDI，JTA，JMS，他们是由java指定的标准规范，一流企业做标准、二流企业做品牌、三流企业做产品，虽然有点调侃的意味，但也可以见得它的重要意义。而JMS就是java在消息服务上指定的标准 The Java Message Service (JMS) API is a messaging standard that allows application components based on the Java Platform Enterprise Edition (Java EE) to create, send, receive, and read messages. It enables distributed communication that is loosely coupled, reliable, and asynchronous. JMS（JAVA Message Service,java消息服务）API是一个消息服务的标准或者说是规范，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。 消息中间件有非常多的实现，如ActiveMQ，RabbitMQ，RocketMQ，而他们同一遵循的接口规范，便是JMS。在下文中即将出现的ConnectionFactory，Destination，Connection，Session，MessageListener，Topic，Queue等等名词，都是JMS核心的接口，由于本文的初衷并不是讲解MQ&amp;JMS，所以这些机制暂且跳过。 定义分布式事件需求在上一个项目中，我们对接了外网的http接口，而安全性的保障则是交给OAuth2来完成，作为OAuth2的客户端，我们需要获取服务端返回的token，而token接口的获取次数每个月是有限制的，于是我们选择使用Redis来保存，定时刷新。由于每次发起请求时都要携带token，为了更高的性能减少一次redis io，我们在TokenService中使用了本地变量缓存token。于是形成如下的token获取机制： 这个图并不复杂，只是为了方便描述需求：首先去本地变量中加载token，若token==null，则去Redis加载，若Redis未命中（token过期了），则最终调用外部的http接口获取实时的token，同时存入redis中和本地变量中。 这个需求设计到这样一个问题：大多数情况下是单个实例中发现redis中的token为空，而它需要同时获取最新token，并通知其他的实例也去加载最新的token，这个时候事件广播就可以派上用场了。 由于token缓存在了Redis中，我们首先介绍Redis的发布订阅机制。","text":"上一篇文章《浅析Spring中的事件驱动机制》简单介绍了Spring对事件的支持。Event的整个生命周期，从publisher发出，经过applicationContext容器通知到EventListener，都是发生在单个Spring容器中，而在分布式场景下，有些时候一个事件的产生，可能需要被多个实例响应，本文主要介绍分布式场景下的事件驱动机制，由于使用了Redis，ActiveMQ，也可以换一个名词来理解：分布式下的发布订阅模式。 JMS规范在日常项目开发中，我们或多或少的发现一些包一些类位于java或javax中，他们主要提供抽象类，接口，提供了一种规范，如JPA，JSR，JNDI，JTA，JMS，他们是由java指定的标准规范，一流企业做标准、二流企业做品牌、三流企业做产品，虽然有点调侃的意味，但也可以见得它的重要意义。而JMS就是java在消息服务上指定的标准 The Java Message Service (JMS) API is a messaging standard that allows application components based on the Java Platform Enterprise Edition (Java EE) to create, send, receive, and read messages. It enables distributed communication that is loosely coupled, reliable, and asynchronous. JMS（JAVA Message Service,java消息服务）API是一个消息服务的标准或者说是规范，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。 消息中间件有非常多的实现，如ActiveMQ，RabbitMQ，RocketMQ，而他们同一遵循的接口规范，便是JMS。在下文中即将出现的ConnectionFactory，Destination，Connection，Session，MessageListener，Topic，Queue等等名词，都是JMS核心的接口，由于本文的初衷并不是讲解MQ&amp;JMS，所以这些机制暂且跳过。 定义分布式事件需求在上一个项目中，我们对接了外网的http接口，而安全性的保障则是交给OAuth2来完成，作为OAuth2的客户端，我们需要获取服务端返回的token，而token接口的获取次数每个月是有限制的，于是我们选择使用Redis来保存，定时刷新。由于每次发起请求时都要携带token，为了更高的性能减少一次redis io，我们在TokenService中使用了本地变量缓存token。于是形成如下的token获取机制： 这个图并不复杂，只是为了方便描述需求：首先去本地变量中加载token，若token==null，则去Redis加载，若Redis未命中（token过期了），则最终调用外部的http接口获取实时的token，同时存入redis中和本地变量中。 这个需求设计到这样一个问题：大多数情况下是单个实例中发现redis中的token为空，而它需要同时获取最新token，并通知其他的实例也去加载最新的token，这个时候事件广播就可以派上用场了。 由于token缓存在了Redis中，我们首先介绍Redis的发布订阅机制。 Redis中的Pub与Subredis不仅仅具备缓存的功能，它还拥有一个channel机制，我们可以使用Redis来进行发布订阅。上述的token流程我们简化一下，省略保存到redis的那一环，直接介绍如何通知其他应用刷新token。 引入依赖和配置1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 12345spring: redis: database: 0 host: localhost port: 6379 定义TokenService1234567891011121314151617181920@Servicepublic class TokenService &#123; @Autowired StringRedisTemplate redisTemplate; public void getToken(String username) &#123; // &lt;1&gt; String token = UUID.randomUUID().toString(); //模拟http接口使用用户名和密码获取token System.out.println(username + \" 成功获取token ...\" + token); //发送token刷新广播 System.out.println(\"广播token刷新事件 ...\"); redisTemplate.convertAndSend(RedisPubSubConfig.tokenChannel, token); &#125; public void refreshTokenListener(String token) &#123; // &lt;2&gt; System.out.println(\"接到token刷新事件，刷新 token : \" + token); &#125;&#125; 模拟获取token的方法，获取token的同时发送广播。 用于接收其他应用发送过来的广播消息。 配置RedisMessageListenerContainer在Spring应用中Event是由Spring容器管理的，而在Redis的消息机制中，Event是由RedisMessageListenerContainer管理的。我们为token配置一个channel，用于刷新token： 123456789101112131415161718192021222324252627@Configurationpublic class RedisPubSubConfig &#123; public final static String tokenChannel = \"tokenChannel\"; @Bean RedisMessageListenerContainer redisMessageListenerContainer(RedisConnectionFactory redisConnectionFactory) &#123; RedisMessageListenerContainer redisMessageListenerContainer = new RedisMessageListenerContainer();// &lt;1&gt; redisMessageListenerContainer.setConnectionFactory(redisConnectionFactory); redisMessageListenerContainer.addMessageListener(tokenRefreshListener(), new ChannelTopic(tokenChannel)); // &lt;2&gt; return redisMessageListenerContainer; &#125; @Autowired TokenService tokenService; MessageListener tokenRefreshListener() &#123; return new MessageListener() &#123; @Override public void onMessage(Message message, byte[] pattern) &#123; byte[] bytes = message.getBody(); // &lt;3&gt; tokenService.refreshTokenListener(new String(bytes)); &#125; &#125;; &#125;&#125; RedisMessageListenerContainer用于管理所有的redis相关的发布与订阅 为Redis容器注册特定的订阅者，在本例中使用tokenRefreshListener监听tokenChannel频道，当收到消息通知时，会自动调用onMessage方法。 使用message.getBody()可以获取消息的具体内容，在本例中即token 测试结果同样的这个应用，我们在8080,8081,8082启动三个，在8080中，我们调用tokenService.getToken(“kirito”);(注意必须要连接到redis的同一个database) 在三个控制台中我们得到了如下的结果： 8080： 123kirito 成功获取token ...5d4d2a48-934f-450d-8806-e6095b172286广播token刷新事件 ...接到token刷新事件，刷新 token : 5d4d2a48-934f-450d-8806-e6095b172286 8081： 1接到token刷新事件，刷新 token : 5d4d2a48-934f-450d-8806-e6095b172286 8082： 1接到token刷新事件，刷新 token : 5d4d2a48-934f-450d-8806-e6095b172286 可以发现其他系统的确收到了通知。 ActiveMQ中的Pub与SubRedis中的发布订阅其实在真正的企业开发中并不是很常用，如果涉及到一致性要求较高的需求，专业的消息中间件可以更好地为我们提供服务。下面介绍一下ActiveMQ如何实现发布订阅。 ActiveMQ为我们提供很好的监控页面，延时队列，消息ACK，事务，持久化等等机制，且拥有较高的吞吐量，是企业架构中不可或缺的一个重要中间件。 引入依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-activemq&lt;/artifactId&gt;&lt;/dependency&gt; 12345678spring: activemq: in-memory: false # &lt;1&gt; broker-url: tcp://127.0.0.1:61616 user: admin password: admin jms: pub-sub-domain: true # &lt;2&gt; springboot的自动配置会帮我们启动一个内存中的消息队列，引入spring-boot-starter-activemq倚赖时需要特别注意这一点，本例连接本机的ActiveMQ。 springboot默认不支持PubSub模式，需要手动开启。 定义TokenService123456789101112131415161718192021222324252627282930@Servicepublic class TokenService &#123; @Autowired JmsTemplate jmsTemplate; // &lt;1&gt; @Autowired Topic tokenTopic; // &lt;3&gt; public void getToken(String username) &#123; String token = UUID.randomUUID().toString(); //模拟http接口使用用户名和密码获取token System.out.println(username + \" 成功获取token ...\" + token); //发送token刷新广播 System.out.println(\"广播token刷新事件 ...\"); try &#123; Message message = new ActiveMQMessage(); message.setStringProperty(\"token\", token); jmsTemplate.convertAndSend(tokenTopic, message);// &lt;1&gt; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; @JmsListener(destination = ActivemqPubSubConfig.tokenTopic) // &lt;2&gt; public void refreshTokenListener(Message message) throws Exception &#123; System.out.println(\"接到token刷新事件，刷新 token : \" + message.getStringProperty(\"token\")); &#125;&#125; 使用模板设计模式的好处体现了出来，再前面的RedisTemplate中我们也是使用同样的template.convertAndSend()发送消息 JmsListener对应于EventListener，接收来自ActiveMQ中tokenTopic的消息通知 tokenTopic定义在下面的config中 配置ActiveMQ的topic123456789101112@Configurationpublic class ActivemqPubSubConfig &#123; public final static String tokenTopic = \"tokenTopic\"; @Bean Topic tokenTopic()&#123; return new ActiveMQTopic(ActivemqPubSubConfig.tokenTopic); &#125;&#125; 非常简单的配置，因为ActiveMQAutoConfiguration已经帮我们做了相当多的配置，我们只需要顶一个topic即可使用ActiveMQ的功能。 查看ActiveMQ的监控端省略了发送消息的过程，实际上可以得到和Redis PubSub一样的效果。来看一下ActiveMQ自带的监控端，在发送消息后，发生了什么变化，访问本地端口http://localhost:8161/admin ，可以看到消息被消费了。 总结本文介绍了Redis，ActiveMQ的PubSub特性，这是我理解的分布式场景下的事件驱动的使用。事件驱动是一种思想，PubSub是一种模式，Redis，ActiveMQ是一种应用，落到实处，便可以是本文介绍的token这个小小的业务实现。但是注意，使用Redis，ActiveMQ理解事件驱动可以，但是不能等同事件驱动，事件驱动还有很多其他场景下体现，笔者功力不够，无法一一介绍，怕人误解，特此强调一下。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"},{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/tags/架构设计/"}]},{"title":"上一个电商项目的反思","slug":"rethink-1","date":"2017-09-11T13:02:43.000Z","updated":"2017-09-14T01:07:57.618Z","comments":true,"path":"2017/09/11/rethink-1/","link":"","permalink":"http://lexburner.github.io/2017/09/11/rethink-1/","excerpt":"","text":"加入中科软已经有了一个年头，从去年实习到今年转正，陆陆续续接触了大概四个项目。有电商类，互联网保险类，也经历过管理系统。幸运的是，这些项目都是从零开始，避免了让我去维护不堪入目的老旧系统。而这么多项目中令我印象最深刻的，就要属上一个电商项目了。这也是我接触到的真正意义的第一个微服务项目，到今天回首去看曾经的这个项目，有很多突破性地尝试，同时不可避免地也踩入了一些坑点，毕竟摸着石头过河。今天想聊聊我对上一个电商项目的反思。 项目简介准确的说是一个第三方的电商项目，商品来源是由主流电商的http接口提供（目前接入了京东，苏宁），打造我们自己的商城体系。使用的技术包括springboot，jpa，rpc框架使用的是motan，数据库使用的是oracle，基本都还算是主流的技术。 盲目地拆分微服务使用了springboot就是微服务了吗？使用rpc通信就是微服务了吗？刚接触到所谓的微服务架构时，无疑是让人兴奋的，但也没有太多的经验，以至于每提出一个新的需求，几乎就会新建一个服务。没有从宏观去思考如何拆分服务，那时还没有项目组成员尝试去使用领域驱动设计的思想去划分服务的边界，会议室中讨论最多的话题也是：我们的数据库该如何设计，而不是我们的领域该如何划分。项目初期，使用单体式的思想开发着分布式项目，新技术的引入还只是使人有点稍微的不顺手，但是项目越做越大后，越来越大的不适感逐渐侵蚀着我们的开发速度。 说道微服务的拆分，有很多个维度，这里主要谈两个维度： 系统维度：业务功能不同的需求，交给不同的系统完成，如订单，商品，地址，用户等系统需要拆分。 模块维度：基础架构层（如公用util），领域层，接口层，服务层，表现层的拆分。 在项目的初期，我们错误地认为微服务的拆分仅仅是系统维度的拆分，如商品系统和订单系统，而在模块维度上，缺少拆分的意识，如订单模块的表现层和服务层，我们虽然做了隔离（两个独立的tomcat）。但在后来，业务添加了一个新的需求：商城增加积分支持，让用户可以使用积分购买商品。我们突然发现，所谓的服务层和表现层严重的耦合，仅仅是在物理上进行了隔离，逻辑层面并没有拆分，这导致新的积分服务模块从原先的订单服务层拷贝了大量的代码。吸取了这个教训后，我们新的项目中采取了如下的分层方式： 其中比较关键的一点便是表现层与应用层的完全分离，交互完全使用DTO对象。不少同事产生了困惑，抱怨在表现层不能访问数据库，这让他们获取数据变得十分“麻烦”，应用层和表现层还多了一次数据拷贝的工作，用于将DO持久化对象转换成DTO对象。但这样的好处从长远来看，是不言而喻的。总结为以下几点： 1 应用层高度重用，没有表现形式的阻碍，PC端，移动端，外部服务都可以同时接入，需要组装什么样的数据，请自行组装。 2 应用层和领域层可以交由经验较为丰富的程序员负责，避免了一些低性能的数据操作，错误的并发控制等等。 3 解决远程调用数据懒加载的问题。从前的设计中，表现层拿到了领域层的对象，而领域层会使用懒加载技术，当表现层想要获取懒加载属性时，或得到一个no session的异常。在没有这个分层之前，如何方便地解决这个问题一度困扰了我们很长的一段时间。 数据库的滥用项目使用了oracle，我们所有的数据都存在于同一个oracle实例中，各个系统模块并没有做到物理层面的数据库隔离。这并不符合设计，一方面这给那些想要跨模块执行join操作的人留了后门，如执行订单模块和用户模块的级联查询；另一方面，还困扰了一部分对微服务架构不甚了解的程序员，在他们的想法中，同一个数据库实例反而方便了他们的数据操作。 严格意义上，不仅仅是不同系统之间的数据库不能互相访问。同一个系统维度的不同模块也应当限制，正如前面一节的分层架构中，表现层（web层）是不应该出现DAO的，pom文件中也不应该出现任何JPA，Hibernate，Mybatis一类的依赖，它所有的数据来源，必须是应用层。 另外一方面，由于历史遗留问题，需要对接一个老系统，他们的表和这个电商的oracle实例是同一个，而我竟然在他们的表上发现了触发器这种操作…在新的项目中，我们已经禁止使用数据库层面的触发器和物理约束。 在新的项目中，我们采用了阿里云的RDS(mysql)作为oracle的替代品，核心业务数据则放到了分布式数据库DRDS中，严格做到了数据库层面的拆分。 并发的控制电商系统不同于OA系统，CMS系统，余额，订单等等操作都是敏感操作，实实在在跟钱打交道的东西容不得半点马虎，然而即使是一些有经验的程序员，也写出了这样的扣减余额操作： 12345678public void reduce(String accountId,BigDecimal cost)&#123; Account account = accountService.findOne(accountId); BigDecimal balance = account.getBalance(); if(balance &gt; cost) balance = balance - cost;//用四则运算代替BigDecimal的api，方便表达 account.setBalance(balance); accountService.save(account);&#125; 很多人没有控制并发的意识，即使意识到了，也不知道如何根据业务场景采取合适的手段控制并发，是使用JPA中的乐观锁，还是使用数据库的行级自旋锁完成简单并发控制，还是for update悲观锁（这不建议被使用），还是基于redis或zookeeper一类的分布式锁？ 这种错误甚至都不容许等到code revivew时才被发现，而应该是尽力地杜绝。 代码规范小到java的变量的驼峰命名法，数据库中用‘_’分割单词，到业务代码该如何规范的书写，再到并发规范，性能调优。准确的说，没有人管理这些事，这样的工作落到了每个有悟性的开发者身上。模块公用的常量，系统公用的常量应当区分放置，禁止使用魔鬼数字，bool变量名不能以is开头等等细小的但是重要的规范，大量的条件查询findByxxx污染了DAO层，完全可以被predicates，criteria替代，RESTFUL规范指导设计web接口等等… 在新的项目中，一条条规范被逐渐添加到了项目单独的模块READ.me中。作为公司的一个junior developer，在建议其他成员使用规范开发项目时，得到的回应通常是：我的功能都已经实现了，干嘛要改；不符合规范又怎么样，要改你改时。有时候也是挺无力的，算是个人的一点牢骚吧。 软件设计的一点不足还是拿订单系统和商品系统来说事，虽然两个系统在物理上被拆分开了，但如果需要展示订单列表，订单详情，如今系统的设计会发起多次的远程调用，用于查询订单的归属商品，这是违背领域驱动设计的。订单中的商品就应当是归属于订单模块，正确的设计应该是使用冗余，代替频繁的跨网络节点远程调用。 另外一点便是高可用，由于机器内存的限制，所有的系统都只部署了单个实例，这其实并不是微服务的最佳实践。从系统应用，到zookeeper，redis，mq等中间件，都应当保证高可用，避免单点问题。没有真正实现做到横向扩展（知识理论上实现了），实在是有点遗憾。 系统没有熔断，降级处理，在新的项目中，由于我们引入了Spring Cloud，很多地方都可以out of box式使用框架提供的fallback处理，而这上一个电商项目由于框架的限制以及接口设计之初就没有预想到要做这样的操作，使得可靠性再减了几分。 自动化运维的缺失单体式应用的美好时代，只需要发布同一份war包。而微服务项目中，一切都变得不同，在我们这个不算特别庞大的电商系统中，需要被运行的服务模块也到达了30-40个。由于这个电商系统是部署在甲方自己的服务器中，一方面是业务部门的业务审批流程，一方面是如此众多的jar包运行，没有自动发布，没有持续集成。令我比较难忘的是初期发布版本，始终有一两个服务莫名奇妙的挂掉，对着终端中的服务列表，一个个排查，这种痛苦的经历。至今，这个系统仍然依靠运维人员，手动管理版本。 上一个项目有一些不可控的项目因素，而新的项目中，系统服务全部在阿里云上部署，也引入了Jenkins，一切都在逐渐变好，其他的devops工具仍然需要完善，以及docker一类的容器技术还未在计划日程之内，这些都是我们今年努力的目标。 总结原本积累了很多自己的想法，可惜落笔之后能够捕捉到一些点，便只汇聚成了上述这些，而这上一个电商项目在逐渐的迭代开发之后也变得越来越好了（我去了新的项目组后，其他同事负责了后续的开发）。这个经历，于我是非常珍贵的，它比那些大牛直接告诉我微服务设计的要素要更加有意义。知道了不足之处，经历了自己解决问题的过程，才会了解到好的方案的优势，了解到开源方案到底是为了解决什么样的问题而设计的。","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/categories/技术杂谈/"}],"tags":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/tags/技术杂谈/"}]},{"title":"浅析Spring中的事件驱动机制","slug":"event-1","date":"2017-09-10T12:03:58.000Z","updated":"2017-09-14T01:07:57.604Z","comments":true,"path":"2017/09/10/event-1/","link":"","permalink":"http://lexburner.github.io/2017/09/10/event-1/","excerpt":"","text":"今天来简单地聊聊事件驱动，其实写这篇文章挺令我挺苦恼的，因为事件驱动这个名词，我没有找到很好的定性解释，担心自己的表述有误，而说到事件驱动可能立刻联想到如此众多的概念：观察者模式，发布订阅模式，消息队列MQ，消息驱动，事件，EventSourcing…为了不产生歧义，笔者把自己所了解的这些模棱两可的概念都列了出来，再开始今天的分享。 在设计模式中，观察者模式可以算得上是一个非常经典的行为型设计模式，猫叫了，主人醒了，老鼠跑了，这一经典的例子，是事件驱动模型在设计层面的体现。 另一模式，发布订阅模式往往被人们等同于观察者模式，但我的理解是两者唯一区别，是发布订阅模式需要有一个调度中心，而观察者模式不需要，例如观察者的列表可以直接由被观察者维护。不过两者即使被混用，互相替代，通常不影响表达。 MQ，中间件级别的消息队列（e.g. ActiveMQ,RabbitMQ），可以认为是发布订阅模式的一个具体体现。事件驱动-&gt;发布订阅-&gt;MQ，从抽象到具体。 java和spring中都拥有Event的抽象，分别代表了语言级别和三方框架级别对事件的支持。 EventSourcing这个概念就要关联到领域驱动设计，DDD对事件驱动也是非常地青睐，领域对象的状态完全是由事件驱动来控制，由其衍生出了CQRS架构，具体实现框架有AxonFramework。 Nginx可以作为高性能的应用服务器（e.g. openResty），以及Nodejs事件驱动的特性，这些也是都是事件驱动的体现。 本文涵盖的内容主要是前面4点。 Spring对Event的支持Spring的文档对Event的支持翻译之后描述如下： ApplicationContext通过ApplicationEvent类和ApplicationListener接口进行事件处理。 如果将实现ApplicationListener接口的bean注入到上下文中，则每次使用ApplicationContext发布ApplicationEvent时，都会通知该bean。 本质上，这是标准的观察者设计模式。 而在spring4.2之后，提供了注解式的支持，我们可以使用任意的java对象配合注解达到同样的效果，首先来看看不适用注解如何在Spring中使用事件驱动机制。 定义业务需求：用户注册后，系统需要给用户发送邮件告知用户注册成功，需要给用户初始化积分；隐含的设计需求，用户注册后，后续需求可能会添加其他操作，如再发送一条短信等等，希望程序具有扩展性，以及符合开闭原则。 如果不使用事件驱动，代码可能会像这样子： 1234567891011121314151617public class UserService &#123; @Autowired EmailService emailService; @Autowired ScoreService scoreService; @Autowired OtherService otherService; public void register(String name) &#123; System.out.println(\"用户：\" + name + \" 已注册！\"); emailService.sendEmail(name); scoreService.initScore(name); otherService.execute(name); &#125; &#125; 要说有什么毛病，其实也不算有，因为可能大多数人在开发中都会这么写，喜欢写同步代码。但这么写，实际上并不是特别的符合隐含的设计需求，假设增加更多的注册项service，我们需要修改register的方法，并且让UserService注入对应的Service。而实际上，register并不关心这些“额外”的操作，如何将这些多余的代码抽取出去呢？便可以使用Spring提供的Event机制。 定义用户注册事件1234567public class UserRegisterEvent extends ApplicationEvent&#123; public UserRegisterEvent(String name) &#123; //name即source super(name); &#125;&#125; ApplicationEvent是由Spring提供的所有Event类的基类，为了简单起见，注册事件只传递了name（可以复杂的对象，但注意要了解清楚序列化机制）。 定义用户注册服务(事件发布者)123456789101112131415@Service // &lt;1&gt;public class UserService implements ApplicationEventPublisherAware &#123; // &lt;2&gt; public void register(String name) &#123; System.out.println(\"用户：\" + name + \" 已注册！\"); applicationEventPublisher.publishEvent(new UserRegisterEvent(name));// &lt;3&gt; &#125; private ApplicationEventPublisher applicationEventPublisher; // &lt;2&gt; @Override public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) &#123; // &lt;2&gt; this.applicationEventPublisher = applicationEventPublisher; &#125;&#125; 服务必须交给Spring容器托管 ApplicationEventPublisherAware是由Spring提供的用于为Service注入ApplicationEventPublisher事件发布器的接口，使用这个接口，我们自己的Service就拥有了发布事件的能力。 用户注册后，不再是显示调用其他的业务Service，而是发布一个用户注册事件。 定义邮件服务，积分服务，其他服务(事件订阅者)12345678@Service // &lt;1&gt;public class EmailService implements ApplicationListener&lt;UserRegisterEvent&gt; &#123; // &lt;2&gt; @Override public void onApplicationEvent(UserRegisterEvent userRegisterEvent) &#123; System.out.println(\"邮件服务接到通知，给 \" + userRegisterEvent.getSource() + \" 发送邮件...\");// &lt;3&gt; &#125;&#125; 事件订阅者的服务同样需要托管于Spring容器 ApplicationListener&lt;E extends ApplicationEvent&gt;接口是由Spring提供的事件订阅者必须实现的接口，我们一般把该Service关心的事件类型作为泛型传入。 处理事件，通过event.getSource()即可拿到事件的具体内容，在本例中便是用户的姓名。 其他两个Service，也同样编写，实际的业务操作仅仅是打印一句内容即可，篇幅限制，这里省略。 编写启动类1234567891011121314151617@SpringBootApplication@RestControllerpublic class EventDemoApp &#123; public static void main(String[] args) &#123; SpringApplication.run(EventDemoApp.class, args); &#125; @Autowired UserService userService; @RequestMapping(\"/register\") public String register()&#123; userService.register(\"kirito\"); return \"success\"; &#125;&#125; 当我们调用userService.register(“kirito”);方法时，控制台打印信息如下： 他们的顺序是无序的，如果需要控制顺序，需要重写order接口，这点不做介绍。其次，我们完成了用户注册和其他服务的解耦，这也是事件驱动的最大特性之一，如果需要在用户注册时完成其他操作，只需要再添加相应的事件订阅者即可。 Spring 对Event的注解支持上述的几个接口已经非常清爽了，如果习惯使用注解，Spring也提供了，不再需要显示实现 注解式的事件发布者123456789101112@Servicepublic class UserService &#123; public void register(String name) &#123; System.out.println(\"用户：\" + name + \" 已注册！\"); applicationEventPublisher.publishEvent(new UserRegisterEvent(name)); &#125; @Autowired private ApplicationEventPublisher applicationEventPublisher;&#125; Spring4.2之后，ApplicationEventPublisher自动被注入到容器中，采用Autowired即可获取。 注解式的事件订阅者12345678@Servicepublic class EmailService &#123; @EventListener public void listenUserRegisterEvent(UserRegisterEvent userRegisterEvent) &#123; System.out.println(\"邮件服务接到通知，给 \" + userRegisterEvent.getSource() + \" 发送邮件...\"); &#125;&#125; @EventListener注解完成了ApplicationListener&lt;E extends ApplicationEvent&gt;接口的使命。 更多的特性可以参考SpringFramework的文档。 Spring中事件的应用在以往阅读Spring源码的经验中，接触了不少使用事件的地方，大概列了以下几个，加深以下印象： Spring Security中使用AuthenticationEventPublisher处理用户认证成功，认证失败的消息处理。 1234567public interface AuthenticationEventPublisher &#123; void publishAuthenticationSuccess(Authentication authentication); void publishAuthenticationFailure(AuthenticationException exception, Authentication authentication);&#125; Hibernate中持久化对象属性的修改是如何被框架得知的？正是采用了一系列持久化相关的事件，如DefaultSaveEventListener，DefaultUpdateEventListener,事件非常多，有兴趣可以去org.hibernate.event包下查看。 Spring Cloud Zuul中刷新路由信息使用到的ZuulRefreshListener 1234567891011121314private static class ZuulRefreshListener implements ApplicationListener&lt;ApplicationEvent&gt; &#123; ... public void onApplicationEvent(ApplicationEvent event) &#123; if(!(event instanceof ContextRefreshedEvent) &amp;&amp; !(event instanceof RefreshScopeRefreshedEvent) &amp;&amp; !(event instanceof RoutesRefreshedEvent)) &#123; if(event instanceof HeartbeatEvent &amp;&amp; this.heartbeatMonitor.update(((HeartbeatEvent)event).getValue())) &#123; this.zuulHandlerMapping.setDirty(true); &#125; &#125; else &#123; this.zuulHandlerMapping.setDirty(true); &#125; &#125; &#125; Spring容器生命周期相关的一些默认Event 1ContextRefreshedEvent,ContextStartedEvent,ContextStoppedEvent,ContextClosedEvent,RequestHandledEvent 。。。其实吧，非常多。。。 总结本文暂时只介绍了Spring中的一些简单的事件驱动机制，相信如果之后再看到Event，Publisher，EventListener一类的单词后缀时，也能立刻和事件机制联系上了。再阅读Spring源码时，如果发现出现了某个Event，但由于不是同步调用，所以很容易被忽视，我一般习惯下意识的去寻找有没有提供默认的Listener，这样不至于漏掉一些“隐藏”的特性。下一篇文章打算聊一聊分布式场景下，事件驱动使用的注意点。 公众号刚刚创立，如果觉得文章不错，希望能分享到您的朋友圈，如果对文章有什么想法和建议，可以与我沟通。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"},{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/tags/架构设计/"}]},{"title":"从Feign使用注意点到RESUFUL接口设计规范","slug":"feign-1","date":"2017-09-09T06:43:28.000Z","updated":"2017-09-11T01:21:11.841Z","comments":true,"path":"2017/09/09/feign-1/","link":"","permalink":"http://lexburner.github.io/2017/09/09/feign-1/","excerpt":"","text":"最近项目中大量使用了Spring Cloud Feign来对接http接口，踩了不少坑，也产生了一些对RESTFUL接口设计的想法，特此一篇记录下。 [TOC] SpringMVC的请求参数绑定机制了解Feign历史的朋友会知道，Feign本身是Netflix的产品，Spring Cloud Feign是在原生Feign的基础上进行了封装，引入了大量的SpringMVC注解支持，这一方面使得其更容易被广大的Spring使用者开箱即用，但也产生了不小的混淆作用。所以在使用Spring Cloud Feign之前，笔者先介绍一下SpringMVC的一个入参机制。预设一个RestController，在本地的8080端口启动一个应用，用于接收http请求。 123456789@RestControllerpublic class BookController &#123; @RequestMapping(value = \"/hello\") // &lt;1&gt; public String hello(String name) &#123; // &lt;2&gt; return \"hello \" + name; &#125;&#125; 这个接口写起来非常简单，但实际springmvc做了非常多的兼容，使得这个接口可以接受多种请求方式。 RequestMapping代表映射的路径，使用GET,POST,PUT,DELETE方式都可以映射到该端点。 SpringMVC中常用的请求参数注解有（@RequestParam,@RequestBody,@PathVariable）等。name被默认当做@RequestParam。形参String name由框架使用字节码技术获取name这个名称，自动检测请求参数中key值为name的参数，也可以使用@RequestParam(“name”)覆盖变量本身的名称。当我们在url中携带name参数或者form表单中携带name参数时，会被获取到。 12345POST /hello HTTP/1.1Host: localhost:8080Content-Type: application/x-www-form-urlencodedname=formParam 或 12GET /hello?name=queryString HTTP/1.1Host: localhost:8080 Feign的请求参数绑定机制上述的SpringMVC参数绑定机制，大家应该都是非常熟悉的，但这一切在Feign中有些许的不同。 我们来看一个非常简单的，但是实际上错误的接口写法： 12345678//注意：错误的接口写法@FeignClient(\"book\")public interface BookApi &#123; @RequestMapping(value = \"/hello\",method = RequestMethod.GET) String hello(String name);&#125; 配置请求地址： 1234567ribbon: eureka: enabled: falsebook: ribbon: listOfServers: http://localhost:8080 我们按照写SpringMVC的RestController的习惯写了一个FeignClient，按照我们的一开始的想法，由于指定了请求方式是GET，那么name应该会作为QueryString拼接到Url中吧？发出一个这样的GET请求： 12GET /hello?name=xxx HTTP/1.1Host: localhost:8080 而实际上，RestController并没有接收到，我们在RestController一侧的应用中获得了一些提示： 并没有按照期望使用GET方式发送请求，而是POST方式 name参数没有被封装，获得了一个null值 查看文档发现，如果不加默认的注解，Feign则会对参数默认加上@RequestBody注解，而RequestBody一定是包含在请求体中的，GET方式无法包含。所以上述两个现象得到了解释。Feign在GET请求包含RequestBody时强制转成了POST请求，而不是报错。 理解清楚了这个机制我们就可以在开发Feign接口避免很多坑。而解决上述这个问题也很简单 在Feign接口中为name添加@RequestParam(“name”)注解，name必须指定，Feign的请求参数不会利用SpringMVC字节码的机制自动给定一个默认的名称。 由于Feign默认使用@RequestBody，也可以改造RestController，使用@RequestBody接收。但是，请求参数通常是多个，推荐使用上述的@RequestParam，而@RequestBody一般只用于传递对象。 Feign绑定复合参数指定请求参数的类型与请求方式，上述问题的出现实际上是由于在没有理清楚Feign内部机制的前提下想当然的和SpringMVC进行了类比。同样，在使用对象作为参数时，也需要注意这样的问题。 对于这样的接口 1234567891011121314151617@FeignClient(\"book\")public interface BookApi &#123; @RequestMapping(value = \"/book\",method = RequestMethod.POST) Book book(@RequestBody Book book); // &lt;1&gt; @RequestMapping(value = \"/book\",method = RequestMethod.POST) Book book(@RequestParam(\"id\") String id,@RequestParam(\"name\") String name); // &lt;2&gt; @RequestMapping(value = \"/book\",method = RequestMethod.POST) Book book(@RequestParam Map map); // &lt;3&gt; //错误的写法 @RequestMapping(value = \"/book\",method = RequestMethod.POST) Book book(@RequestParam Book book); // &lt;4&gt;&#125; 使用@RequestBody传递对象是最常用的方式。 如果参数并不是很多，可以平铺开使用@RequestParam 使用Map，这也是完全可以的，但不太符合面向对象的思想，不能从代码立刻看出该接口需要什么样的参数。 错误的用法，Feign没有提供这样的机制自动转换实体为Map。 Feign中使用@PathVariable与RESTFUL规范这涉及到一个如何设计RESTFUL接口的话题，我们知道在自从RESTFUL在2000年初被提出来之后，就不乏文章提到资源，契约规范，CRUD对应增删改查操作等等。下面笔者从两个实际的接口来聊聊自己的看法。 根据id查找用户接口： 1234567@FeignClient(\"user\")public interface UserApi &#123; @RequestMapping(value = \"/user/&#123;userId&#125;\",method = RequestMethod.GET) String findById(@PathVariable(\"id\") String userId);&#125; 这应该是没有争议的，注意前面强调的，@PathVariable(“id”)括号中的id不可以忘记。那如果是“根据邮箱查找用户呢”?很有可能下意识的写出这样的接口： 1234567@FeignClient(\"user\")public interface UserApi &#123; @RequestMapping(value = \"/user/&#123;email&#125;\",method = RequestMethod.GET) String findByEmail(@PathVariable(\"email\") String email);&#125; 首先看看Feign的问题。email中通常包含’.‘这个特殊字符，如果在路径中包含，会出现意想不到的结果。我不想探讨如何去解决它（实际上可以使用{email:.+}的方式),因为我觉得这不符合设计。 再谈谈规范的问题。这两个接口是否是相似的，email是否应该被放到path中？这就要聊到RESTFUL的初衷，为什么userId这个属性被普遍认为适合出现在RESTFUL路径中，因为id本身起到了资源定位的作用，他是资源的标记。而email不同，它可能是唯一的，但更多的，它是资源的属性，所以，笔者认为不应该在路径中出现非定位性的动态参数。而是把email作为@RequestParam参数。 RESUFTL结构化查询笔者成功的从Feign的话题过度到了RESTFUL接口的设计问题，也导致了本文的篇幅变长了，不过也不打算再开一片文章谈了。 再考虑一个接口设计，查询某一个月某个用户的订单，可能还会携带分页参数，这时候参数变得很多，按照传统的设计，这应该是一个查询操作，也就是与GET请求对应，那是不是意味着应当将这些参数拼接到url后呢？再思考Feign，正如本文的第二段所述，是不支持GET请求携带实体类的，这让我们设计陷入了两难的境地。而实际上参考一些DSL语言的设计如elasticSearch，也是使用POST JSON的方式来进行查询的，所以在实际项目中，笔者并不是特别青睐CRUD与四种请求方式对应的这种所谓的RESTFUL规范，如果说设计RESTFUL应该遵循什么规范，那大概是另一些名词，如契约规范和领域驱动设计。 1234567@FeignClient(\"order\")public interface BookApi &#123; @RequestMapping(value = \"/order/history\",method = RequestMethod.POST) Page&lt;List&lt;Orders&gt;&gt; queryOrderHistory(@RequestBody QueryVO queryVO);&#125; RESTFUL行为限定在实际接口设计中，我遇到了这样的需求，用户模块的接口需要支持修改用户密码，修改用户邮箱，修改用户姓名，而笔者之前阅读过一篇文章，也是讲舍弃CRUD而是用领域驱动设计来规范RESTFUL接口的定义，与项目中我的想法不谋而合。看似这三个属性是同一个实体类的三个属性，完全可以如下设计： 1234567@FeignClient(&quot;user&quot;)public interface UserApi &#123; @RequestMapping(value = &quot;/user&quot;,method = RequestMethod.POST) User update(@RequestBody User user);&#125; 但实际上，如果再考虑多一层，就应该产生这样的思考：这三个功能所需要的权限一致吗？真的应该将他们放到一个接口中吗？实际上，笔者并不希望接口调用方传递一个实体，因为这样的行为是不可控的，完全不知道它到底是修改了什么属性，如果真的要限制行为，还需要在User中添加一个操作类型的字段，然后在接口实现方加以校验，这太麻烦了。而实际上，笔者觉得规范的设计应当如下： 12345678910111213@FeignClient(\"user\")public interface UserApi &#123; @RequestMapping(value = \"/user/&#123;userId&#125;/password/update\",method = RequestMethod.POST) ResultBean&lt;Boolean&gt; updatePassword(@PathVariable(\"userId) String userId,@RequestParam(\"password\") password); @RequestMapping(value = \"/user/&#123;userId&#125;/email/update\",method = RequestMethod.POST) ResultBean&lt;Boolean&gt; updateEmail(@PathVariable(\"userId) String userId,@RequestParam(\"email\") String email); @RequestMapping(value = \"/user/&#123;userId&#125;/username/update\",method = RequestMethod.POST) ResultBean&lt;Boolean&gt; updateUsername(@PathVariable(\"userId) String userId,@RequestParam(\"username\") String username);&#125; 一般意义上RESTFUL接口不应该出现动词，这里的update并不是一个动作，而是标记着操作的类型，因为针对某个属性可能出现的操作类型可能会有很多，所以我习惯加上一个update后缀，明确表达想要进行的操作，而不是仅仅依赖于GET，POST，PUT，DELETE。实际上，修改操作推荐使用的请求方式应当是PUT，这点笔者的理解是，已经使用update标记了行为，实际开发中不习惯使用PUT。 password，email，username都是user的属性，而userId是user的识别符号，所以userId以PathVariable的形式出现在url中，而三个属性出现在ReqeustParam中。 顺带谈谈逻辑删除，如果一个需求是删除用户的常用地址，这个api的操作类型，我通常也不会设计为DELETE请求，而是同样使用delete来标记操作行为 12@RequestMapping(value = \"/user/&#123;userId&#125;/address/&#123;addressId&#125;/delete\",method = RequestMethod.POST) ResultBean&lt;Boolean&gt; updateEmail(@PathVariable(\"userId\") String userId,@PathVariable(\"userId\") String email); 总结本文从Feign的使用注意点，聊到了RESTFUL接口的设计问题，其实是一个互相补充的行为。接口设计需要载体，所以我以Feign的接口风格谈了谈自己对RESTFUL设计的理解，而Feign中一些坑点，也正是我想要规范RESTFUL设计的出发点。如有对RESTFUL设计不同的理解，欢迎与我沟通。","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/tags/Spring-Cloud/"}]},{"title":"Re：从零开始的Spring Session(三)","slug":"Re：从零开始的Spring Session(三)","date":"2017-09-04T12:57:43.000Z","updated":"2017-09-05T01:53:23.568Z","comments":true,"path":"2017/09/04/Re：从零开始的Spring Session(三)/","link":"","permalink":"http://lexburner.github.io/2017/09/04/Re：从零开始的Spring Session(三)/","excerpt":"","text":"上一篇文章中，我们使用Redis集成了Spring Session。大多数的配置都是Spring Boot帮我们自动配置的，这一节我们介绍一点Spring Session较为高级的特性。 集成Spring Security之所以把Spring Session和Spring Security放在一起讨论，是因为我们的应用在集成Spring Security之后，用户相关的认证与Session密不可分，如果不注意一些细节，会引发意想不到的问题。 与Spring Session相关的依赖可以参考上一篇文章，这里给出增量的依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 我们引入依赖后，就已经自动配置了Spring Security，我们在application.yml添加一个内存中的用户： 1234security: user: name: admin password: admin 测试登录点沿用上一篇文章的端点，访问http://localhost:8080/test/cookie?browser=chrome端点后会出现http basic的认证框，我们输入admin/admin，即可获得结果，也遇到了第一个坑点，我们会发现每次请求，sessionId都会被刷新，这显然不是我们想要的结果。 这个现象笔者研究了不少源码，但并没有得到非常满意的解释，只能理解为SecurityAutoConfiguration提供的默认配置，没有触发到响应的配置，导致了session的不断刷新（如果读者有合理的解释可以和我沟通）。Spring Session之所以能够替换默认的tomcat httpSession是因为配置了springSessionRepositoryFilter这个过滤器，且提供了非常高的优先级，这归功于AbstractSecurityWebApplicationInitializer ，AbstractHttpSessionApplicationInitializer 这两个初始化器，当然，也保证了Spring Session会在Spring Security之前起作用。 而解决上述的诡异现象也比较容易（但原理不清），我们使用@EnableWebSecurity对Spring Security进行一些配置，即可解决这个问题。 1234567891011121314151617181920212223242526@EnableWebSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter &#123; // @formatter:off @Override protected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(\"/resources/**\").permitAll() .anyRequest().authenticated() .and() .httpBasic()//&lt;1&gt; .and() .logout().permitAll(); &#125; // @formatter:on // @formatter:off @Autowired public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception &#123; auth .inMemoryAuthentication() .withUser(\"admin\").password(\"admin\").roles(\"USER\");//&lt;2&gt; &#125; // @formatter:on&#125; 不想大费周章写一个登录页面，于是开启了http basic认证 配置了security config之后，springboot的autoConfig就会失效，于是需要手动配置用户。 再次请求，可以发现SessionId返回正常，@EnableWebSecurity似乎触发了相关的配置，当然了，我们在使用Spring Security时不可能使用autoconfig，但是这个现象的确是一个疑点。 使用自定义CookieSerializer12345678@Beanpublic CookieSerializer cookieSerializer() &#123; DefaultCookieSerializer serializer = new DefaultCookieSerializer(); serializer.setCookieName(\"JSESSIONID\"); serializer.setCookiePath(\"/\"); serializer.setDomainNamePattern(\"^.+?\\\\.(\\\\w+\\\\.[a-z]+)$\"); return serializer;&#125; 使用上述配置后，我们可以将Spring Session默认的Cookie Key从SESSION替换为原生的JSESSIONID。而CookiePath设置为根路径且配置了相关的正则表达式，可以达到同父域下的单点登录的效果，在未涉及跨域的单点登录系统中，这是一个非常优雅的解决方案。如果我们的当前域名是moe.cnkirito.moe，该正则会将Cookie设置在父域cnkirito.moe中，如果有另一个相同父域的子域名blog.cnkirito.moe也会识别这个Cookie，便可以很方便的实现同父域下的单点登录。 根据用户名查找用户归属的SESSION这个特性听起来非常有意思，你可以在一些有趣的场景下使用它，如知道用户名后即可删除其SESSION。一直以来我们都是通过线程绑定的方式，让用户操作自己的SESSION，包括获取用户名等操作。但如今它提供了一个反向的操作，根据用户名获取SESSION，恰巧，在一些项目中真的可以使用到这个特性，最起码，当别人问起你，或者讨论到和SESSION相关的知识时，你可以明晰一点，这是可以做到的。 我们使用Redis作为Session Store还有一个好处，就是其实现了FindByIndexNameSessionRepository接口，下面让我们来见证这一点。 123456789101112@Controllerpublic class CookieController &#123; @Autowired FindByIndexNameSessionRepository&lt;? extends ExpiringSession&gt; sessionRepository; @RequestMapping(\"/test/findByUsername\") @ResponseBody public Map findByUsername(@RequestParam String username) &#123; Map&lt;String, ? extends ExpiringSession&gt; usersSessions = sessionRepository.findByIndexNameAndIndexValue(FindByIndexNameSessionRepository.PRINCIPAL_NAME_INDEX_NAME, username); return usersSessions; &#125;&#125; 由于一个用户可能拥有多个Session，所以返回的是一个Map信息，而这里的username，则就是与Spring Security集成之后的用户名，最令人感动Spring厉害的地方，是这一切都是自动配置好的。我们在内存中配置的用户的username是admin，于是我们访问这个端点,可以看到如下的结果 连同我们存入session中的browser=chrome，browser=360都可以看见（只有键名）。 总结Spring Session对各种场景下的Session管理提供一套非常完善的实现。笔者所介绍的，仅仅是Spring Session常用的一些特性，更多的知识点可以在spring.io的文档中一览无余，以及本文中作者存在的一个疑惑，如有兴趣可与我沟通。","categories":[{"name":"Spring Session","slug":"Spring-Session","permalink":"http://lexburner.github.io/categories/Spring-Session/"}],"tags":[{"name":"Spring Session","slug":"Spring-Session","permalink":"http://lexburner.github.io/tags/Spring-Session/"},{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"}]},{"title":"Re：从零开始的Spring Session(二)","slug":"Re：从零开始的Spring Session(二)","date":"2017-09-03T12:06:12.000Z","updated":"2017-09-04T01:37:18.453Z","comments":true,"path":"2017/09/03/Re：从零开始的Spring Session(二)/","link":"","permalink":"http://lexburner.github.io/2017/09/03/Re：从零开始的Spring Session(二)/","excerpt":"上一篇文章介绍了一些Session和Cookie的基础知识，这篇文章开始正式介绍Spring Session是如何对传统的Session进行改造的。官网这么介绍Spring Session： Spring Session provides an API and implementations for managing a user’s session information. It also provides transparent integration with: HttpSession - allows replacing the HttpSession in an application container (i.e. Tomcat) neutral way. Additional features include: Clustered Sessions - Spring Session makes it trivial to support clustered sessions without being tied to an application container specific solution. Multiple Browser Sessions - Spring Session supports managing multiple users’ sessions in a single browser instance (i.e. multiple authenticated accounts similar to Google). RESTful APIs - Spring Session allows providing session ids in headers to work with RESTful APIs WebSocket - provides the ability to keep the HttpSession alive when receiving WebSocket messages 其具体的特性非常之多，具体的内容可以从文档中了解到，笔者做一点自己的总结，Spring Session的特性包括但不限于以下： 使用GemFire来构建C/S架构的httpSession（不关注） 使用第三方仓储来实现集群session管理，也就是常说的分布式session容器，替换应用容器（如tomcat的session容器）。仓储的实现，Spring Session提供了三个实现（redis，mongodb，jdbc），其中redis使我们最常用的。程序的实现，使用AOP技术，几乎可以做到透明化地替换。（核心） 可以非常方便的扩展Cookie和自定义Session相关的Listener，Filter。 可以很方便的与Spring Security集成，增加诸如findSessionsByUserName，rememberMe，限制同一个账号可以同时在线的Session数（如设置成1，即可达到把前一次登录顶掉的效果）等等 介绍完特性，下面开始一步步集成Spring Session","text":"上一篇文章介绍了一些Session和Cookie的基础知识，这篇文章开始正式介绍Spring Session是如何对传统的Session进行改造的。官网这么介绍Spring Session： Spring Session provides an API and implementations for managing a user’s session information. It also provides transparent integration with: HttpSession - allows replacing the HttpSession in an application container (i.e. Tomcat) neutral way. Additional features include: Clustered Sessions - Spring Session makes it trivial to support clustered sessions without being tied to an application container specific solution. Multiple Browser Sessions - Spring Session supports managing multiple users’ sessions in a single browser instance (i.e. multiple authenticated accounts similar to Google). RESTful APIs - Spring Session allows providing session ids in headers to work with RESTful APIs WebSocket - provides the ability to keep the HttpSession alive when receiving WebSocket messages 其具体的特性非常之多，具体的内容可以从文档中了解到，笔者做一点自己的总结，Spring Session的特性包括但不限于以下： 使用GemFire来构建C/S架构的httpSession（不关注） 使用第三方仓储来实现集群session管理，也就是常说的分布式session容器，替换应用容器（如tomcat的session容器）。仓储的实现，Spring Session提供了三个实现（redis，mongodb，jdbc），其中redis使我们最常用的。程序的实现，使用AOP技术，几乎可以做到透明化地替换。（核心） 可以非常方便的扩展Cookie和自定义Session相关的Listener，Filter。 可以很方便的与Spring Security集成，增加诸如findSessionsByUserName，rememberMe，限制同一个账号可以同时在线的Session数（如设置成1，即可达到把前一次登录顶掉的效果）等等 介绍完特性，下面开始一步步集成Spring Session ##使用Redis集成Spring Session 引入依赖，Spring Boot的版本采用1.5.4 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 配置 配置类开启Redis Http Session 12345@Configuration@EnableRedisHttpSessionpublic class HttpSessionConfig &#123;&#125; 基本是0配置，只需要让主配置扫描到@EnableRedisHttpSession即可 配置文件application.yml，配置连接的redis信息 12345spring: redis: host: localhost port: 6379 database: 0 编写测试Controller，以便于观察Spring Session的特性，和前一篇文章使用同样的代码 12345678910111213141516171819202122@Controllerpublic class CookieController &#123; @RequestMapping(\"/test/cookie\") public String cookie(@RequestParam(\"browser\") String browser, HttpServletRequest request, HttpSession session) &#123; //取出session中的browser Object sessionBrowser = session.getAttribute(\"browser\"); if (sessionBrowser == null) &#123; System.out.println(\"不存在session，设置browser=\" + browser); session.setAttribute(\"browser\", browser); &#125; else &#123; System.out.println(\"存在session，browser=\" + sessionBrowser.toString()); &#125; Cookie[] cookies = request.getCookies(); if (cookies != null &amp;&amp; cookies.length &gt; 0) &#123; for (Cookie cookie : cookies) &#123; System.out.println(cookie.getName() + \" : \" + cookie.getValue()); &#125; &#125; return \"index\"; &#125;&#125; 启动类省略，下面开始测试。 在浏览器中访问如下端点：http://localhost:8080/test/cookie?browser=chrome，下面是连续访问4次的结果 12345671 不存在session，设置browser=chrome2 存在session，browser=chrome SESSION : 70791b17-83e1-42db-8894-73fbd2f2a1593 存在session，browser=chrome SESSION : 70791b17-83e1-42db-8894-73fbd2f2a1594 存在session，browser=chrome SESSION : 70791b17-83e1-42db-8894-73fbd2f2a159 如果还记得上一篇文章中运行结果的话，会发现和原生的session管理是有一些差别，原先的信息中我们记得Cookie中记录的Key值是JSESSIONID，而替换成RedisHttpSession之后变成了SESSION。接着观察redis中的变化： 解析一下这个redis store，如果不纠结于细节，可以跳过，不影响使用。 ​1 spring:session是默认的Redis HttpSession前缀（redis中，我们常用’:’作为分割符）。 2 每一个session都会有三个相关的key，第三个key最为重要，它是一个HASH数据结构，将内存中的session信息序列化到了redis中。如上文的browser，就被记录为sessionAttr:browser=chrome,还有一些meta信息，如创建时间，最后访问时间等。 3 另外两个key，expirations:1504446540000和sessions:expires:7079…我发现大多数的文章都没有对其分析，前者是一个SET类型，后者是一个STRING类型，可能会有读者发出这样的疑问，redis自身就有过期时间的设置方式TTL，为什么要额外添加两个key来维持session过期的特性呢？这需要对redis有一定深入的了解才能想到这层设计。当然这不是本节的重点，简单提一下：redis清除过期key的行为是一个异步行为且是一个低优先级的行为，用文档中的原话来说便是，可能会导致session不被清除。于是引入了专门的expiresKey，来专门负责session的清除，包括我们自己在使用redis时也需要关注这一点。在开发层面，我们仅仅需要关注第三个key就行了。 总结本节主要讲解了Spring Boot如何集成Spring Session，下一节将介绍更加复杂的特性。包括自定义Cookie序列化策略，与Spring Security的集成，根据用户名查找session等特性以及使用注意点。","categories":[{"name":"Spring Session","slug":"Spring-Session","permalink":"http://lexburner.github.io/categories/Spring-Session/"}],"tags":[{"name":"Spring Session","slug":"Spring-Session","permalink":"http://lexburner.github.io/tags/Spring-Session/"},{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"}]},{"title":"Re:从零开始的Spring Session(一)","slug":"Re：从零开始的Spring Session(一)","date":"2017-09-03T07:27:04.000Z","updated":"2017-09-04T01:51:48.012Z","comments":true,"path":"2017/09/03/Re：从零开始的Spring Session(一)/","link":"","permalink":"http://lexburner.github.io/2017/09/03/Re：从零开始的Spring Session(一)/","excerpt":"Session和Cookie这两个概念，在学习java web开发之初，大多数人就已经接触过了。最近在研究跨域单点登录的实现时，发现对于Session和Cookie的了解，并不是很深入，所以打算写两篇文章记录一下自己的理解。在我们的应用集成Spring Session之前，先补充一点Session和Cookie的关键知识。 Session与Cookie基础由于http协议是无状态的协议，为了能够记住请求的状态，于是引入了Session和Cookie的机制。我们应该有一个很明确的概念，那就是Session是存在于服务器端的，在单体式应用中，他是由tomcat管理的，存在于tomcat的内存中，当我们为了解决分布式场景中的session共享问题时，引入了redis，其共享内存，以及支持key自动过期的特性，非常契合session的特性，我们在企业开发中最常用的也就是这种模式。但是只要你愿意，也可以选择存储在JDBC，Mongo中，这些，spring都提供了默认的实现，在大多数情况下，我们只需要引入配置即可。而Cookie则是存在于客户端，更方便理解的说法，可以说存在于浏览器。Cookie并不常用，至少在我不长的web开发生涯中，并没有什么场景需要我过多的关注Cookie。http协议允许从服务器返回Response时携带一些Cookie，并且同一个域下对Cookie的数量有所限制，之前说过Session的持久化依赖于服务端的策略，而Cookie的持久化则是依赖于本地文件。虽然说Cookie并不常用，但是有一类特殊的Cookie却是我们需要额外关注的，那便是与Session相关的sessionId，他是真正维系客户端和服务端的桥梁。","text":"Session和Cookie这两个概念，在学习java web开发之初，大多数人就已经接触过了。最近在研究跨域单点登录的实现时，发现对于Session和Cookie的了解，并不是很深入，所以打算写两篇文章记录一下自己的理解。在我们的应用集成Spring Session之前，先补充一点Session和Cookie的关键知识。 Session与Cookie基础由于http协议是无状态的协议，为了能够记住请求的状态，于是引入了Session和Cookie的机制。我们应该有一个很明确的概念，那就是Session是存在于服务器端的，在单体式应用中，他是由tomcat管理的，存在于tomcat的内存中，当我们为了解决分布式场景中的session共享问题时，引入了redis，其共享内存，以及支持key自动过期的特性，非常契合session的特性，我们在企业开发中最常用的也就是这种模式。但是只要你愿意，也可以选择存储在JDBC，Mongo中，这些，spring都提供了默认的实现，在大多数情况下，我们只需要引入配置即可。而Cookie则是存在于客户端，更方便理解的说法，可以说存在于浏览器。Cookie并不常用，至少在我不长的web开发生涯中，并没有什么场景需要我过多的关注Cookie。http协议允许从服务器返回Response时携带一些Cookie，并且同一个域下对Cookie的数量有所限制，之前说过Session的持久化依赖于服务端的策略，而Cookie的持久化则是依赖于本地文件。虽然说Cookie并不常用，但是有一类特殊的Cookie却是我们需要额外关注的，那便是与Session相关的sessionId，他是真正维系客户端和服务端的桥梁。 代码示例用户发起请求，服务器响应请求，并做一些用户信息的处理，随后返回响应给用户；用户再次发起请求，携带sessionId，服务器便能够识别，这个用户就是之前请求的那个。 使用Springboot编写一个非常简单的服务端，来加深对其的理解。需求很简单，当浏览器访问localhost:8080/test/cookie?browser=xxx时，如果没有获取到session，则将request中的browser存入session；如果获取到session，便将session中的browser值输出。顺便将request中的所有cookie打印出来。 12345678910111213141516171819202122@Controllerpublic class CookieController &#123; @RequestMapping(\"/test/cookie\") public String cookie(@RequestParam(\"browser\") String browser, HttpServletRequest request, HttpSession session) &#123; //取出session中的browser Object sessionBrowser = session.getAttribute(\"browser\"); if (sessionBrowser == null) &#123; System.out.println(\"不存在session，设置browser=\" + browser); session.setAttribute(\"browser\", browser); &#125; else &#123; System.out.println(\"存在session，browser=\" + sessionBrowser.toString()); &#125; Cookie[] cookies = request.getCookies(); if (cookies != null &amp;&amp; cookies.length &gt; 0) &#123; for (Cookie cookie : cookies) &#123; System.out.println(cookie.getName() + \" : \" + cookie.getValue()); &#125; &#125; return \"index\"; &#125;&#125; 我们没有引入其他任何依赖，看看原生的session机制是什么。 1 使用chrome浏览器，访问localhost:8080/test/cookie?browser=chrome,控制台输出如下： 1Session Info: 不存在session，设置browser=chrome 既没有session，也没有cookie，我们将browser=chrome设置到session中。 再次访问同样的端点，控制台输出如下： 12Session Info: 存在session，browser=chromeCookie Info: JSESSIONID : 4CD1D96E04FC390EA6C60E8C40A636AF 多次访问之后，控制台依旧打印出同样的信息。 稍微解读下这个现象，可以验证一些结论。当服务端往session中保存一些数据时，Response中自动添加了一个Cookie：JSESSIONID：xxxx,再后续的请求中，浏览器也是自动的带上了这个Cookie，服务端根据Cookie中的JSESSIONID取到了对应的session。这验证了一开始的说法，客户端服务端是通过JSESSIONID进行交互的，并且，添加和携带key为JSESSIONID的Cookie都是tomcat和浏览器自动帮助我们完成的，这很关键。 2 使用360浏览器，访问localhost:8080/test/cookie?browser=360 第一次访问： 1Session Info: 不存在session，设置browser=360 后续访问： 12Session Info: 存在session，browser=360Cookie Info: JSESSIONID : 320C21A645A160C4843D076204DA2F40 为什么要再次使用另一个浏览器访问呢？先卖个关子，我们最起码可以得出结论，不同浏览器，访问是隔离的，甚至重新打开同一个浏览器，JSESSIONID也是不同的。另外可以尝试把保存session的操作注视掉，则可以发现Response中就不会返回JSESSIONID了，即这是一次无状态的请求。 安全问题其实上述的知识点，都是非常浅显的，之所以啰嗦一句，是为了引出这一节的内容，以及方便观察后续我们引入Spring Session之后的发生的变化。 还记得上一节的代码示例中，我们使用了两个浏览器： chrome浏览器访问时，JSESSIONID为4CD1D96E04FC390EA6C60E8C40A636AF，后端session记录的值为：browser=chrome。 360浏览器访问时，JSESSIONID为320C21A645A160C4843D076204DA2F40,后端session记录的值为：browser=360。 我们使用chrome插件Edit this Cookie，将chrome浏览器中的JSESSIONID修改为360浏览器中的值 同样访问原来的端点：localhost:8080/test/cookie?browser=chrome，得到的输出如下： 12存在session，browser=360JSESSIONID : 320C21A645A160C4843D076204DA2F40 证实了一点，存放在客户端的Cookie的确是存在安全问题的，我们使用360的JSESSIONID“骗”过了服务器。毕竟，服务器只能通过Cookie中的JSESSIONID来辨别身份。（这提示我们不要在公共场合保存Cookie信息，现在的浏览器在保存Cookie时通常会让你确定一次） 下一篇文章，将正式讲解如何在应用中集成Spring Session。","categories":[{"name":"Spring Session","slug":"Spring-Session","permalink":"http://lexburner.github.io/categories/Spring-Session/"}],"tags":[{"name":"Spring Session","slug":"Spring-Session","permalink":"http://lexburner.github.io/tags/Spring-Session/"},{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"}]},{"title":"解析Spring中的ResponseBody和RequestBody","slug":"解析Spring中的ResponseBody和RequestBody","date":"2017-08-30T04:44:21.000Z","updated":"2017-08-31T01:32:09.921Z","comments":true,"path":"2017/08/30/解析Spring中的ResponseBody和RequestBody/","link":"","permalink":"http://lexburner.github.io/2017/08/30/解析Spring中的ResponseBody和RequestBody/","excerpt":"","text":"spring，restful，前后端分离这些关键词都是大家耳熟能详的关键词了，一般spring常常需要与前端、第三方使用JSON，XML等形式进行交互，你也一定不会对@RequestBody和@ResponseBody这两个注解感到陌生。 @ResponseBody的使用由于@ResponseBody和@RequestBody的内部实现是同样的原理（封装请求和封装响应），所以本文以@ResponseBody为主要入手点，理解清楚任何一者，都可以同时掌握另一者。 如果想要从spring获得一个json形式返回值，操作起来是非常容易的。首先定义一个实体类: 1234public class Book &#123; private Integer id; private String bookName;&#125; 接着定义一个后端端点： 123456789@RestControllerpublic class BookController &#123; @GetMapping(value = \"/book/&#123;bookId&#125;\") public Book getBook(@PathVariable(\"bookId\") Integer bookId) &#123; return new Book(bookId, \"book\" + bookId); &#125;&#125; 在RestController中，相当于给所有的xxxMapping端点都添加了@ResponseBody注解，不返回视图，只返回数据。使用http工具访问这个后端端点localhost:8080/book/2，便可以得到如下的响应： 1234&#123; \"id\": 2, \"bookName\": \"book2\"&#125; 这是一个最简单的返回JSON对象的使用示例了，相信这样的代码很多人在项目中都写过。 添加XML解析如果我们需要将Book对象以XML的形式返回，该如何操作呢？这也很简单，给Book对象添加@XmlRootElement注解，让spring内部能够解析XML对象。 12345@XmlRootElementpublic class Book &#123; private Integer id; private String bookName;&#125; 在我们未对web层的BookController做任何改动之前，尝试访问localhost:8080/book/2时，会发现得到的结果仍然是前面的JSON对象。这也能够理解，因为Book对象如今既可以被解析为XML，也可以被解析为JSON，我们隐隐察觉这背后有一定的解析顺序关系，但不着急，先看看如何让RestController返回XML解析结果。 方法1 http客户端指定接收的返回结果类型 http协议中，可以给请求头添加Accept属性，笔者常用的http客户端是idea自带的Test RESTful Web Service以及chrome的插件Postman。简单的调试，前者基本可以满足我们大多数的需求，而这里为了给大家更直观的体验，笔者使用了Postman。以code形式展示： 123GET /book/2 HTTP/1.1Host: localhost:8080Accept: application/xml 响应内容如下： 12345&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;&lt;book&gt; &lt;bookName&gt;book2&lt;/bookName&gt; &lt;id&gt;2&lt;/id&gt;&lt;/book&gt; 方法2 在RestController后端端点中指定返回类型 修改后的RestController如下所示 123456789@RestControllerpublic class BookController &#123; @GetMapping(value = \"/book/&#123;bookId&#125;\", produces = &#123;\"application/xml\"&#125;) public Book getBook(@PathVariable(\"bookId\") Integer bookId) &#123; return new Book(bookId, \"book\" + bookId); &#125;&#125; 此时即使将请求中的Accept: application/xml去除，依旧可以返回上述的XML结果。 通常情况下，我们的服务端返回的形式一般是固定的，即限定了是JSON，XML中的一种，不建议依赖于客户端添加Accept的信息，而是在服务端限定produces类型。 详解Accpect与producesAccpect包含在http协议的请求头中，其本身代表着客户端发起请求时，期望返回的响应结果的媒体类型。如果服务端可能返回多个媒体类型，则可以通过Accpect指定具体的类型。 produces是Spring为我们提供的注解参数，代表着服务端能够支持返回的媒体类型，我们注意到produces后跟随的是一个数组类型，也就意味着服务端支持多种媒体类型的响应。 在上一节中，我们未显示指定produces值时，其实就隐式的表明，支持XML形式，JSON形式的媒体类型响应。从实验结果，我们也可以看出，当请求未指定Accpect，响应未指定produces时，具体采用何种形式返回是有Spring控制的。在接口交互时，最良好的对接方式，当然是客户端指定Accpect，服务端指定produces，这样可以避免模棱两可的请求响应，避免出现意想不到的对接结果。 详解ContentType与consumes恰恰和Accpect&amp;produces相反，这两个参数是与用于限制请求的。理解了前两者的含义，这两个参数可以举一反三理解清楚。 ContentType包含在http协议的请求头中，其本身代表着客户端发起请求时，告知服务端自己的请求媒体类型是什么。 consumes是Spring为我们提供的注解参数，代表着服务端能够支持处理的请求媒体类型，同样是一个数组，意味着服务端支持多种媒体类型的请求。一般而言，consumes与produces对请求响应媒体类型起到的限制作用，我们给他一个专有名词：窄化。 http请求响应媒体类型一览上面描述的4个属性：Accpect与produces，ContentType与consumes究竟有哪些类型与之对应呢？我只将常用的一些列举了出来： 媒体类型 含义 text/html HTML格式 text/plain 纯文本格式 text/xml, application/xml XML数据格式 application/json JSON数据格式 image/gif gif图片格式 image/png png图片格式 application/octet-stream 二进制流数据 application/ x-www-form-urlencoded form表单数据 multipart/form-data 含文件的form表单 其中有几个类型值得一说，web开发中我们常用的提交表单操作，其默认的媒体类型就是application/ x-www-form-urlencoded，而当表单中包含文件时，大家估计都踩过坑，需要将enctype=multipart/form-data设置在form参数中。text/html也就是常见的网页了，json与xml常用于数据交互，其他不再赘述。 而在JAVA中，提供了MediaType这样的抽象，来与http的媒体类型进行对应。‘/’之前的名词，如text，application被称为类型（type），‘/’之后被称为子类型(subType)。 详解HttpMessageConverter我们想要搞懂Spring到底如何完成众多实体类等复杂类型的数据转换以及与媒体类型的对应，就必须要搞懂HttpMessageConverter这个顶级接口： 1234567891011public interface HttpMessageConverter&lt;T&gt; &#123; boolean canRead(Class&lt;?&gt; var1, MediaType var2); boolean canWrite(Class&lt;?&gt; var1, MediaType var2); List&lt;MediaType&gt; getSupportedMediaTypes(); T read(Class&lt;? extends T&gt; var1, HttpInputMessage var2) throws IOException, HttpMessageNotReadableException; void write(T var1, MediaType var2, HttpOutputMessage var3) throws IOException, HttpMessageNotWritableException;&#125; 大致能看出Spring的处理思路。下面的流程图可以更好方便我们的理解： 对于添加了@RequestBody和@ResponseBody注解的后端端点，都会经历由HttpMessageConverter进行的数据转换的过程。而在Spring启动之初，就已经有一些默认的转换器被注册了。通过在RequestResponseBodyMethodProcessor 中打断点，我们可以获取到一个converters列表： 源码方面不做过多的解读，有兴趣的朋友可以研究一下RequestResponseBodyMethodProcessor 中的handleReturnValue方法，包含了转换的核心实现。 自定义HttpMessageConverter前面已经提及了消息转换器是通过判断媒体类型来调用响应的转换类的，不禁引发了我们的思考，如果我们遇到了不常用的MediaType，或者自定义的MediaType，又想要使用Spring的@RequestBody，@ResponseBody注解，该如何添加代码呢？下面我们通过自定义一个HttpMessageConverter来了解Spring内部的转换过程。 先定义我们的需求，自定一个MediaType：application/toString，当返回一个带有@ResponseBody注解的实体类时，将该实体类的ToString作为响应内容。 1 首先重写Book的ToString方法，方便后期效果展示 1234567@Overridepublic String toString() &#123; return \"~~~Book&#123;\" + \"id=\" + id + \", bookName='\" + bookName + '\\'' + \"&#125;~~~\";&#125; 2 编写自定义的消息转换器 123456789101112131415161718192021222324public class ToStringHttpMessageConverter extends AbstractHttpMessageConverter&lt;Object&gt; &#123; public ToStringHttpMessageConverter() &#123; super(new MediaType(\"application\", \"toString\", Charset.forName(\"UTF-8\")));// &lt;1&gt; &#125; @Override protected boolean supports(Class&lt;?&gt; clazz) &#123; return true; &#125; //从请求体封装数据 对应RequestBody 用String接收 @Override protected Object readInternal(Class&lt;?&gt; clazz, HttpInputMessage inputMessage) throws IOException, HttpMessageNotReadableException &#123; return StreamUtils.copyToString(inputMessage.getBody(), Charset.forName(\"UTF-8\")); &#125; //从响应体封装数据 对应ResponseBody @Override protected void writeInternal(Object o, HttpOutputMessage outputMessage) throws IOException, HttpMessageNotWritableException &#123; String result = o.toString();//&lt;2&gt; outputMessage.getBody().write(result.getBytes()); &#125;&#125; 此处指定了支持的媒体类型 调用类的ToString方法，将结果写入到输出流中 3 配置自定义的消息转换器 12345678@Configurationpublic class WebMvcConfig extends WebMvcConfigurerAdapter&#123; @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; converters.add(new ToStringHttpMessageConverter()); &#125;&#125; 4 配置后端端点，指定生产类型 12345678@RestControllerpublic class BookController &#123; @GetMapping(value = \"/book/&#123;bookId&#125;\",produces = &#123;\"application/toString\",\"application/json\",\"application/xml\"&#125;) public Book getBook(@PathVariable(\"bookId\") Integer bookId) &#123; return new Book(bookId, \"book\" + bookId); &#125;&#125; 此处只是为了演示，添加了三个生产类型，我们的后端端点可以支持输出三种类型，而具体输出哪一者，则依赖客户端的Accept指定。 5 客户端请求 123GET /book/2 HTTP/1.1Host: localhost:8080Accept: application/toString 响应结果如下： 1​~~~Book&#123;id=2, bookName=&apos;book2&apos;&#125;~~~ 此时，你可以任意指定Accept的类型，即可获得不同形式的Book返回结果，可以是application/toString，application/json，application/xml，都会对应各自的HttpMessageConverter。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"}]},{"title":"XML与javabean的转换","slug":"xml与javabean的转换","date":"2017-08-25T19:41:27.000Z","updated":"2017-09-04T01:37:18.614Z","comments":true,"path":"2017/08/26/xml与javabean的转换/","link":"","permalink":"http://lexburner.github.io/2017/08/26/xml与javabean的转换/","excerpt":"XML可以说是一种被时代淘汰的数据传输格式，毕竟相比较JSON，其语法，表现形式，以及第三方类库的支持，都要略逊一筹，但最近在对接一些老接口时，主要还是以XML为主，而翻阅相关的文档以及博客，没看到很好的文章介绍如何使用xml进行数据传输，所以简单写下此文，做一下记录。内心多多少少还是会抵制对接如此老旧的接口，不过生活还是要继续。 Code First先上一段代码，展示一下如何封装，讲解放到后面 一个典型的对接方提供的XML如下： 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;&lt;ORDER&gt; &lt;ORDER_NO&gt;10086&lt;/ORDER_NO&gt; &lt;TOTAL_PRICE&gt;3.14&lt;/TOTAL_PRICE&gt; &lt;CREATE_TIME&gt;2017-08-26 03:39:30&lt;/CREATE_TIME&gt; &lt;ORDER_ITEMS&gt; &lt;ORDER_ITEM&gt; &lt;GOODS_NAME&gt;德芙&lt;/GOODS_NAME&gt; &lt;NUM&gt;3&lt;/NUM&gt; &lt;/ORDER_ITEM&gt; &lt;ORDER_ITEM&gt; &lt;GOODS_NAME&gt;旺仔&lt;/GOODS_NAME&gt; &lt;NUM&gt;10&lt;/NUM&gt; &lt;/ORDER_ITEM&gt; &lt;/ORDER_ITEMS&gt;&lt;/ORDER&gt; 而我们要对应的实体类，则应当如下： 12345678910111213141516171819@XmlRootElement(name = \"ORDER\")// &lt;1&gt;@XmlAccessorType(XmlAccessType.FIELD)// &lt;1&gt;public class Order &#123; @XmlElement(name = \"ORDER_NO\")// &lt;1&gt; private String orderNo; @XmlElement(name = \"TOTAL_PRICE\") private BigDecimal totalPrice; @XmlElement(name = \"CREATE_TIME\") @XmlJavaTypeAdapter(DateAdapter.class) // &lt;2&gt; private Date createTime; @XmlElementWrapper(name = \"ORDER_ITEMS\") // &lt;3&gt; @XmlElement(name = \"ORDER_ITEM\") private List&lt;OrderItem&gt; orderItems;&#125; 12345678910@XmlAccessorType(XmlAccessType.FIELD)public class OrderItem &#123; @XmlElement(name = \"GOODS_NAME\") private String goodsName; @XmlElement(name = \"NUM\") private Integer num;&#125; 我举的这个示例基本包含一般情况下所有可能出现的需求 常用注解XmlRootElement，XmlAccessorType，XmlElement 日期转换的适配器注解 如何在XML中设置集合 在介绍这三点之前，先给出转换的工具类","text":"XML可以说是一种被时代淘汰的数据传输格式，毕竟相比较JSON，其语法，表现形式，以及第三方类库的支持，都要略逊一筹，但最近在对接一些老接口时，主要还是以XML为主，而翻阅相关的文档以及博客，没看到很好的文章介绍如何使用xml进行数据传输，所以简单写下此文，做一下记录。内心多多少少还是会抵制对接如此老旧的接口，不过生活还是要继续。 Code First先上一段代码，展示一下如何封装，讲解放到后面 一个典型的对接方提供的XML如下： 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;&lt;ORDER&gt; &lt;ORDER_NO&gt;10086&lt;/ORDER_NO&gt; &lt;TOTAL_PRICE&gt;3.14&lt;/TOTAL_PRICE&gt; &lt;CREATE_TIME&gt;2017-08-26 03:39:30&lt;/CREATE_TIME&gt; &lt;ORDER_ITEMS&gt; &lt;ORDER_ITEM&gt; &lt;GOODS_NAME&gt;德芙&lt;/GOODS_NAME&gt; &lt;NUM&gt;3&lt;/NUM&gt; &lt;/ORDER_ITEM&gt; &lt;ORDER_ITEM&gt; &lt;GOODS_NAME&gt;旺仔&lt;/GOODS_NAME&gt; &lt;NUM&gt;10&lt;/NUM&gt; &lt;/ORDER_ITEM&gt; &lt;/ORDER_ITEMS&gt;&lt;/ORDER&gt; 而我们要对应的实体类，则应当如下： 12345678910111213141516171819@XmlRootElement(name = \"ORDER\")// &lt;1&gt;@XmlAccessorType(XmlAccessType.FIELD)// &lt;1&gt;public class Order &#123; @XmlElement(name = \"ORDER_NO\")// &lt;1&gt; private String orderNo; @XmlElement(name = \"TOTAL_PRICE\") private BigDecimal totalPrice; @XmlElement(name = \"CREATE_TIME\") @XmlJavaTypeAdapter(DateAdapter.class) // &lt;2&gt; private Date createTime; @XmlElementWrapper(name = \"ORDER_ITEMS\") // &lt;3&gt; @XmlElement(name = \"ORDER_ITEM\") private List&lt;OrderItem&gt; orderItems;&#125; 12345678910@XmlAccessorType(XmlAccessType.FIELD)public class OrderItem &#123; @XmlElement(name = \"GOODS_NAME\") private String goodsName; @XmlElement(name = \"NUM\") private Integer num;&#125; 我举的这个示例基本包含一般情况下所有可能出现的需求 常用注解XmlRootElement，XmlAccessorType，XmlElement 日期转换的适配器注解 如何在XML中设置集合 在介绍这三点之前，先给出转换的工具类 转换工具类1234567891011121314151617181920212223242526272829public class XML &#123; public static String toXmlString(Object obj) &#123; String result; try &#123; JAXBContext context = JAXBContext.newInstance(obj.getClass()); Marshaller marshaller = context.createMarshaller(); StringWriter writer = new StringWriter(); marshaller.marshal(obj, writer); result = writer.toString(); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; return result; &#125; public static &lt;T&gt; T parseObject(String input, Class&lt;T&gt; claaz) &#123; Object result; try &#123; JAXBContext context = JAXBContext.newInstance(claaz); Unmarshaller unmarshaller = context.createUnmarshaller(); result = unmarshaller.unmarshal(new StringReader(input)); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; return (T) result; &#125;&#125; JSON工具类中，笔者习惯于使用fastjson，所以干脆连同工具类类名命名和方法命名都按照了它的风格，只有两个方法。 注解的介绍给实体类加上注解，再使用工具类，就可以实现实体和XML的相互转换了。那么前面提到的三个注意点中的相关注解分别代表了什么含义呢？ @XmlRootElement 作用域：类 代表一个XML对象的根节点，常使用name属性来可以指定生成XML之后的具体名称 @XmlElement 作用域：字段，方法，参数（不常用） 代表一个XML对象的普通界点信息，常使用name属性来指定生成XML之后的具体名称。需要注意与@XmlAccessorType搭配使用时，有一些注意点，见下 @XmlAccessorType 作用域：类，包（不常用） 告诉解析器，在解析XML时要如何获取类的字段属性，有4个枚举类型： | 枚举类型 | 访问方式 || ——————————- | ——————————- || XmlAccessType.FIELD | 成员变量 || XmlAccessType.PROPERTY | public getter,setter || XmlAccessType.PUBLIC_MEMBER（默认） | public getter,setter+public成员变量 || XmlAccessType.NONE | 必须显示指定@XmlElement | 我们上述的例子中，使用的方式是在类上配置@XmlAccessorType(XmlAccessType.FIELD)，基于成员变量访问属性，并且，在每一个成员变量之上都显示指定了name=xxx；而如果配置@XmlAccessorType(XmlAccessType.PUBLIC_MEMBER)即默认配置，则你需要将@XmlElement注解写在getter方法上,笔者比较习惯例子中的写法。需要注意点的一点是，如果@XmlAccessorType与@XmlElement的配置不对应，很容易触发自动的转换方式，会导致某个节点出现两次的异常。 @XmlJavaTypeAdapter 作用域：字段,方法,类,包,参数（前三者常用） java内置的xml日期转换类不能满足我们的需求（可以动手试试看默认日期的格式是什么），以及遇到自定义的类，需要配置转换器，就可以使用这个注解，@XmlJavaTypeAdapter注解接收一个自定义的Adapter，需要继承自XmlAdapter&lt;ValueType,BoundType&gt;抽象类，一个常用的日期转化适配器如下： 1234567891011121314151617181920212223public class DateAdapter extends XmlAdapter&lt;String, Date&gt; &#123; static ThreadLocal&lt;DateFormat&gt; sdf ; static &#123; sdf =new ThreadLocal&lt;DateFormat&gt;() &#123; @Override protected DateFormat initialValue() &#123; return new SimpleDateFormat(\"yyyy-MM-dd hh:mm:ss\"); &#125; &#125;; &#125; @Override public Date unmarshal(String v) throws Exception &#123; return sdf.get().parse(v); &#125; @Override public String marshal(Date v) throws Exception &#123; return sdf.get().format(v); &#125;&#125; 使用Adapter的弊端也很明显，一个适配器只能对应一个日期的格式，在实际开发中我们往往会将日期区分成天维度的日期和秒维度的日期，不能像大多数JSON那样拥有灵活的注解，如果有读者有想到好的解决方案，欢迎跟我沟通。涉及到日期格式转化，时刻不要忘记SimpleDateFormat线程不安全这一点。 @XmlElementWrapper XML中表示集合时，在最外层通常会有一个Xxxs或者XxxList这样的标签，可以通过@XmlElementWrapper实现，其中name就代表额外添加的包裹信息是什么,如上文的OrderItems。 一些其他的转换工具类我们主要任务是实现XML字符串和javabean之间转换，不是解析XML，所以dom4j一类的类库不用考虑。熟悉spring的人会了解到一点，spring其实已经封装了xml转换相关的类，即org.springframework.oxm.jaxb.Jaxb2Marshaller这个类，他的顶层接口是org.springframework.oxm.Marshaller和org.springframework.oxm.UnMarshaller。而在java规范中，也存在同名的接口：javax.xml.bind.Marshaller,javax.xml.bind.UnMarshaller，这点在使用中需要注意下。笔者的建议是，这种数据格式转换操作，应当尽量引入最少的依赖。所以使用javax的类库下的相关方法进行封装。上述的工具类，仅仅只需要引入javax包，即可使用了。非常方便、","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"XML","slug":"XML","permalink":"http://lexburner.github.io/tags/XML/"}]},{"title":"一些需要普及的项目规范","slug":"project_rules","date":"2017-08-25T04:18:45.000Z","updated":"2017-09-18T09:14:18.898Z","comments":true,"path":"2017/08/25/project_rules/","link":"","permalink":"http://lexburner.github.io/2017/08/25/project_rules/","excerpt":"介绍好的开发规范不仅能够使得项目变得易维护，易升级。一些通用的规范可以参考《阿里巴巴java开发手册》本文档主要针对我们项目内部正在使用的框架提出一些开发规范，欢迎补充 包结构规范以短信邮件项目（mail-sms）为例，介绍包结构命名规范。","text":"介绍好的开发规范不仅能够使得项目变得易维护，易升级。一些通用的规范可以参考《阿里巴巴java开发手册》本文档主要针对我们项目内部正在使用的框架提出一些开发规范，欢迎补充 包结构规范以短信邮件项目（mail-sms）为例，介绍包结构命名规范。 短信邮件项目主要包含短信，邮件两个子模块【强制】 包分层–通用格式如下：公司名.模块名.层次名包名应当尽量使用能够概括模块总体含义,单词义,单数,不包含特殊字符的单词【正例】: sinosoftgz.message.admin【反例】: sinosoftgz.mailsms.admin sinosoftgz.mail.sms.admin【推荐】包分层–业务当项目模块的职责较为复杂，且考虑到以后拓展的情况下，单个模块依旧包含着很多小的业务模块时，应当优先按照业务区分包名【正例】:12345678910111213141516171819202122232425262728293031sinosoftgz.message.admin config 模块公用Config.java service 模块公用Service.java web 模块公用Controller.java IndexController.java mail config MailConfig.java service Mail私有Service.java MailTemplateService.java MailMessageService.java web Mail私有Controller.java MailTemplateController.java MailMessageController.java sms config Smsconfig.java service Sms私有Service.java SmsTemplateService.java SmsMessageService.java web Sms私有Controller.java SmsTemplateController.java SmsMessageController.java MessageAdminApp.java 【反例】:12345678910111213141516171819202122232425sinosoftgz.message.admin config 模块公用Config.java service 模块公用Service.java mail Mail私有Service.java MailTemplateService.java MailMessageService.java sms Sms私有Service.java SmsTemplateService.java SmsMessageService.java web 模块公用Controller.java IndexController.java mail Mail私有Controller.java MailTemplateController.java MailMessageController.java sms Sms私有Controller.java SmsTemplateController.java SmsMessageController.java MailSmsAdminApp.java service和controller以及其他业务模块相关的包相隔太远，或者干脆全部丢到一个包内，单纯用前缀区分，会形成臃肿，充血的包结构。如果是项目结构较为单一，可以仅仅使用前缀区分；如果是项目中业务模块有明显的区分条件，应当单独作为一个包，用包名代表业务模块的含义。 数据库规范【强制】必要的地方必须添加索引，如唯一索引，以及作为条件查询的列【强制】生产环境，uat环境，不允许使用jpa.hibernate.ddl-auto: create自动建表，每次ddl的修改需要保留脚本，统一管理【强制】业务数据不能使用deleteBy…而要使用逻辑删除setDeleteFlag(true),查询时，findByxxxAndDeleteFlag(xxx,false) ORM规范【强制】条件查询超过三个参数的，使用criteriaQuery，predicates 而不能使用springdata的findBy【正例】123456789101112131415161718192021222324252627public Page&lt;MailTemplateConfig&gt; findAll(MailTemplateConfig mailTemplateConfig, Pageable pageable) &#123; Specification querySpecification = (Specification&lt;MailTemplateConfig&gt;) (root, criteriaQuery, criteriaBuilder) -&gt; &#123; List&lt;Predicate&gt; predicates = new ArrayList&lt;&gt;(); predicates.add(criteriaBuilder.isFalse(root.get(\"deleteFlag\"))); //级联查询mailTemplate if (!Lang.isEmpty(mailTemplateConfig.getMailTemplate())) &#123; //短信模板名称 if (!Lang.isEmpty(mailTemplateConfig.getMailTemplate().getTemplateName())) &#123; predicates.add(criteriaBuilder.like(root.join(\"mailTemplate\").get(\"templateName\"), String.format(\"%%%s%%\", mailTemplateConfig.getMailTemplate().getTemplateName()))); &#125; //短信模板类型 if (!Lang.isEmpty(mailTemplateConfig.getMailTemplate().getTemplateType())) &#123; predicates.add(criteriaBuilder.equal(root.join(\"mailTemplate\").get(\"templateType\"), mailTemplateConfig.getMailTemplate().getTemplateType())); &#125; &#125; //产品分类 if (!Lang.isEmpty(mailTemplateConfig.getProductType())) &#123; predicates.add(criteriaBuilder.equal(root.get(\"productType\"), mailTemplateConfig.getProductType())); &#125; //客户类型 if (!Lang.isEmpty(mailTemplateConfig.getConsumerType())) &#123; predicates.add(criteriaBuilder.equal(root.get(\"consumerType\"), mailTemplateConfig.getConsumerType())); &#125; return criteriaBuilder.and(predicates.toArray(new Predicate[predicates.size()])); &#125;; return mailTemplateConfigRepos.findAll(querySpecification, pageable); &#125; 【说明】条件查询是admin模块不可避免的一个业务功能，使用criteriaQuery可以轻松的添加条件，使得代码容易维护，他也可以进行分页，排序，连表操作，充分发挥jpa面向对象的特性，使得业务开发变得快捷。【反例】 12345678910111213public Page&lt;GatewayApiDefine&gt; findAll(GatewayApiDefine gatewayApiDefine,Pageable pageable)&#123; if(Lang.isEmpty(gatewayApiDefine.getRole()))&#123; gatewayApiDefine.setRole(\"\"); &#125; if(Lang.isEmpty(gatewayApiDefine.getApiName()))&#123; gatewayApiDefine.setApiName(\"\"); &#125; if(Lang.isEmpty(gatewayApiDefine.getEnabled()))&#123; return gatewayApiDefineDao.findByRoleLikeAndApiNameLikeOrderByLastUpdatedDesc(\"%\"+gatewayApiDefine.getRole()+\"%\",\"%\"+gatewayApiDefine.getApiName()+\"%\",pageable); &#125;else&#123; return gatewayApiDefineDao.findByRoleLikeAndApiNameLikeAndEnabledOrderByLastUpdatedDesc(\"%\"+gatewayApiDefine.getRole()+\"%\",\"%\"+gatewayApiDefine.getApiName()+\"%\",gatewayApiDefine.getEnabled(),pageable); &#125; &#125; 【说明】在Dao层定义了大量的findBy方法，在Service写了过多的if else判断，导致业务逻辑不清晰 禁止使用魔鬼数字【模型层与业务层】【强制】一些固定业务含义的代码可以使用枚举类型，或者final static常量表示，在设值时，不能直接使用不具备业务含义的数值。【正例】：使用final static常量:1234567891011121314151617181920212223242526272829303132333435//实体类定义 /** * 发送设置标志 * * @see sendFlag */ public final static String SEND_FLAG_NOW = \"1\"; //立即发送 public final static String SEND_FLAG_DELAY = \"2\"; //预设时间发送 /** * 发送成功标志 * * @see sendSuccessFlag */ public final static Map&lt;String, String&gt; SEND_SUCCESS_FLAG_MAP = new LinkedHashMap&lt;&gt;(); public final static String SEND_WAIT = \"0\"; public final static String SEND_SUCCESS = \"1\"; public final static String SEND_FAIL = \"2\"; static &#123; SEND_SUCCESS_FLAG_MAP.put(SEND_WAIT, \"未发送\"); SEND_SUCCESS_FLAG_MAP.put(SEND_SUCCESS, \"发送成功\"); SEND_SUCCESS_FLAG_MAP.put(SEND_FAIL, \"发送失败\"); &#125; /** * 发送设置标志 (1：立即发送 2：预设时间发送 ) */ @Column(columnDefinition = \"varchar(1) comment '发送设置标志'\") protected String sendFlag;//业务代码赋值使用MailMessage mailMessage = new MailMessage();mailMessage.setSendSuccessFlag(MailMessage.SEND_WAIT);mailMessage.setValidStatus(MailMessage.VALID_WAIT);mailMessage.setCustom(true); 【反例】1234567891011//实体类定义/** * 发送设置标志 (1：立即发送 2：预设时间发送 ) */@Column(columnDefinition = \"varchar(1) comment '发送设置标志'\")protected String sendFlag;//业务代码赋值使用MailMessage mailMessage = new MailMessage();mailMessage.setSendSuccessFlag(\"1\");mailMessage.setValidStatus(\"0\");mailMessage.setCustom(true); 【说明】魔鬼数字不能使代码一眼能够看明白到底赋的是什么值，并且，实体类发生变化后，可能会导致赋值错误，与预期赋值不符合且错误不容易被发现。 【正例】：也可以使用枚举类型避免魔鬼数字 1234567891011121314151617181920212223242526protected String productType;protected String productName;@Enumerated(EnumType.STRING)protected ConsumerTypeEnum consumerType;@Enumerated(EnumType.STRING)protected PolicyTypeEnum policyType;@Enumerated(EnumType.STRING)protected ReceiverEnum receiver;public enum ConsumerTypeEnum &#123; PERSONAL, ORGANIZATION; public String getLabel() &#123; switch (this) &#123; case PERSONAL: return \"个人\"; case ORGANIZATION: return \"团体\"; default: return \"\"; &#125; &#125;&#125; 【视图层】【推荐】例如，页面迭代select的option，不应该在view层判断，而应该在后台传入map在前台迭代【正例】：12345678model.put(\"typeMap\",typeMap);模板类型：&lt;select type=\"text\" name=\"templateType\"&gt; &lt;option value=\"\"&gt;全部&lt;/option&gt; &lt;#list typeMap?keys as key&gt; &lt;option &lt;#if ((mailTemplate.templateType!\"\")==key)&gt;selected=\"selected\"&lt;/#if&gt;value=\"$&#123;key&#125;\"&gt;$&#123;typeMap[key]&#125;&lt;/option&gt; &lt;/#list&gt;&lt;/select&gt; 【反例】：12345678模板类型：&lt;select type=\"text\" name=\"templateType\"&gt; &lt;option value=\"\"&gt;全部&lt;/option&gt; &lt;option &lt;#if $&#123;xxx.templateType!&#125;==\"1\" selected=\"selected\"&lt;/#if&gt; value=\"1\"&gt;承保通知&lt;/option&gt; ... &lt;option &lt;#if $&#123;xxx.templateType!&#125;==\"5\" selected=\"selected\"&lt;/#if&gt; value=\"5\"&gt;核保通知&lt;/option&gt;&lt;/select&gt; 【说明】：否则修改后台代码后，前端页面也要修改，设计模式的原则，应当是修改一处，其他全部变化。且 1，2…,5的含义可能会变化，不能从页面得知value和option的含义是否对应。 并发注意事项项目中会出现很多并发问题，要做到根据业务选择合适的并发解决方案，避免线程安全问题 【强制】simpleDateFormat有并发问题，不能作为static类变量【反例】：这是我在某个项目模块中，发现的一段代码1234567891011Class XxxController&#123; public final static SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd hh:mm:ss\"); @RequestMapping(\"/xxxx\") public String xxxx(String dateStr)&#123; XxxEntity xxxEntity = new XxxEntity(); xxxEntity.setDate(simpleDateFormat.parse(dateStr)); xxxDao.save(xxxEntity); return \"xxx\"; &#125;&#125; 【说明】SimpleDateFormat 是线程不安全的类，不能作为静态类变量给多线程并发访问。如果不了解多线程，可以将其作为实例变量，每次使用时都new一个出来使用。不过更推荐使用ThreadLocal来维护，减少new的开销。【正例】一个使用ThreadLocal维护SimpleDateFormat的线程安全的日期转换类：1234567891011121314151617public class ConcurrentDateUtil &#123; private static ThreadLocal&lt;DateFormat&gt; threadLocal = new ThreadLocal&lt;DateFormat&gt;() &#123; @Override protected DateFormat initialValue() &#123; return new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); &#125; &#125;; public static Date parse(String dateStr) throws ParseException &#123; return threadLocal.get().parse(dateStr); &#125; public static String format(Date date) &#123; return threadLocal.get().format(date); &#125;&#125; 【推荐】名称唯一性校验出现的线程安全问题各个项目的admin模块在需求中经常会出现要求名称不能重复，即唯一性问题。通常在前台做ajax校验，后台使用select count(1) from table_name where name=?的方式查询数据库。这么做无可厚非，但是在极端的情况下，会出现并发问题。两个线程同时插入一条相同的name，如果没有做并发控制，会导致出现脏数据。如果仅仅是后台系统，那么没有必要加锁去避免，只需要对数据库加上唯一索引，并且再web层或者service层捕获数据异常即可。【正例】： 12345678910111213141516171819202122232425262728293031323334353637//实体类添加唯一索引@Entity@Table(name = \"mns_mail_template\", uniqueConstraints = &#123;@UniqueConstraint(columnNames = &#123;\"templateName\"&#125;)&#125;)public class MailTemplate extends AbstractTemplate &#123; /** * 模板名称 */ @Column(columnDefinition = \"varchar(160) comment '模板名称'\") private String templateName;&#125;//业务代码捕获异常@RequestMapping(value = &#123;\"/saveOrUpdate\"&#125;, method = RequestMethod.POST) @ResponseBody public AjaxResponseVo saveOrUpdate(MailTemplate mailTemplate) &#123; AjaxResponseVo ajaxResponseVo = new AjaxResponseVo(AjaxResponseVo.STATUS_CODE_SUCCESS, \"操作成功\", \"邮件模板定义\", AjaxResponseVo.CALLBACK_TYPE_CLOSE_CURRENT); try &#123; //管理端新增时初始化一些数据 if (Lang.isEmpty(mailTemplate.getId())) &#123; mailTemplate.setValidStatus(MailTemplate.VALID_WAIT); &#125; mailTemplateService.save(mailTemplate); &#125; catch (DataIntegrityViolationException ce) &#123; ajaxResponseVo.setStatusCode(AjaxResponseVo.STATUS_CODE_ERROR); ajaxResponseVo.setMessage(\"模板名称已经存在\"); ajaxResponseVo.setCallbackType(null); logger.error(ce.getMessage()); &#125; catch (Exception e) &#123; ajaxResponseVo.setStatusCode(AjaxResponseVo.STATUS_CODE_ERROR); ajaxResponseVo.setMessage(\"操作失败!\"); ajaxResponseVo.setCallbackType(null); logger.error(e.getMessage(), e); &#125; return ajaxResponseVo; &#125; 【说明】关于其他一些并发问题,如分布式锁，CAS，不仅仅是一篇文档能够讲解清楚的，需要对开发有很深的理解，我还记录了一些并发问题，仅供参考：http://blog.csdn.net/u013815546/article/details/56481842 moton使用注意事项【注意】包的扫描 每个模块都要扫描自身的项目结构12345678910mail-sms-admin:application.ymlmotan: client-group: sinosoftrpc client-access-log: false server-group: sinosoftrpc server-access-log: false export-port: $&#123;random.int[9001,9999]&#125; zookeeper-host: 127.0.0.1:2181 annotaiong-package: sinosoftgz.message.admin app模块由于将api-impl脱离出了自身的模块，通常还需要扫描api-impl的模块 配置pom.xml依赖 1234&lt;dependency&gt; &lt;groupId&gt;sinosoftgz&lt;/groupId&gt; &lt;artifactId&gt;mail-sms-api-impl&lt;/artifactId&gt;&lt;/dependency&gt; 配置spring ioc扫描 AutoImportConfig.java 123@ComponentScans(&#123; @ComponentScan(basePackages = &#123;\"sinosoftgz.message.app\", \"sinosoftgz.message.api\"&#125;)&#125;) 配置motan扫描 mail-sms-app:application.yml 12345678motan: annotaiong-package: sinosoftgz.message.app,sinosoftgz.message.api client-group: sinosoftrpc client-access-log: true server-group: sinosoftrpc server-access-log: true export-port: $&#123;random.int[9001,9999]&#125; zookeeper-host: localhost:2181 【注意】motan跨模块传输实体类时懒加载失效遇到的时候注意一下，由于jpa，hibernate懒加载的问题，因为其内部使用动态代理去实现的懒加载，导致懒加载对象无法被正确的跨模块传输，此时需要进行深拷贝。【正例】： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * 深拷贝OrderMain对象，主要用于防止Hibernate序列化懒加载Session关闭问题 * &lt;p/&gt; * // * @param order * * @return */ public OrderMain cpyOrder(OrderMain from, OrderMain to) &#123; OrderMain orderMainNew = to == null ? new OrderMain() : to; Copys copys = Copys.create(); List&lt;OrderItem&gt; orderItemList = new ArrayList&lt;&gt;(); List&lt;SubOrder&gt; subOrders = new ArrayList&lt;&gt;(); List&lt;OrderGift&gt; orderGifts = new ArrayList&lt;&gt;(); List&lt;OrderMainAttr&gt; orderMainAttrs = new ArrayList&lt;&gt;(); OrderItem orderItemTmp; SubOrder subOrderTmp; OrderGift orderGiftTmp; OrderMainAttr orderMainAttrTmp; copys.from(from).excludes(\"orderItems\", \"subOrders\", \"orderGifts\", \"orderAttrs\").to(orderMainNew).clear(); if (!Lang.isEmpty(from.getOrderItems())) &#123; for (OrderItem i : from.getOrderItems()) &#123; orderItemTmp = new OrderItem(); copys.from(i).excludes(\"order\").to(orderItemTmp).clear(); orderItemTmp.setOrder(orderMainNew); orderItemList.add(orderItemTmp); &#125; orderMainNew.setOrderItems(orderItemList); &#125; SubOrderItem subOrderItem; List&lt;SubOrderItem&gt; subOrderItemList = new ArrayList&lt;&gt;(); if (from.getSubOrders() != null) &#123; for (SubOrder s : from.getSubOrders()) &#123; subOrderTmp = new SubOrder(); copys.from(s).excludes(\"order\", \"subOrderItems\").to(subOrderTmp).clear(); subOrderTmp.setOrder(from); for (SubOrderItem soi : s.getSubOrderItems()) &#123; subOrderItem = new SubOrderItem(); copys.from(soi).excludes(\"order\", \"subOrder\", \"orderItem\").to(subOrderItem).clear(); subOrderItem.setOrder(orderMainNew); subOrderItem.setSubOrder(subOrderTmp); subOrderItemList.add(subOrderItem); if (!Lang.isEmpty(soi.getOrderItem())) &#123; for (OrderItem i : orderMainNew.getOrderItems()) &#123; if (i.getId().equals(soi.getOrderItem().getId())) &#123; subOrderItem.setOrderItem(soi.getOrderItem()); &#125; else &#123; subOrderItem.setOrderItem(soi.getOrderItem()); &#125; &#125; &#125; &#125; subOrderTmp.setSubOrderItems(subOrderItemList); subOrders.add(subOrderTmp); &#125; orderMainNew.setSubOrders(subOrders); &#125; if (from.getOrderGifts() != null) &#123; for (OrderGift og : from.getOrderGifts()) &#123; orderGiftTmp = new OrderGift(); copys.from(og).excludes(\"order\").to(orderGiftTmp).clear(); orderGiftTmp.setOrder(orderMainNew); orderGifts.add(orderGiftTmp); &#125; orderMainNew.setOrderGifts(orderGifts); &#125; if (from.getOrderAttrs() != null) &#123; for (OrderMainAttr attr : from.getOrderAttrs()) &#123; orderMainAttrTmp = new OrderMainAttr(); copys.from(attr).excludes(\"order\").to(orderMainAttrTmp).clear(); orderMainAttrTmp.setOrder(orderMainNew); orderMainAttrs.add(orderMainAttrTmp); &#125; orderMainNew.setOrderAttrs(orderMainAttrs); &#125; return orderMainNew; &#125; 公用常量规范【强制】模块常量模块自身公用的常量放置于模块的Constants 类中，以final static的方式声明1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class Constants &#123; public static final String birthdayPattern = \"yyyy-MM-dd\"; //生日格式 public static final String inputTimePattern = \"yyyy-MM-dd HH:mm:ss\"; //录入时间格式 public static class PolicyType &#123; public static final String personal = \"0\"; //个单 public static final String group = \"1\"; //团单 &#125; public static class InsuredNature &#123; public static final String naturePerson = \"1\"; //自然人 public static final String artificialPerson = \"0\"; //法人 &#125; public static class InsuredIdentity &#123; public static final String myself = \"0\"; //本人 &#125; public static class JfeeFlag &#123; public static final String noFeeFlag = \"0\"; //非见费标志 public static final String feeFlag = \"1\"; //见费标志 &#125; public static class ItemKindFlag &#123; public static final String mainRiskFlag = \"1\"; //主险标志 public static final String additionalRiskFlag = \"2\"; //附加险标志 public static final String otherRiskFlag = \"3\"; //其它标志 &#125; public static class CalculateAmountFlag &#123; public static final String calculateFlag = \"Y\"; //计算保额标志 public static final String noCalculateFlag = \"N\"; //不计算保额标志 &#125; public static class LimitGrade &#123; public static final String policyLevel = \"1\"; //限额/免赔保单级别 public static final String clauseLevel = \"2\"; //限额/免赔条款级别 &#125; /** * 批改类型 * * 命名规则：对象（可选）+行为 */ public static class EndorType &#123; public static final String collectivePolicyInsuredModify = \"22\"; //团单变更被保险人 public static final String collectivePolicyInsuredAdd = \"Z1\"; //团单批增被保险人 public static final String collectivePolicyInsuredRemove = \"J1\"; //团单批减被保险人 public static final String surrender = \"04\"; //全单退保 public static final String withdraw = \"05\"; //注销 public static final String insurancePeriodModify = \"06\"; //平移保险期限 public static final String applicantModify = \"H01\"; //更改投保人 public static final String customerModify = \"50\"; //变更客户信息 public static final String insuredModify = \"29\"; //变更被保人职业 public static final String individualPolicyBeneficiaryModify = \"03\"; //变更受益人信息 public static final String engageModify = \"15\"; //变更特别约定 public static final String individualPolicyInsuredModify = \"77\";//个单变更被保人 &#125;&#125; Constants类在一个限界上下文只能有一个，一个限界上下文包含了一整个业务模块（如policy-admin,policy-admin,policy-api,policy-model）构成一个限界上下文 在Constants类中使用静态内部类尽量细化到常量的归属，不要散放 【强制】项目常量项目公用的常量放置于util模块的GlobalContants类中，以内部类和final static的方式声明 1234567891011121314151617181920public abstract class GlobalContants &#123; /** * 返回的状态 */ public class ResponseStatus&#123; public static final String SUCCESS = \"success\";//成功 public static final String ERROR = \"error\";//错误 &#125; /** * 响应状态 */ public class ResponseString&#123; public static final String STATUS = \"status\";//状态 public static final String ERROR_CODE = \"error\";// 错误代码 public static final String MESSAGE = \"message\";//消息 public static final String DATA = \"data\";//数据 &#125; ...&#125; 领域模型规范javabean规范（一些像驼峰命名法之类通用的规范就不说了，强调一些可能会犯错的规范）【强制】BigDecimal规范【说明】业务实体类中的与金额相关的变量统一使用BigDecimal,四则运算采用BigDecimal的相关api进行，做除法时需要额外注意保留精度的问题，否则可能会报异常，并且不易被测试出【正例】12BigDecimal totalMoney = new BigDecimal(\"100.42\");BigDecimal averageMoney = totalMoney.divide(new BigDecimal(\"22\"),2); 【强制】布尔类型规范【说明】所有的布尔类型不允许以is开头，否则会导致部分序列化，hibernate框架出现解析异常。【反例】原来项目的BaseDomain中标记逻辑删除的字段,在部分场景下会出现问题123456789101112@Column(name = \"is_delete\")private Boolean isDelete = false;public Boolean getIsDelete() &#123; return isDelete; &#125;public void setIsDelete(Boolean isDelete) &#123; if(deleteFlag) this.deleteDate = new Date(); this.isDelete = isDelete;&#125; tips: 使用intellij idea的快捷键（for eclipse）alt+shift+r，或者菜单栏Refactor-&gt;Rename，可以重构字段名称【正例】12@Column(name = \"is_delete\")private Boolean deleteFlag = false; 【推荐】装箱类型优于原生类型在业务代码中，更加推荐使用装箱类型Integer Double Boolean…【说明】在未设值的情况下，原生类型具有默认值，而装箱类型为null以Boolean类型为例，如果使用boolean，那么在未复制时，无法得知其到底是被赋值成了false，还是为赋值 其他常用的领域模型首先理解各个常用的领域模型的含义： 领域模型 全称 中文含义 DO Domain Object 领域对象 DTO Data Transfer Object 数据传输对象 VO View Object 视图对象 对于View Object，PO等等其他一些的对象不在此做要求，只说明一下常用的几个DO就是我们最常用的数据库持久对象，是OOP对于现实中的抽象，一般使用orm框架映射到数据库DTO这一层，目前我们的项目还没有投入使用，即将考虑投入使用，理论上来说，两个微服务模块是严禁共享数据库的所以A模块要查询B模块的数据，需要使用B模块app层暴露出来的api来查询，其中B模块返回的实体，不能是直接从数据库中查询出来的DO，而应该是DO转换而成的DTO。以及其他服务服务用语传输的变量，都叫做DTOVO就是常存在于视图层模板渲染使用的实体类 tips：DO最特殊的一点在于，它拥有主键，而DTO不应该包含数据库的主键 【推荐】领域模型命名规范【说明】由于DO这一层大家已经养成了习惯，不做要求了。DTO有些特殊，他常常与业务的传输对象相关，而不限于以DTO结尾，如xxxQuery也可以是DTO对象。VO对象推荐以VO结尾。注意：不要命名为Vo,Dto。","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/categories/技术杂谈/"}],"tags":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/tags/技术杂谈/"},{"name":"代码规范","slug":"代码规范","permalink":"http://lexburner.github.io/tags/代码规范/"}]},{"title":"博客搬家","slug":"博客搬家","date":"2017-08-22T08:38:44.000Z","updated":"2017-08-23T05:09:40.250Z","comments":true,"path":"2017/08/22/博客搬家/","link":"","permalink":"http://lexburner.github.io/2017/08/22/博客搬家/","excerpt":"","text":"陆陆续续，写博客已经写了有4年多了，之前一直在CSDN维护博客（博客旧址），最近有了点空余时间，使用hexo搭了这个博客，的确比CSDN清爽多了，首先感谢@程序猿DD推荐的icarus模板，国人开发的一个hexo模板，插件支持可能不是很完善，但是样式非常让人喜欢。 作为一个前端弱渣，搭建博客的过程还是遇到了不少的困难。原先是打算直接使用github个人主页作为博客地址，hexo对git有很好的支持，源代码和博客静态页面都托管在了github，master分支放静态页面，hexo分支放源文件。可惜的是国内坑爹的网速,github.io的访问速度不尽如人意（github.com倒还好），于是在宇泽学妹@ntzyz的帮助下，搞了github的hook，本地提交到github时，代理服务器自动向master分支拉取页面，同时设置反向代理和https。由于hexo是静态文件搭建的博客，这种方式可以说是非常合适的。所以，国内的朋友浏览本博客可以直接访问https://www.cnkirito.moe，如果有国外代理的朋友可以直接访问我的github个人主页https://lexburner.github.io。 目前博客功能还不算完善，缺少评论，分享，和一些小插件，以后逐渐完善，不过不影响主要功能。以后这儿就作为我主要更新博客的地方了！","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/categories/技术杂谈/"}],"tags":[]},{"title":"一个DDD指导下的实体类设计案例","slug":"一个DDD指导下的实体类设计案例","date":"2017-08-21T07:59:52.000Z","updated":"2017-08-22T04:31:22.057Z","comments":true,"path":"2017/08/21/一个DDD指导下的实体类设计案例/","link":"","permalink":"http://lexburner.github.io/2017/08/21/一个DDD指导下的实体类设计案例/","excerpt":"1 引子项目开发中的工具类代码总是随着项目发展逐渐变大，在公司诸多的公用代码中，笔者发现了一个简单的，也是经常被使用的类：BaseDomain，引起了我的思考。在我们公司的开发习惯中，数据库实体类通常会继承一个叫做BaseDomain的类，这个类很简单，主要用来填充一些数据库实体公用的属性，它的设计如下：","text":"1 引子项目开发中的工具类代码总是随着项目发展逐渐变大，在公司诸多的公用代码中，笔者发现了一个简单的，也是经常被使用的类：BaseDomain，引起了我的思考。在我们公司的开发习惯中，数据库实体类通常会继承一个叫做BaseDomain的类，这个类很简单，主要用来填充一些数据库实体公用的属性，它的设计如下：1234567891011121314151617181920212223@MappedSuperclass &lt;1&gt;public class BaseDomain &#123; private Boolean deleteFlag; &lt;2&gt; private Date deleteDate; private Date lastUpdateDate; private Date createDate; @Version &lt;3&gt; private Integer version; @PrePersist &lt;4&gt; public void init()&#123; Date now = new Date(); deleteFlag = false; createDate = lastUpdateDate = now; &#125; @PreUpdate &lt;4&gt; public void update()&#123; lastUpdateDate = new Date(); &#125; &#125; 小小的一个类其实还是蕴含了不少的知识点在里面，至少可以包含以下几点： 被其他类继承后，父类的字段不会被忽略，也就意味着子类没有必要自己写这一堆公用的属性了。 逻辑删除标识，业务类的删除必须是这种打标识的行为，不能进行物理删除。值得一提的是，公司原先的该字段被命名成了isDelete，这不符合变量命名的规范，会导致一些序列化框架出现问题，而delete是数据库的保留字，所以本文中用deleteFlag。 使用version作为乐观锁的实现，version的自增以及版本失效异常受@Version该注解的影响，是由框架控制的。 创建日期，更新日期等等属性，在我们使用JPA的save方法后，框架会自动去填充相应的值。 2 发现问题与解决问题这个基类使用的频次是怎么样的呢？every class！是的，公司的每个开发者在新增一个实体类时总是优先写上Xxx extends BaseDomain 。初级开发者总是有什么学什么，他们看到公司原来的代码都是会继承这个类，以及周围的同事也是这么写着，他们甚至不知道version乐观锁的实现，不知道类的创建日期更新日期是在基类中被声明的；高级开发者能够掌握我上面所说的那些技术要点，尽管开发中因此遇到一些不适，但也是尽可能的克服。等等，上面说到添加这个基类后，对开发造成了不适感，这引起了我的思考，下面就来谈谈直观的有哪些不适感以及解决方案。 2.1 没有物理删除，只有逻辑删除真正delete操作不会再出现了,物理删除操作被setDeleteFlag(true)代替。在列表展示中，再也不能使用findAll()操作了，而是需要使用findByDeleteFlagFalse()。更多的数据库查询操作，都要考虑到，deleteFlag=true的那些记录，不应该被影响到。 解决问题：在DDD中，值得推崇的方式是使用specification模式来解决这个问题，对应到实际开发中，也就是JPA的Predicate，或者是熟悉Hibernate的人所了解的Criteria。但不可避免的一点是由于只有逻辑删除，导致了我们的数据库越来越大（解决方法不是没有，正是EventSouring+CQRS架构，这属于DDD的高级实践，本文不进行讨论）。从技术开发角度出发，这的确使得我们的编码变得稍微复杂了一点，但是其业务意义远大于这点开发工作量，所以是值得的。 2.2 级联查询变得麻烦一个会员有多个通信地址，多个银行卡。反映到实体设计，便是这样的： 1234567891011public class Member extends BaseDomain&#123; private String username; @OneToMany private List&lt;MemberAddress&gt; memberAddresses; @OneToMany private List&lt;BankCard&gt; bankCards; &#125; 其中，MemberAddress及BankCard都继承了BaseDomain。使用orm框架自带的级联功能，我们本可以查询出会员信息时，顺带查出其对应的通讯地址列表和银行卡列表。但现在不是那么的美好了，使用级联查询，可能会查询出已经被删除的MemberAddress，BankCard，只能在应用层进行deleteFlag的判断，从而过滤被删除的信息，这无法避免，因为框架不认识逻辑删除标识！ 解决问题：这个问题和2.3节的问题，恰恰是促成我写这篇文章的初衷，这与DDD有着密不可分的关联。DDD将对象划分成了entity（实体）和value object（值对象）。如果仔细分析下上面的业务并且懂一点DDD，你会立刻意识到。Member对象就是一个entity，而MemberAddress以及BankCard则是value object（username也是value object）。value object的一个重要特点，就是作为entity的修饰，从业务角度出发，MemberAddress和BankCard的确是为了更好描述Member信息，而抽象出的一个集合。而value object的另一特性，不可变性，指导了我们，不应该让MemberAddress，BankCard继承BaseDomain。说了这么多，就是想从一个理论的高度，让那些设计一个新实体便继承BaseDomain的人戒掉这个习惯。在value object丧失了deleteFlag，lastUpdateDate等属性后，可能会引发一些的质疑，他们会声称：“数据库里面member_address这张表没有lastUpdateDate字段了，我再也无法得知这条会员地址最后修改的时间了!”。是的，从逻辑意义上看，地址并没有改变，而改变的只是会员自己的地址，这个UpdateDate字段在地址上极为不合理，应该是会员的修改。也就是说lastUpdateDate应该反映到Member上。实际的开发经验告诉我，从前那么多的value object继承了BaseDomain，99%不会使用到其中的相关属性，如果真的需要使用，那么请单独为类添加，而不是继承BaseDomain。其次这些人犯了另一个错误，我们设计一个系统时，应该是entity first，而不应该database first。DDD告诉我们一个软件开发的大忌，到现在2017年，仍然有大帮的人在问：“我要实现xxxx功能，我的数据库应该如何设计？”这些人犯了根本性的错误，就是把软件的目的搞错了，软件研究的是什么？是研究如何使用计算机来解决实际（领域）问题，而不是去研究数据应该如何保存更合理。我的公司中有不少的程序员新人，希望这番话能够帮助那些“步入歧途”的从业人员 “走上正路”。软件设计应该从“数据库驱动”走向“领域驱动”，而DDD的实践经验正是为设计和开发大型复杂的软件系统提供了实践指导。 2.3 乐观锁的尴尬地位再说回BaseDomain中的version字段，由于MemberAddress和BankCard这样的value object也被赋予了乐观锁的行为，这意味着加锁的粒度变小了。DDD的指导下，改动也可以理解为由Member这个根发出，统一由Member中的version来控制，这使锁的粒度变大了。换言之，从技术开发角度，对value object加上version可以允许同时（操作系统级别真正的同时）修改一个用户的地址信息和银行卡信息，甚至是多个银行卡中不同的银行卡，而单独由Member控制，则意味着，系统在同一时刻只能进行单独一项操作。在业务并发的一般角度上考虑，一个用户是不会出现多线程修改行为的。而从软件设计的角度，单独为value object 添加version，破坏了value object的不可变性，若要修改，应当是被整个替换。 解决方案：在一般情况下，请不要为value object添加乐观锁。如果有一个场景下，你的value object需要出现版本控制，那可能有两种情况：1 你的value object是压根不是value object，可能是一个entity 2 聚合根划分错误 ….这，要真是这样源头都弄错了，压根没法聊了对吧 3 总结BaseDomain这样的设计本身并不是我想要强调的重点，但是既然出现了BaseDomain这样的设计，那么它究竟应该被什么样的实体继承，就是需要被考虑的了。DDD下，识别aggregate root，entity，value object，是整个软件设计的核心点，在本文中，判别是否继承BaseDomain的前提，就是这个对象是entity，还是value object。大家都是存在数据库中的，但是地位是不一样的。 本文若有什么不足之处，欢迎DDD爱好者指出。","categories":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"http://lexburner.github.io/categories/领域驱动设计/"}],"tags":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"http://lexburner.github.io/tags/领域驱动设计/"}]},{"title":"使用spring validation完成数据后端校验","slug":"使用spring validation完成数据后端校验","date":"2017-08-16T07:52:52.000Z","updated":"2017-08-22T04:30:17.483Z","comments":true,"path":"2017/08/16/使用spring validation完成数据后端校验/","link":"","permalink":"http://lexburner.github.io/2017/08/16/使用spring validation完成数据后端校验/","excerpt":"前言数据的校验是交互式网站一个不可或缺的功能，前端的js校验可以涵盖大部分的校验职责，如用户名唯一性，生日格式，邮箱格式校验等等常用的校验。但是为了避免用户绕过浏览器，使用http工具直接向后端请求一些违法数据，服务端的数据校验也是必要的，可以防止脏数据落到数据库中，如果数据库中出现一个非法的邮箱格式，也会让运维人员头疼不已。我在之前保险产品研发过程中，系统对数据校验要求比较严格且追求可变性及效率，曾使用drools作为规则引擎，兼任了校验的功能。而在一般的应用，可以使用本文将要介绍的validation来对数据进行校验。 简述JSR303/JSR-349，hibernate validation，spring validation之间的关系。JSR303是一项标准,JSR-349是其的升级版本，添加了一些新特性，他们规定一些校验规范即校验注解，如@Null，@NotNull，@Pattern，他们位于javax.validation.constraints包下，只提供规范不提供实现。而hibernate validation是对这个规范的实践（不要将hibernate和数据库orm框架联系在一起），他提供了相应的实现，并增加了一些其他校验注解，如@Email，@Length，@Range等等，他们位于org.hibernate.validator.constraints包下。而万能的spring为了给开发者提供便捷，对hibernate validation进行了二次封装，显示校验validated bean时，你可以使用spring validation或者hibernate validation，而spring validation另一个特性，便是其在springmvc模块中添加了自动校验，并将校验信息封装进了特定的类中。这无疑便捷了我们的web开发。本文主要介绍在springmvc中自动校验的机制。","text":"前言数据的校验是交互式网站一个不可或缺的功能，前端的js校验可以涵盖大部分的校验职责，如用户名唯一性，生日格式，邮箱格式校验等等常用的校验。但是为了避免用户绕过浏览器，使用http工具直接向后端请求一些违法数据，服务端的数据校验也是必要的，可以防止脏数据落到数据库中，如果数据库中出现一个非法的邮箱格式，也会让运维人员头疼不已。我在之前保险产品研发过程中，系统对数据校验要求比较严格且追求可变性及效率，曾使用drools作为规则引擎，兼任了校验的功能。而在一般的应用，可以使用本文将要介绍的validation来对数据进行校验。 简述JSR303/JSR-349，hibernate validation，spring validation之间的关系。JSR303是一项标准,JSR-349是其的升级版本，添加了一些新特性，他们规定一些校验规范即校验注解，如@Null，@NotNull，@Pattern，他们位于javax.validation.constraints包下，只提供规范不提供实现。而hibernate validation是对这个规范的实践（不要将hibernate和数据库orm框架联系在一起），他提供了相应的实现，并增加了一些其他校验注解，如@Email，@Length，@Range等等，他们位于org.hibernate.validator.constraints包下。而万能的spring为了给开发者提供便捷，对hibernate validation进行了二次封装，显示校验validated bean时，你可以使用spring validation或者hibernate validation，而spring validation另一个特性，便是其在springmvc模块中添加了自动校验，并将校验信息封装进了特定的类中。这无疑便捷了我们的web开发。本文主要介绍在springmvc中自动校验的机制。 引入依赖我们使用maven构建springboot应用来进行demo演示。 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 我们只需要引入spring-boot-starter-web依赖即可，如果查看其子依赖，可以发现如下的依赖： 12345678&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;&lt;/dependency&gt; 验证了我之前的描述，web模块使用了hibernate-validation，并且databind模块也提供了相应的数据绑定功能。 构建启动类无需添加其他注解，一个典型的启动类​1234567@SpringBootApplicationpublic class ValidateApp &#123; public static void main(String[] args) &#123; SpringApplication.run(ValidateApp.class, args); &#125;&#125; 创建需要被校验的实体类1234567891011121314151617public class Foo &#123; @NotBlank private String name; @Min(18) private Integer age; @Pattern(regexp = \"^1(3|4|5|7|8)\\\\d&#123;9&#125;$\",message = \"手机号码格式错误\") @NotBlank(message = \"手机号码不能为空\") private String phone; @Email(message = \"邮箱格式错误\") private String email; //... getter setter&#125; 使用一些比较常用的校验注解，还是比较浅显易懂的，字段上的注解名称即可推断出校验内容，每一个注解都包含了message字段，用于校验失败时作为提示信息，特殊的校验注解，如Pattern（正则校验），还可以自己添加正则表达式。 在@Controller中校验数据springmvc为我们提供了自动封装表单参数的功能，一个添加了参数校验的典型controller如下所示。 123456789101112131415@Controllerpublic class FooController &#123; @RequestMapping(\"/foo\") public String foo(@Validated Foo foo &lt;1&gt;, BindingResult bindingResult &lt;2&gt;) &#123; if(bindingResult.hasErrors())&#123; for (FieldError fieldError : bindingResult.getFieldErrors()) &#123; //... &#125; return \"fail\"; &#125; return \"success\"; &#125;&#125; 值得注意的地方： 参数Foo前需要加上@Validated注解，表明需要spring对其进行校验，而校验的信息会存放到其后的BindingResult中。注意，必须相邻，如果有多个参数需要校验，形式可以如下。foo(@Validated Foo foo, BindingResult fooBindingResult ，@Validated Bar bar, BindingResult barBindingResult);即一个校验类对应一个校验结果。 校验结果会被自动填充，在controller中可以根据业务逻辑来决定具体的操作，如跳转到错误页面。 一个最基本的校验就完成了，总结下框架已经提供了哪些校验：JSR提供的校验注解:12345678910111213@Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Min(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max=, min=) 被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction) 被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past 被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 @Pattern(regex=,flag=) 被注释的元素必须符合指定的正则表达式 Hibernate Validator提供的校验注解： 12345@NotBlank(message =) 验证字符串非null，且长度必须大于0 @Email 被注释的元素必须是电子邮箱地址 @Length(min=,max=) 被注释的字符串的大小必须在指定的范围内 @NotEmpty 被注释的字符串的必须非空 @Range(min=,max=,message=) 被注释的元素必须在合适的范围内 校验实验我们对上面实现的校验入口进行一次测试请求：访问 http://localhost:8080/foo?name=xujingfeng&amp;email=000&amp;age=19 可以得到如下的debug信息： 实验告诉我们，校验结果起了作用。并且，可以发现当发生多个错误，spring validation不会在第一个错误发生后立即停止，而是继续试错，告诉我们所有的错误。debug可以查看到更多丰富的错误信息，这些都是spring validation为我们提供的便捷特性，基本适用于大多数场景。 你可能不满足于简单的校验特性，下面进行一些补充。 分组校验如果同一个类，在不同的使用场景下有不同的校验规则，那么可以使用分组校验。未成年人是不能喝酒的，而在其他场景下我们不做特殊的限制，这个需求如何体现同一个实体，不同的校验规则呢？ 改写注解，添加分组： 12345678Class Foo&#123; @Min(value = 18,groups = &#123;Adult.class&#125;) private Integer age; public interface Adult&#123;&#125; public interface Minor&#123;&#125;&#125; 这样表明，只有在Adult分组下，18岁的限制才会起作用。 Controller层改写： 123456789101112131415161718192021@RequestMapping(\"/drink\")public String drink(@Validated(&#123;Foo.Adult.class&#125;) Foo foo, BindingResult bindingResult) &#123; if(bindingResult.hasErrors())&#123; for (FieldError fieldError : bindingResult.getFieldErrors()) &#123; //... &#125; return \"fail\"; &#125; return \"success\";&#125;@RequestMapping(\"/live\")public String live(@Validated Foo foo, BindingResult bindingResult) &#123; if(bindingResult.hasErrors())&#123; for (FieldError fieldError : bindingResult.getFieldErrors()) &#123; //... &#125; return \"fail\"; &#125; return \"success\";&#125; drink方法限定需要进行Adult校验，而live方法则不做限制。 自定义校验业务需求总是比框架提供的这些简单校验要复杂的多，我们可以自定义校验来满足我们的需求。自定义spring validation非常简单，主要分为两步。 1 自定义校验注解我们尝试添加一个“字符串不能包含空格”的限制。 123456789101112131415161718192021222324@Target(&#123;METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER&#125;)@Retention(RUNTIME)@Documented@Constraint(validatedBy = &#123;CannotHaveBlankValidator.class&#125;)&lt;1&gt;public @interface CannotHaveBlank &#123; //默认错误消息 String message() default \"不能包含空格\"; //分组 Class&lt;?&gt;[] groups() default &#123;&#125;; //负载 Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;; //指定多个时使用 @Target(&#123;FIELD, METHOD, PARAMETER, ANNOTATION_TYPE&#125;) @Retention(RUNTIME) @Documented @interface List &#123; CannotHaveBlank[] value(); &#125;&#125; 我们不需要关注太多东西，使用spring validation的原则便是便捷我们的开发，例如payload，List ，groups，都可以忽略。 自定义注解中指定了这个注解真正的验证者类。 2 编写真正的校验者类 1234567891011121314151617181920212223public class CannotHaveBlankValidator implements &lt;1&gt; ConstraintValidator&lt;CannotHaveBlank, String&gt; &#123; @Override public void initialize(CannotHaveBlank constraintAnnotation) &#123; &#125; @Override public boolean isValid(String value, ConstraintValidatorContext context &lt;2&gt;) &#123; //null时不进行校验 if (value != null &amp;&amp; value.contains(\" \")) &#123; &lt;3&gt; //获取默认提示信息 String defaultConstraintMessageTemplate = context.getDefaultConstraintMessageTemplate(); System.out.println(\"default message :\" + defaultConstraintMessageTemplate); //禁用默认提示信息 context.disableDefaultConstraintViolation(); //设置提示语 context.buildConstraintViolationWithTemplate(\"can not contains blank\").addConstraintViolation(); return false; &#125; return true; &#125;&#125; 所有的验证者都需要实现ConstraintValidator接口，它的接口也很形象，包含一个初始化事件方法，和一个判断是否合法的方法。 1234public interface ConstraintValidator&lt;A extends Annotation, T&gt; &#123; void initialize(A constraintAnnotation); boolean isValid(T value, ConstraintValidatorContext context);&#125; ConstraintValidatorContext 这个上下文包含了认证中所有的信息，我们可以利用这个上下文实现获取默认错误提示信息，禁用错误提示信息，改写错误提示信息等操作。 一些典型校验操作，或许可以对你产生启示作用。 值得注意的一点是，自定义注解可以用在METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER之上，ConstraintValidator的第二个泛型参数T，是需要被校验的类型。 手动校验可能在某些场景下需要我们手动校验，即使用校验器对需要被校验的实体发起validate，同步获得校验结果。理论上我们既可以使用Hibernate Validation提供Validator，也可以使用Spring对其的封装。在spring构建的项目中，提倡使用经过spring封装过后的方法，这里两种方法都介绍下： Hibernate Validation： 123456789Foo foo = new Foo();foo.setAge(22);foo.setEmail(\"000\");ValidatorFactory vf = Validation.buildDefaultValidatorFactory();Validator validator = vf.getValidator();Set&lt;ConstraintViolation&lt;Foo&gt;&gt; set = validator.validate(foo);for (ConstraintViolation&lt;Foo&gt; constraintViolation : set) &#123; System.out.println(constraintViolation.getMessage());&#125; 由于依赖了Hibernate Validation框架，我们需要调用Hibernate相关的工厂方法来获取validator实例，从而校验。 在spring framework文档的Validation相关章节，可以看到如下的描述： Spring provides full support for the Bean Validation API. This includes convenient support for bootstrapping a JSR-303/JSR-349 Bean Validation provider as a Spring bean. This allows for a javax.validation.ValidatorFactory or javax.validation.Validator to be injected wherever validation is needed in your application. Use the LocalValidatorFactoryBean to configure a default Validator as a Spring bean: bean id=”validator” class=”org.springframework.validation.beanvalidation.LocalValidatorFactoryBean” The basic configuration above will trigger Bean Validation to initialize using its default bootstrap mechanism. A JSR-303/JSR-349 provider, such as Hibernate Validator, is expected to be present in the classpath and will be detected automatically. 上面这段话主要描述了spring对validation全面支持JSR-303、JSR-349的标准，并且封装了LocalValidatorFactoryBean作为validator的实现。值得一提的是，这个类的责任其实是非常重大的，他兼容了spring的validation体系和hibernate的validation体系，也可以被开发者直接调用，代替上述的从工厂方法中获取的hibernate validator。由于我们使用了springboot，会触发web模块的自动配置，LocalValidatorFactoryBean已经成为了Validator的默认实现，使用时只需要自动注入即可。 12345678910111213141516@AutowiredValidator globalValidator; &lt;1&gt;@RequestMapping(\"/validate\")public String validate() &#123; Foo foo = new Foo(); foo.setAge(22); foo.setEmail(\"000\"); Set&lt;ConstraintViolation&lt;Foo&gt;&gt; set = globalValidator.validate(foo);&lt;2&gt; for (ConstraintViolation&lt;Foo&gt; constraintViolation : set) &#123; System.out.println(constraintViolation.getMessage()); &#125; return \"success\";&#125; 真正使用过Validator接口的读者会发现有两个接口，一个是位于javax.validation包下，另一个位于org.springframework.validation包下，注意我们这里使用的是前者javax.validation，后者是spring自己内置的校验接口，LocalValidatorFactoryBean同时实现了这两个接口。 此处校验接口最终的实现类便是LocalValidatorFactoryBean。 基于方法校验12345678910111213141516171819202122@RestController@Validated &lt;1&gt;public class BarController &#123; @RequestMapping(\"/bar\") public @NotBlank &lt;2&gt; String bar(@Min(18) Integer age &lt;3&gt;) &#123; System.out.println(\"age : \" + age); return \"\"; &#125; @ExceptionHandler(ConstraintViolationException.class) public Map handleConstraintViolationException(ConstraintViolationException cve)&#123; Set&lt;ConstraintViolation&lt;?&gt;&gt; cves = cve.getConstraintViolations();&lt;4&gt; for (ConstraintViolation&lt;?&gt; constraintViolation : cves) &#123; System.out.println(constraintViolation.getMessage()); &#125; Map map = new HashMap(); map.put(\"errorCode\",500); return map; &#125;&#125; 为类添加@Validated注解 校验方法的返回值和入参 添加一个异常处理器，可以获得没有通过校验的属性相关信息 基于方法的校验，个人不推荐使用，感觉和项目结合的不是很好。 使用校验框架的一些想法理论上spring validation可以实现很多复杂的校验，你甚至可以使你的Validator获取ApplicationContext，获取spring容器中所有的资源，进行诸如数据库校验，注入其他校验工具，完成组合校验（如前后密码一致）等等操作，但是寻求一个易用性和封装复杂性之间的平衡点是我们作为工具使用者应该考虑的，我推崇的方式，是仅仅使用自带的注解和自定义注解，完成一些简单的，可复用的校验。而对于复杂的校验，则包含在业务代码之中，毕竟如用户名是否存在这样的校验，仅仅依靠数据库查询还不够，为了避免并发问题，还是得加上唯一索引之类的额外工作，不是吗？","categories":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"},{"name":"Validation","slug":"Validation","permalink":"http://lexburner.github.io/tags/Validation/"}]},{"title":"Re：从零开始的Spring Security OAuth2（三）","slug":"Re：从零开始的Spring Security OAuth2（三）","date":"2017-08-10T06:22:12.000Z","updated":"2017-08-22T03:19:56.161Z","comments":true,"path":"2017/08/10/Re：从零开始的Spring Security OAuth2（三）/","link":"","permalink":"http://lexburner.github.io/2017/08/10/Re：从零开始的Spring Security OAuth2（三）/","excerpt":"上一篇文章中我们介绍了获取token的流程，这一篇重点分析一下，携带token访问受限资源时，内部的工作流程。 @EnableResourceServer与@EnableAuthorizationServer还记得我们在第一节中就介绍过了OAuth2的两个核心概念，资源服务器与身份认证服务器。我们对两个注解进行配置的同时，到底触发了内部的什么相关配置呢？ 上一篇文章重点介绍的其实是与身份认证相关的流程，即如果获取token，而本节要分析的携带token访问受限资源，自然便是与@EnableResourceServer相关的资源服务器配置了。 我们注意到其相关配置类是ResourceServerConfigurer，内部关联了ResourceServerSecurityConfigurer和HttpSecurity。前者与资源安全配置相关，后者与http安全配置相关。（类名比较类似，注意区分，以Adapter结尾的是适配器，以Configurer结尾的是配置器，以Builder结尾的是建造器，他们分别代表不同的设计模式，对设计模式有所了解可以更加方便理解其设计思路） 1234567891011public class ResourceServerConfigurerAdapter implements ResourceServerConfigurer &#123; @Override public void configure(ResourceServerSecurityConfigurer resources &lt;1&gt; ) throws Exception &#123; &#125; @Override public void configure(HttpSecurity http) throws Exception &#123; http.authorizeRequests().anyRequest().authenticated(); &#125;&#125; ResourceServerSecurityConfigurer显然便是我们分析的重点了。","text":"上一篇文章中我们介绍了获取token的流程，这一篇重点分析一下，携带token访问受限资源时，内部的工作流程。 @EnableResourceServer与@EnableAuthorizationServer还记得我们在第一节中就介绍过了OAuth2的两个核心概念，资源服务器与身份认证服务器。我们对两个注解进行配置的同时，到底触发了内部的什么相关配置呢？ 上一篇文章重点介绍的其实是与身份认证相关的流程，即如果获取token，而本节要分析的携带token访问受限资源，自然便是与@EnableResourceServer相关的资源服务器配置了。 我们注意到其相关配置类是ResourceServerConfigurer，内部关联了ResourceServerSecurityConfigurer和HttpSecurity。前者与资源安全配置相关，后者与http安全配置相关。（类名比较类似，注意区分，以Adapter结尾的是适配器，以Configurer结尾的是配置器，以Builder结尾的是建造器，他们分别代表不同的设计模式，对设计模式有所了解可以更加方便理解其设计思路） 1234567891011public class ResourceServerConfigurerAdapter implements ResourceServerConfigurer &#123; @Override public void configure(ResourceServerSecurityConfigurer resources &lt;1&gt; ) throws Exception &#123; &#125; @Override public void configure(HttpSecurity http) throws Exception &#123; http.authorizeRequests().anyRequest().authenticated(); &#125;&#125; ResourceServerSecurityConfigurer显然便是我们分析的重点了。 ResourceServerSecurityConfigurer（了解）其核心配置如下所示： 123456789101112131415161718192021222324public void configure(HttpSecurity http) throws Exception &#123; AuthenticationManager oauthAuthenticationManager = oauthAuthenticationManager(http); resourcesServerFilter = new OAuth2AuthenticationProcessingFilter();//&lt;1&gt; resourcesServerFilter.setAuthenticationEntryPoint(authenticationEntryPoint); resourcesServerFilter.setAuthenticationManager(oauthAuthenticationManager);//&lt;2&gt; if (eventPublisher != null) &#123; resourcesServerFilter.setAuthenticationEventPublisher(eventPublisher); &#125; if (tokenExtractor != null) &#123; resourcesServerFilter.setTokenExtractor(tokenExtractor);//&lt;3&gt; &#125; resourcesServerFilter = postProcess(resourcesServerFilter); resourcesServerFilter.setStateless(stateless); // @formatter:off http .authorizeRequests().expressionHandler(expressionHandler) .and() .addFilterBefore(resourcesServerFilter, AbstractPreAuthenticatedProcessingFilter.class) .exceptionHandling() .accessDeniedHandler(accessDeniedHandler)//&lt;4&gt; .authenticationEntryPoint(authenticationEntryPoint); // @formatter:on&#125; 这段是整个oauth2与HttpSecurity相关的核心配置，其中有非常多的注意点，顺带的都强调一下： 创建OAuth2AuthenticationProcessingFilter，即下一节所要介绍的OAuth2核心过滤器。 为OAuth2AuthenticationProcessingFilter提供固定的AuthenticationManager即OAuth2AuthenticationManager，它并没有将OAuth2AuthenticationManager添加到spring的容器中，不然可能会影响spring security的普通认证流程（非oauth2请求），只有被OAuth2AuthenticationProcessingFilter拦截到的oauth2相关请求才被特殊的身份认证器处理。 设置了TokenExtractor默认的实现—-BearerTokenExtractor，这个类在下一节介绍。 相关的异常处理器，可以重写相关实现，达到自定义异常的目的。 还记得我们在一开始的配置中配置了资源服务器，是它触发了相关的配置。123@Configuration@EnableResourceServerprotected static class ResourceServerConfiguration extends ResourceServerConfigurerAdapter &#123;&#125; 核心过滤器 OAuth2AuthenticationProcessingFilter（掌握）回顾一下我们之前是如何携带token访问受限资源的：http://localhost:8080/order/1?access_token=950a7cc9-5a8a-42c9-a693-40e817b1a4b0唯一的身份凭证，便是这个access_token，携带它进行访问，会进入OAuth2AuthenticationProcessingFilter之中，其核心代码如下： 1234567891011121314151617181920212223242526272829303132public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain)&#123; final HttpServletRequest request = (HttpServletRequest) req; final HttpServletResponse response = (HttpServletResponse) res; try &#123; //从请求中取出身份信息，即access_token Authentication authentication = tokenExtractor.extract(request); if (authentication == null) &#123; ... &#125; else &#123; request.setAttribute(OAuth2AuthenticationDetails.ACCESS_TOKEN_VALUE, authentication.getPrincipal()); if (authentication instanceof AbstractAuthenticationToken) &#123; AbstractAuthenticationToken needsDetails = (AbstractAuthenticationToken) authentication; needsDetails.setDetails(authenticationDetailsSource.buildDetails(request)); &#125; //认证身份 Authentication authResult = authenticationManager.authenticate(authentication); ... eventPublisher.publishAuthenticationSuccess(authResult); //将身份信息绑定到SecurityContextHolder中 SecurityContextHolder.getContext().setAuthentication(authResult); &#125; &#125; catch (OAuth2Exception failed) &#123; ... return; &#125; chain.doFilter(request, response);&#125; 整个过滤器便是oauth2身份鉴定的关键，在源码中，对这个类有一段如下的描述 A pre-authentication filter for OAuth2 protected resources. Extracts an OAuth2 token from the incoming request and uses it to populate the Spring Security context with an {@link OAuth2Authentication} (if used in conjunction with an {@link OAuth2AuthenticationManager}). OAuth2保护资源的预先认证过滤器。如果与OAuth2AuthenticationManager结合使用，则会从到来的请求之中提取一个OAuth2 token，之后使用OAuth2Authentication来填充Spring Security上下文。 其中涉及到了两个关键的类TokenExtractor，AuthenticationManager。相信后者这个接口大家已经不陌生，但前面这个类之前还未出现在我们的视野中。 OAuth2的身份管理器–OAuth2AuthenticationManager（掌握）在之前的OAuth2核心过滤器中出现的AuthenticationManager其实在我们意料之中，携带access_token必定得经过身份认证，但是在我们debug进入其中后，发现了一个出乎意料的事，AuthenticationManager的实现类并不是我们在前面文章中聊到的常用实现类ProviderManager，而是OAuth2AuthenticationManager。 图1 新的AuthenticationManager实现类OAuth2AuthenticationManager 回顾我们第一篇文章的配置，压根没有出现过这个OAuth2AuthenticationManager，并且它脱离了我们熟悉的认证流程（第二篇文章中的认证管理器UML图是一张经典的spring security结构类图），它直接重写了容器的顶级身份认证接口，内部维护了一个ClientDetailService和ResourceServerTokenServices，这两个核心类在 Re：从零开始的Spring Security Oauth2（二）有分析过。在ResourceServerSecurityConfigurer的小节中我们已经知晓了它是如何被框架自动配置的，这里要强调的是OAuth2AuthenticationManager是密切与token认证相关的，而不是与获取token密切相关的。 其判别身份的关键代码如下： 123456789101112131415161718public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; ... String token = (String) authentication.getPrincipal(); //最终还是借助tokenServices根据token加载身份信息 OAuth2Authentication auth = tokenServices.loadAuthentication(token); ... checkClientDetails(auth); if (authentication.getDetails() instanceof OAuth2AuthenticationDetails) &#123; OAuth2AuthenticationDetails details = (OAuth2AuthenticationDetails) authentication.getDetails(); ... &#125; auth.setDetails(authentication.getDetails()); auth.setAuthenticated(true); return auth;&#125; 说到tokenServices这个密切与token相关的接口，这里要强调下，避免产生误解。tokenServices分为两类，一个是用在AuthenticationServer端，第二篇文章中介绍的 123456789public interface AuthorizationServerTokenServices &#123; //创建token OAuth2AccessToken createAccessToken(OAuth2Authentication authentication) throws AuthenticationException; //刷新token OAuth2AccessToken refreshAccessToken(String refreshToken, TokenRequest tokenRequest) throws AuthenticationException; //获取token OAuth2AccessToken getAccessToken(OAuth2Authentication authentication);&#125; 而在ResourceServer端有自己的tokenServices接口： 12345678public interface ResourceServerTokenServices &#123; //根据accessToken加载客户端信息 OAuth2Authentication loadAuthentication(String accessToken) throws AuthenticationException, InvalidTokenException; //根据accessToken获取完整的访问令牌详细信息。 OAuth2AccessToken readAccessToken(String accessToken);&#125; 具体内部如何加载，和AuthorizationServer大同小异，只是从tokenStore中取出相应身份的流程有点区别，不再详细看实现类了。 TokenExtractor（了解）这个接口只有一个实现类，而且代码非常简单 1234567891011121314151617181920212223242526272829303132333435363738public class BearerTokenExtractor implements TokenExtractor &#123; private final static Log logger = LogFactory.getLog(BearerTokenExtractor.class); @Override public Authentication extract(HttpServletRequest request) &#123; String tokenValue = extractToken(request); if (tokenValue != null) &#123; PreAuthenticatedAuthenticationToken authentication = new PreAuthenticatedAuthenticationToken(tokenValue, \"\"); return authentication; &#125; return null; &#125; protected String extractToken(HttpServletRequest request) &#123; // first check the header... String token = extractHeaderToken(request); // bearer type allows a request parameter as well if (token == null) &#123; ... //从requestParameter中获取token &#125; return token; &#125;/** * Extract the OAuth bearer token from a header. */ protected String extractHeaderToken(HttpServletRequest request) &#123; Enumeration&lt;String&gt; headers = request.getHeaders(\"Authorization\"); while (headers.hasMoreElements()) &#123; // typically there is only one (most servers enforce that) ... //从Header中获取token &#125; return null; &#125;&#125; 它的作用在于分离出请求中包含的token。也启示了我们可以使用多种方式携带token。1 在Header中携带 123http://localhost:8080/order/1Header：Authentication：Bearer f732723d-af7f-41bb-bd06-2636ab2be135 2 拼接在url中作为requestParam 1http://localhost:8080/order/1?access_token=f732723d-af7f-41bb-bd06-2636ab2be135 3 在form表单中携带 123http://localhost:8080/order/1form param：access_token=f732723d-af7f-41bb-bd06-2636ab2be135 异常处理OAuth2在资源服务器端的异常处理不算特别完善，但基本够用，如果想要重写异常机制，可以直接替换掉相关的Handler，如权限相关的AccessDeniedHandler。具体的配置应该在@EnableResourceServer中被覆盖，这是适配器+配置器的好处。 总结到这儿，Spring Security OAuth2的整个内部流程就算是分析结束了。本系列的文章只能算是揭示一个大概的流程，重点还是介绍相关设计+接口，想要了解更多的细节，需要自己去翻看源码，研究各个实现类。在分析源码过程中总结出的一点经验，与君共勉： 先掌握宏观，如研究UML类图，搞清楚关联 分析顶级接口，设计是面向接口的，不重要的部分，具体实现类甚至都可以忽略 学会对比，如ResourceServer和AuthenticationServer是一种对称的设计，整个框架内部的类非常多，但分门别类的记忆，会加深记忆。如ResourceServerTokenServices ，AuthenticationServerTokenServices就一定是作用相关，但所属领域不同的两个接口 熟悉设计模式，spring中涉及了大量的设计模式，在框架的设计中也是遵循着设计模式的规范，如以Adapter结尾，便是运用了适配器模式；以Factory结尾，便是运用了适配器模式；Template结尾，便是运用了模板方法模式；Builder结尾，便是运用了建造者模式… 一点自己的理解：对源码的理解和灵感，这一切都建立自身的编码经验之上，自己遵循规范便能更好的理解别人同样遵守规范的代码。相对的，阅读好的源码，也能帮助我们自身提升编码规范。","categories":[{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","permalink":"http://lexburner.github.io/categories/Spring-Security-OAuth2/"}],"tags":[{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","permalink":"http://lexburner.github.io/tags/Spring-Security-OAuth2/"}]},{"title":"Re：从零开始的Spring Security OAuth2（二）","slug":"Re：从零开始的Spring Security OAuth2（二）","date":"2017-08-09T06:58:52.000Z","updated":"2017-08-22T04:27:58.368Z","comments":true,"path":"2017/08/09/Re：从零开始的Spring Security OAuth2（二）/","link":"","permalink":"http://lexburner.github.io/2017/08/09/Re：从零开始的Spring Security OAuth2（二）/","excerpt":"本文开始从源码的层面，讲解一些Spring Security Oauth2的认证流程。本文较长，适合在空余时间段观看。且涉及了较多的源码，非关键性代码以…代替。 准备工作首先开启debug信息： 123logging: level: org.springframework: DEBUG 可以完整的看到内部的运转流程。 client模式稍微简单一些，使用client模式获取token http://localhost:8080/oauth/token?client_id=client_1&amp;client_secret=123456&amp;scope=select&amp;grant_type=client_credentials 由于debug信息太多了，我简单按照顺序列了一下关键的几个类： 1234ClientCredentialsTokenEndpointFilterDaoAuthenticationProviderTokenEndpointTokenGranter","text":"本文开始从源码的层面，讲解一些Spring Security Oauth2的认证流程。本文较长，适合在空余时间段观看。且涉及了较多的源码，非关键性代码以…代替。 准备工作首先开启debug信息： 123logging: level: org.springframework: DEBUG 可以完整的看到内部的运转流程。 client模式稍微简单一些，使用client模式获取token http://localhost:8080/oauth/token?client_id=client_1&amp;client_secret=123456&amp;scope=select&amp;grant_type=client_credentials 由于debug信息太多了，我简单按照顺序列了一下关键的几个类： 1234ClientCredentialsTokenEndpointFilterDaoAuthenticationProviderTokenEndpointTokenGranter @EnableAuthorizationServer上一篇博客中我们尝试使用了password模式和client模式，有一个比较关键的endpoint：/oauth/token。从这个入口开始分析，spring security oauth2内部是如何生成token的。获取token，与第一篇文章中的两个重要概念之一有关，也就是AuthorizationServer与ResourceServer中的AuthorizationServer。 在之前的配置中 123@Configuration@EnableAuthorizationServerprotected static class AuthorizationServerConfiguration extends AuthorizationServerConfigurerAdapter &#123;&#125; 出现了AuthorizationServerConfigurerAdapter 关键类，他关联了三个重要的配置类，分别是 1234567891011121314public class AuthorizationServerConfigurerAdapter implements AuthorizationServerConfigurer &#123; @Override public void configure(AuthorizationServerSecurityConfigurer security &lt;1&gt;) throws Exception&#123; &#125; @Override public void configure(ClientDetailsServiceConfigurer clients &lt;2&gt;) throws Exception &#123; &#125; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints &lt;3&gt;) throws Exception &#123; &#125;&#125; 配置AuthorizationServer安全认证的相关信息，创建ClientCredentialsTokenEndpointFilter核心过滤器 配置OAuth2的客户端相关信息 配置AuthorizationServerEndpointsConfigurer众多相关类，包括配置身份认证器，配置认证方式，TokenStore，TokenGranter，OAuth2RequestFactory 我们逐步分析其中关键的类 客户端身份认证核心过滤器ClientCredentialsTokenEndpointFilter（掌握）截取关键的代码，可以分析出大概的流程在请求到达/oauth/token之前经过了ClientCredentialsTokenEndpointFilter这个过滤器，关键方法如下 1234567891011121314public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException, IOException, ServletException &#123; ... String clientId = request.getParameter(\"client_id\"); String clientSecret = request.getParameter(\"client_secret\"); ... clientId = clientId.trim(); UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken(clientId, clientSecret); return this.getAuthenticationManager().authenticate(authRequest);&#125; 顶级身份管理者AuthenticationManager（掌握）用来从请求中获取client_id,client_secret，组装成一个UsernamePasswordAuthenticationToken作为身份标识，使用容器中的顶级身份管理器AuthenticationManager去进行身份认证（AuthenticationManager的实现类一般是ProviderManager。而ProviderManager内部维护了一个List,真正的身份认证是由一系列AuthenticationProvider去完成。而AuthenticationProvider的常用实现类则是DaoAuthenticationProvider，DaoAuthenticationProvider内部又聚合了一个UserDetailsService接口，UserDetailsService才是获取用户详细信息的最终接口，而我们上一篇文章中在内存中配置用户，就是使用了UserDetailsService的一个实现类InMemoryUserDetailsManager）。UML类图可以大概理解下这些类的关系，省略了授权部分。 图1 认证相关UML类图可能机智的读者会发现一个问题，我前面一篇文章已经提到了client模式是不存在“用户”的概念的，那么这里的身份认证是在认证什么呢？debug可以发现UserDetailsService的实现被适配成了ClientDetailsUserDetailsService，这个设计是将client客户端的信息（client_id,client_secret）适配成用户的信息(username,password)，这样我们的认证流程就不需要修改了。经过ClientCredentialsTokenEndpointFilter之后，身份信息已经得到了AuthenticationManager的验证。接着便到达了TokenEndpoint。## Token处理端点TokenEndpoint（掌握）前面的两个ClientCredentialsTokenEndpointFilter和AuthenticationManager可以理解为一些前置校验，和身份封装，而这个类一看名字就知道和我们的token是密切相关的。1234567891011121314151617181920@FrameworkEndpointpublic class TokenEndpoint extends AbstractEndpoint &#123; @RequestMapping(value = \"/oauth/token\", method=RequestMethod.POST) public ResponseEntity&lt;OAuth2AccessToken&gt; postAccessToken(Principal principal, @RequestParam Map&lt;String, String&gt; parameters) throws HttpRequestMethodNotSupportedException &#123; ... String clientId = getClientId(principal); ClientDetails authenticatedClient = getClientDetailsService().loadClientByClientId(clientId);//&lt;1&gt; ... TokenRequest tokenRequest = getOAuth2RequestFactory().createTokenRequest(parameters, authenticatedClient);//&lt;2&gt; ... OAuth2AccessToken token = getTokenGranter().grant(tokenRequest.getGrantType(), tokenRequest);//&lt;3&gt; ... return getResponse(token); &#125; private TokenGranter tokenGranter;&#125; 加载客户端信息 结合请求信息，创建TokenRequest 将TokenRequest传递给TokenGranter颁发token 省略了一些校验代码之后，真正的/oauth/token端点暴露在了我们眼前，其中方法参数中的Principal经过之前的过滤器，已经被填充了相关的信息，而方法的内部则是依赖了一个TokenGranter 来颁发token。其中OAuth2AccessToken的实现类DefaultOAuth2AccessToken就是最终在控制台得到的token序列化之前的原始类:​12345678910public class DefaultOAuth2AccessToken implements Serializable, OAuth2AccessToken &#123; private static final long serialVersionUID = 914967629530462926L; private String value; private Date expiration; private String tokenType = BEARER_TYPE.toLowerCase(); private OAuth2RefreshToken refreshToken; private Set&lt;String&gt; scope; private Map&lt;String, Object&gt; additionalInformation = Collections.emptyMap(); //getter,setter&#125;1234567891011121314@org.codehaus.jackson.map.annotate.JsonSerialize(using = OAuth2AccessTokenJackson1Serializer.class)@org.codehaus.jackson.map.annotate.JsonDeserialize(using = OAuth2AccessTokenJackson1Deserializer.class)@com.fasterxml.jackson.databind.annotation.JsonSerialize(using = OAuth2AccessTokenJackson2Serializer.class)@com.fasterxml.jackson.databind.annotation.JsonDeserialize(using = OAuth2AccessTokenJackson2Deserializer.class)public interface OAuth2AccessToken &#123; public static String BEARER_TYPE = \"Bearer\"; public static String OAUTH2_TYPE = \"OAuth2\"; public static String ACCESS_TOKEN = \"access_token\"; public static String TOKEN_TYPE = \"token_type\"; public static String EXPIRES_IN = \"expires_in\"; public static String REFRESH_TOKEN = \"refresh_token\"; public static String SCOPE = \"scope\"; ...&#125;一个典型的样例token响应,如下所示，就是上述类序列化后的结果：1234567&#123; \"access_token\":\"950a7cc9-5a8a-42c9-a693-40e817b1a4b0\", \"token_type\":\"bearer\", \"refresh_token\":\"773a0fcd-6023-45f8-8848-e141296cb3cb\", \"expires_in\":27036, \"scope\":\"select\" &#125;## TokenGranter（掌握）先从UML类图对TokenGranter接口的设计有一个宏观的认识图2 TokenGranter相关UML类图 TokenGranter的设计思路是使用CompositeTokenGranter管理一个List列表，每一种grantType对应一个具体的真正授权者，在debug过程中可以发现CompositeTokenGranter 内部就是在循环调用五种TokenGranter实现类的grant方法，而granter内部则是通过grantType来区分是否是各自的授权类型。 123456789101112131415161718public class CompositeTokenGranter implements TokenGranter &#123; private final List&lt;TokenGranter&gt; tokenGranters; public CompositeTokenGranter(List&lt;TokenGranter&gt; tokenGranters) &#123; this.tokenGranters = new ArrayList&lt;TokenGranter&gt;(tokenGranters); &#125; public OAuth2AccessToken grant(String grantType, TokenRequest tokenRequest) &#123; for (TokenGranter granter : tokenGranters) &#123; OAuth2AccessToken grant = granter.grant(grantType, tokenRequest); if (grant!=null) &#123; return grant; &#125; &#125; return null; &#125;&#125; 五种类型分别是： ResourceOwnerPasswordTokenGranter ==&gt; password密码模式 AuthorizationCodeTokenGranter ==&gt; authorization_code授权码模式 ClientCredentialsTokenGranter ==&gt; client_credentials客户端模式 ImplicitTokenGranter ==&gt; implicit简化模式 RefreshTokenGranter ==&gt;refresh_token 刷新token专用 以客户端模式为例，思考如何产生token的，则需要继续研究5种授权者的抽象类：AbstractTokenGranter 123456789101112131415161718192021222324252627282930313233343536public abstract class AbstractTokenGranter implements TokenGranter &#123; protected final Log logger = LogFactory.getLog(getClass()); //与token相关的service，重点 private final AuthorizationServerTokenServices tokenServices; //与clientDetails相关的service，重点 private final ClientDetailsService clientDetailsService; //创建oauth2Request的工厂，重点 private final OAuth2RequestFactory requestFactory; private final String grantType; ... public OAuth2AccessToken grant(String grantType, TokenRequest tokenRequest) &#123; ... String clientId = tokenRequest.getClientId(); ClientDetails client = clientDetailsService.loadClientByClientId(clientId); validateGrantType(grantType, client); logger.debug(\"Getting access token for: \" + clientId); return getAccessToken(client, tokenRequest); &#125; protected OAuth2AccessToken getAccessToken(ClientDetails client, TokenRequest tokenRequest) &#123; return tokenServices.createAccessToken(getOAuth2Authentication(client, tokenRequest)); &#125; protected OAuth2Authentication getOAuth2Authentication(ClientDetails client, TokenRequest tokenRequest) &#123; OAuth2Request storedOAuth2Request = requestFactory.createOAuth2Request(client, tokenRequest); return new OAuth2Authentication(storedOAuth2Request, null); &#125; ...&#125; 回过头去看TokenEndpoint中，正是调用了这里的三个重要的类变量的相关方法。由于篇幅限制，不能延展太多，不然没完没了，所以重点分析下AuthorizationServerTokenServices是何方神圣。 AuthorizationServerTokenServices（了解）AuthorizationServer端的token操作service，接口设计如下： 12345678910public interface AuthorizationServerTokenServices &#123; //创建token OAuth2AccessToken createAccessToken(OAuth2Authentication authentication) throws AuthenticationException; //刷新token OAuth2AccessToken refreshAccessToken(String refreshToken, TokenRequest tokenRequest) throws AuthenticationException; //获取token OAuth2AccessToken getAccessToken(OAuth2Authentication authentication);&#125; 在默认的实现类DefaultTokenServices中，可以看到token是如何产生的，并且了解了框架对token进行哪些信息的关联。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Transactionalpublic OAuth2AccessToken createAccessToken(OAuth2Authentication authentication) throws AuthenticationException &#123; OAuth2AccessToken existingAccessToken = tokenStore.getAccessToken(authentication); OAuth2RefreshToken refreshToken = null; if (existingAccessToken != null) &#123; if (existingAccessToken.isExpired()) &#123; if (existingAccessToken.getRefreshToken() != null) &#123; refreshToken = existingAccessToken.getRefreshToken(); // The token store could remove the refresh token when the // access token is removed, but we want to // be sure... tokenStore.removeRefreshToken(refreshToken); &#125; tokenStore.removeAccessToken(existingAccessToken); &#125; else &#123; // Re-store the access token in case the authentication has changed tokenStore.storeAccessToken(existingAccessToken, authentication); return existingAccessToken; &#125; &#125; // Only create a new refresh token if there wasn't an existing one // associated with an expired access token. // Clients might be holding existing refresh tokens, so we re-use it in // the case that the old access token // expired. if (refreshToken == null) &#123; refreshToken = createRefreshToken(authentication); &#125; // But the refresh token itself might need to be re-issued if it has // expired. else if (refreshToken instanceof ExpiringOAuth2RefreshToken) &#123; ExpiringOAuth2RefreshToken expiring = (ExpiringOAuth2RefreshToken) refreshToken; if (System.currentTimeMillis() &gt; expiring.getExpiration().getTime()) &#123; refreshToken = createRefreshToken(authentication); &#125; &#125; OAuth2AccessToken accessToken = createAccessToken(authentication, refreshToken); tokenStore.storeAccessToken(accessToken, authentication); // In case it was modified refreshToken = accessToken.getRefreshToken(); if (refreshToken != null) &#123; tokenStore.storeRefreshToken(refreshToken, authentication); &#125; return accessToken;&#125; 简单总结一下AuthorizationServerTokenServices的作用，他提供了创建token，刷新token，获取token的实现。在创建token时，他会调用tokenStore对产生的token和相关信息存储到对应的实现类中，可以是redis，数据库，内存，jwt。 总结本篇总结了使用客户端模式获取Token时，spring security oauth2内部的运作流程，重点是在分析AuthenticationServer相关的类。其他模式有一定的不同，但抽象功能是固定的，只是具体的实现类会被相应地替换。阅读spring的源码，会发现它的设计中出现了非常多的抽象接口，这对我们理清楚内部工作流程产生了不小的困扰，我的方式是可以借助UML类图，先从宏观理清楚作者的设计思路，这会让我们的分析事半功倍。 下一篇文章重点分析用户携带token访问受限资源时，spring security oauth2内部的工作流程。即ResourceServer相关的类。","categories":[{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","permalink":"http://lexburner.github.io/categories/Spring-Security-OAuth2/"}],"tags":[{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","permalink":"http://lexburner.github.io/tags/Spring-Security-OAuth2/"}]},{"title":"Re：从零开始的Spring Security OAuth2（一）","slug":"Re：从零开始的Spring Security OAuth2（一）","date":"2017-08-08T07:16:52.000Z","updated":"2017-08-22T04:27:03.734Z","comments":true,"path":"2017/08/08/Re：从零开始的Spring Security OAuth2（一）/","link":"","permalink":"http://lexburner.github.io/2017/08/08/Re：从零开始的Spring Security OAuth2（一）/","excerpt":"##前言今天来聊聊一个接口对接的场景，A厂家有一套HTTP接口需要提供给B厂家使用，由于是外网环境，所以需要有一套安全机制保障，这个时候oauth2就可以作为一个方案。 关于oauth2，其实是一个规范，本文重点讲解spring对他进行的实现，如果你还不清楚授权服务器，资源服务器，认证授权等基础概念，可以移步理解OAuth 2.0 - 阮一峰，这是一篇对于oauth2很好的科普文章。 需要对spring security有一定的配置使用经验，用户认证这一块，spring security oauth2建立在spring security的基础之上。第一篇文章主要是讲解使用springboot搭建一个简易的授权，资源服务器，在文末会给出具体代码的github地址。后续文章会进行spring security oauth2的相关源码分析。java中的安全框架如shrio，已经有跟我学shiro - 开涛，非常成体系地，深入浅出地讲解了apache的这个开源安全框架，但是spring security包括oauth2一直没有成体系的文章，学习它们大多依赖于较少的官方文档，理解一下基本的使用配置；通过零散的博客，了解一下他人的使用经验；打断点，分析内部的工作流程；看源码中的接口设计，以及注释，了解设计者的用意。spring的各个框架都运用了很多的设计模式，在学习源码的过程中，也大概了解了一些套路。spring也在必要的地方添加了适当的注释，避免了源码阅读者对于一些细节设计的理解产生偏差，让我更加感叹，spring不仅仅是一个工具框架，更像是一个艺术品。","text":"##前言今天来聊聊一个接口对接的场景，A厂家有一套HTTP接口需要提供给B厂家使用，由于是外网环境，所以需要有一套安全机制保障，这个时候oauth2就可以作为一个方案。 关于oauth2，其实是一个规范，本文重点讲解spring对他进行的实现，如果你还不清楚授权服务器，资源服务器，认证授权等基础概念，可以移步理解OAuth 2.0 - 阮一峰，这是一篇对于oauth2很好的科普文章。 需要对spring security有一定的配置使用经验，用户认证这一块，spring security oauth2建立在spring security的基础之上。第一篇文章主要是讲解使用springboot搭建一个简易的授权，资源服务器，在文末会给出具体代码的github地址。后续文章会进行spring security oauth2的相关源码分析。java中的安全框架如shrio，已经有跟我学shiro - 开涛，非常成体系地，深入浅出地讲解了apache的这个开源安全框架，但是spring security包括oauth2一直没有成体系的文章，学习它们大多依赖于较少的官方文档，理解一下基本的使用配置；通过零散的博客，了解一下他人的使用经验；打断点，分析内部的工作流程；看源码中的接口设计，以及注释，了解设计者的用意。spring的各个框架都运用了很多的设计模式，在学习源码的过程中，也大概了解了一些套路。spring也在必要的地方添加了适当的注释，避免了源码阅读者对于一些细节设计的理解产生偏差，让我更加感叹，spring不仅仅是一个工具框架，更像是一个艺术品。 概述使用oauth2保护你的应用，可以分为简易的分为三个步骤 配置资源服务器 配置认证服务器 配置spring security 前两点是oauth2的主体内容，但前面我已经描述过了，spring security oauth2是建立在spring security基础之上的，所以有一些体系是公用的。 oauth2根据使用场景不同，分成了4种模式 授权码模式（authorization code） 简化模式（implicit） 密码模式（resource owner password credentials） 客户端模式（client credentials） 本文重点讲解接口对接中常使用的密码模式（以下简称password模式）和客户端模式（以下简称client模式）。授权码模式使用到了回调地址，是最为复杂的方式，通常网站中经常出现的微博，qq第三方登录，都会采用这个形式。简化模式不常用。 项目准备主要的maven依赖如下 12345678910111213141516171819&lt;!-- 注意是starter,自动配置 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 不是starter,手动配置 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security.oauth&lt;/groupId&gt; &lt;artifactId&gt;spring-security-oauth2&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 将token存储在redis中 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 我们给自己先定个目标，要干什么事？既然说到保护应用，那必须得先有一些资源，我们创建一个endpoint作为提供给外部的接口：123456789101112131415161718@RestControllerpublic class TestEndpoints &#123; @GetMapping(\"/product/&#123;id&#125;\") public String getProduct(@PathVariable String id) &#123; //for debug Authentication authentication = SecurityContextHolder.getContext().getAuthentication(); return \"product id : \" + id; &#125; @GetMapping(\"/order/&#123;id&#125;\") public String getOrder(@PathVariable String id) &#123; //for debug Authentication authentication = SecurityContextHolder.getContext().getAuthentication(); return \"order id : \" + id; &#125;&#125; 暴露一个商品查询接口，后续不做安全限制，一个订单查询接口，后续添加访问控制。 配置资源服务器和授权服务器由于是两个oauth2的核心配置，我们放到一个配置类中。为了方便下载代码直接运行，我这里将客户端信息放到了内存中，生产中可以配置到数据库中。token的存储一般选择使用redis，一是性能比较好，二是自动过期的机制，符合token的特性。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475@Configurationpublic class OAuth2ServerConfig &#123; private static final String DEMO_RESOURCE_ID = \"order\"; @Configuration @EnableResourceServer protected static class ResourceServerConfiguration extends ResourceServerConfigurerAdapter &#123; @Override public void configure(ResourceServerSecurityConfigurer resources) &#123; resources.resourceId(DEMO_RESOURCE_ID).stateless(true); &#125; @Override public void configure(HttpSecurity http) throws Exception &#123; // @formatter:off http // Since we want the protected resources to be accessible in the UI as well we need // session creation to be allowed (it's disabled by default in 2.0.6) .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.IF_REQUIRED) .and() .requestMatchers().anyRequest() .and() .anonymous() .and() .authorizeRequests()// .antMatchers(\"/product/**\").access(\"#oauth2.hasScope('select') and hasRole('ROLE_USER')\") .antMatchers(\"/order/**\").authenticated();//配置order访问控制，必须认证过后才可以访问 // @formatter:on &#125; &#125; @Configuration @EnableAuthorizationServer protected static class AuthorizationServerConfiguration extends AuthorizationServerConfigurerAdapter &#123; @Autowired AuthenticationManager authenticationManager; @Autowired RedisConnectionFactory redisConnectionFactory; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; //配置两个客户端,一个用于password认证一个用于client认证 clients.inMemory().withClient(\"client_1\") .resourceIds(DEMO_RESOURCE_ID) .authorizedGrantTypes(\"client_credentials\", \"refresh_token\") .scopes(\"select\") .authorities(\"client\") .secret(\"123456\") .and().withClient(\"client_2\") .resourceIds(DEMO_RESOURCE_ID) .authorizedGrantTypes(\"password\", \"refresh_token\") .scopes(\"select\") .authorities(\"client\") .secret(\"123456\"); &#125; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; endpoints .tokenStore(new RedisTokenStore(redisConnectionFactory)) .authenticationManager(authenticationManager); &#125; @Override public void configure(AuthorizationServerSecurityConfigurer oauthServer) throws Exception &#123; //允许表单认证 oauthServer.allowFormAuthenticationForClients(); &#125; &#125;&#125; 简单说下spring security oauth2的认证思路。 client模式，没有用户的概念，直接与认证服务器交互，用配置中的客户端信息去申请accessToken，客户端有自己的client_id,client_secret对应于用户的username,password，而客户端也拥有自己的authorities，当采取client模式认证时，对应的权限也就是客户端自己的authorities。 password模式，自己本身有一套用户体系，在认证时需要带上自己的用户名和密码，以及客户端的client_id,client_secret。此时，accessToken所包含的权限是用户本身的权限，而不是客户端的权限。 我对于两种模式的理解便是，如果你的系统已经有了一套用户体系，每个用户也有了一定的权限，可以采用password模式；如果仅仅是接口的对接，不考虑用户，则可以使用client模式。 配置spring security在spring security的版本迭代中，产生了多种配置方式，建造者模式，适配器模式等等设计模式的使用，spring security内部的认证flow也是错综复杂，在我一开始学习ss也产生了不少困惑，总结了一下配置经验：使用了springboot之后，spring security其实是有不少自动配置的，我们可以仅仅修改自己需要的那一部分，并且遵循一个原则，直接覆盖最需要的那一部分。这一说法比较抽象，举个例子。比如配置内存中的用户认证器。有两种配置方式 planA： 1234567@Beanprotected UserDetailsService userDetailsService()&#123; InMemoryUserDetailsManager manager = new InMemoryUserDetailsManager(); manager.createUser(User.withUsername(\"user_1\").password(\"123456\").authorities(\"USER\").build()); manager.createUser(User.withUsername(\"user_2\").password(\"123456\").authorities(\"USER\").build()); return manager;&#125; planB： 12345678910111213141516171819@Configuration@EnableWebSecuritypublic class SecurityConfiguration extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth.inMemoryAuthentication() .withUser(\"user_1\").password(\"123456\").authorities(\"USER\") .and() .withUser(\"user_2\").password(\"123456\").authorities(\"USER\"); &#125; @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception &#123; AuthenticationManager manager = super.authenticationManagerBean(); return manager; &#125;&#125; 你最终都能得到配置在内存中的两个用户，前者是直接替换掉了容器中的UserDetailsService，这么做比较直观；后者是替换了AuthenticationManager，当然你还会在SecurityConfiguration 复写其他配置，这么配置最终会由一个委托者去认证。如果你熟悉spring security，会知道AuthenticationManager和AuthenticationProvider以及UserDetailsService的关系，他们都是顶级的接口，实现类之间错综复杂的聚合关系…配置方式千差万别，但理解清楚认证流程，知道各个实现类对应的职责才是掌握spring security的关键。 下面给出我最终的配置： 123456789101112131415161718192021222324@Configuration@EnableWebSecuritypublic class SecurityConfiguration extends WebSecurityConfigurerAdapter &#123; @Bean @Override protected UserDetailsService userDetailsService()&#123; InMemoryUserDetailsManager manager = new InMemoryUserDetailsManager(); manager.createUser(User.withUsername(\"user_1\").password(\"123456\").authorities(\"USER\").build()); manager.createUser(User.withUsername(\"user_2\").password(\"123456\").authorities(\"USER\").build()); return manager; &#125; @Override protected void configure(HttpSecurity http) throws Exception &#123; // @formatter:off http .requestMatchers().anyRequest() .and() .authorizeRequests() .antMatchers(\"/oauth/*\").permitAll(); // @formatter:on &#125;&#125; 重点就是配置了一个UserDetailsService，和ClientDetailsService一样，为了方便运行，使用内存中的用户，实际项目中，一般使用的是数据库保存用户，具体的实现类可以使用JdbcDaoImpl或者JdbcUserDetailsManager。 获取token进行如上配置之后，启动springboot应用就可以发现多了一些自动创建的endpoints： 123456&#123;[/oauth/authorize]&#125;&#123;[/oauth/authorize],methods=[POST]&#123;[/oauth/token],methods=[GET]&#125;&#123;[/oauth/token],methods=[POST]&#125;&#123;[/oauth/check_token]&#125;&#123;[/oauth/error]&#125; 重点关注一下/oauth/token，它是获取的token的endpoint。启动springboot应用之后，使用http工具访问password模式： http://localhost:8080/oauth/token?username=user_1&amp;password=123456&amp;grant_type=password&amp;scope=select&amp;client_id=client_2&amp;client_secret=123456 响应如下：{&quot;access_token&quot;:&quot;950a7cc9-5a8a-42c9-a693-40e817b1a4b0&quot;,&quot;token_type&quot;:&quot;bearer&quot;,&quot;refresh_token&quot;:&quot;773a0fcd-6023-45f8-8848-e141296cb3cb&quot;,&quot;expires_in&quot;:27036,&quot;scope&quot;:&quot;select&quot;} client模式：http://localhost:8080/oauth/token?grant_type=client_credentials&amp;scope=select&amp;client_id=client_1&amp;client_secret=123456 响应如下：{&quot;access_token&quot;:&quot;56465b41-429d-436c-ad8d-613d476ff322&quot;,&quot;token_type&quot;:&quot;bearer&quot;,&quot;expires_in&quot;:25074,&quot;scope&quot;:&quot;select&quot;} 在配置中，我们已经配置了对order资源的保护，如果直接访问:http://localhost:8080/order/1会得到这样的响应:{&quot;error&quot;:&quot;unauthorized&quot;,&quot;error_description&quot;:&quot;Full authentication is required to access this resource&quot;}（这样的错误响应可以通过重写配置来修改） 而对于未受保护的product资源http://localhost:8080/product/1则可以直接访问，得到响应product id : 1 携带accessToken参数访问受保护的资源： 使用password模式获得的token:http://localhost:8080/order/1?access_token=950a7cc9-5a8a-42c9-a693-40e817b1a4b0，得到了之前匿名访问无法获取的资源：order id : 1 使用client模式获得的token:http://localhost:8080/order/1?access_token=56465b41-429d-436c-ad8d-613d476ff322，同上的响应order id : 1 我们重点关注一下debug后，对资源访问时系统记录的用户认证信息，可以看到如下的debug信息 password模式： client模式： 和我们的配置是一致的，仔细看可以发现两者的身份有些许的不同。想要查看更多的debug信息，可以选择下载demo代码自己查看，为了方便读者调试和验证，我去除了很多复杂的特性，基本实现了一个最简配置，涉及到数据库的地方也尽量配置到了内存中，这点记住在实际使用时一定要修改。 到这儿，一个简单的oauth2入门示例就完成了，一个简单的配置教程。token的工作原理是什么，它包含了哪些信息？spring内部如何对身份信息进行验证？以及上述的配置到底影响了什么？这些内容会放到后面的文章中去分析。 示例代码下载全部的代码可以在我的github上进行下载，项目使用springboot+maven构建：https://github.com/lexburner/oauth2-demo","categories":[{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","permalink":"http://lexburner.github.io/categories/Spring-Security-OAuth2/"}],"tags":[{"name":"Spring Security OAuth2","slug":"Spring-Security-OAuth2","permalink":"http://lexburner.github.io/tags/Spring-Security-OAuth2/"}]},{"title":"对于Spring Cloud Feign入门示例的一点思考","slug":"对于Spring Cloud Feign入门示例的一点思考","date":"2017-08-03T09:40:16.000Z","updated":"2017-08-22T04:26:14.457Z","comments":true,"path":"2017/08/03/对于Spring Cloud Feign入门示例的一点思考/","link":"","permalink":"http://lexburner.github.io/2017/08/03/对于Spring Cloud Feign入门示例的一点思考/","excerpt":"Spring Cloud FeignSpring Cloud Feign是一套基于Netflix Feign实现的声明式服务调用客户端。它使得编写Web服务客户端变得更加简单。我们只需要通过创建接口并用注解来配置它既可完成对Web服务接口的绑定。它具备可插拔的注解支持，包括Feign注解、JAX-RS注解。它也支持可插拔的编码器和解码器。Spring Cloud Feign还扩展了对Spring MVC注解的支持，同时还整合了Ribbon和Eureka来提供均衡负载的HTTP客户端实现。 分布式应用早在十几年前就开始出现，各自的应用运行在各自的tomcat，jboss一类的容器中，他们之间的相互调用变成了一种远程调用，而实现远程调用的方式很多。按照协议划分，可以有RPC，Webservice，http。不同的框架也对他们有了各自的实现，如dubbo(x)，motan就都是RPC框架，本文所要讲解的Feign便可以理解为一种http框架，用于分布式服务之间通过Http进行接口交互。说他是框架，有点过了，可以理解为一个http工具，只不过在spring cloud全家桶的体系中，它比httpclient，okhttp，retrofit这些http工具都要强大的多。 入门先用一个简单的例子，看看如何在项目中使用Feign。示例项目使用maven多module构建，采用springcloud的Dalston.SR1版本","text":"Spring Cloud FeignSpring Cloud Feign是一套基于Netflix Feign实现的声明式服务调用客户端。它使得编写Web服务客户端变得更加简单。我们只需要通过创建接口并用注解来配置它既可完成对Web服务接口的绑定。它具备可插拔的注解支持，包括Feign注解、JAX-RS注解。它也支持可插拔的编码器和解码器。Spring Cloud Feign还扩展了对Spring MVC注解的支持，同时还整合了Ribbon和Eureka来提供均衡负载的HTTP客户端实现。 分布式应用早在十几年前就开始出现，各自的应用运行在各自的tomcat，jboss一类的容器中，他们之间的相互调用变成了一种远程调用，而实现远程调用的方式很多。按照协议划分，可以有RPC，Webservice，http。不同的框架也对他们有了各自的实现，如dubbo(x)，motan就都是RPC框架，本文所要讲解的Feign便可以理解为一种http框架，用于分布式服务之间通过Http进行接口交互。说他是框架，有点过了，可以理解为一个http工具，只不过在spring cloud全家桶的体系中，它比httpclient，okhttp，retrofit这些http工具都要强大的多。 入门先用一个简单的例子，看看如何在项目中使用Feign。示例项目使用maven多module构建，采用springcloud的Dalston.SR1版本 1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Dalston.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 服务提供方在本例子中，使用两个应用模块，展示分布式应用中如何进行接口交互。restful-provider担任服务提供方，restful-consumer担任服务消费者。 restful-provider新建一个modulerestful-provider-app,模块中只需要写一个CalculateController.java即可 ​ 123456789101112131415@RestController@RequestMapping(\"/api\")public class CalculateController &#123; @PostMapping(\"/add\") public Integer add(@RequestParam Integer a,@RequestParam Integer b)&#123; return a+b; &#125; @PostMapping(\"/subtract\") public Integer subtract(@RequestParam Integer a,@RequestParam Integer b)&#123; return a-b; &#125;&#125; 配置文件application.yml： 12server: port: 7070 一个服务端就写好了，提供两个计算服务的接口，可以通过http访问 服务消费方 使用Feign编写消费方，在restful-consumer项目中，我们将接口的定义和消费者应用分成两个module，restful-consumer-api-definition和restful-consumer-app。 在接口定义模块中，只有一个Feign接口： 123456789@FeignClient(value = \"calculate\",path = \"/api\")public interface CalculateApi &#123; @PostMapping(path = \"/add\") Integer add(@RequestParam(\"a\") Integer a,@RequestParam(\"b\") Integer b); @PostMapping(path = \"/subtract\") Integer subtract(@RequestParam(\"a\") Integer a,@RequestParam(\"b\") Integer b);&#125; tip：@RequestParam中的参数值不能省略，否则会出现错误 restful-consumer-app依赖上面的restful-consumer-api-definition模块，并且启用Feign代理，自动生成一个远程调用。启动类配置： 123456789@EnableFeignClients(basePackages = &#123;\"sinosoftsh.consumer.api\"&#125;)@SpringBootApplicationpublic class ConsumerApp &#123; public static void main(String []args)&#123; SpringApplication.run(ConsumerApp.class,args); &#125;&#125; 使用@EnableFeignClients(basePackages = {&quot;sinosoftsh.consumer.api&quot;})扫描接口类所在的包，spring的容器中才会有代理实现类。 不要忘记配置消费者的相关属性，在application.yml中 12345678910111213server: port: 7080ribbon: eureka: enabled: falsecalculate: ribbon: listOfServers: localhost:7070logging: level: org.apache.http: trace 在CalculateApi 接口的定义中，我们使用了一个calculate作为服务名称，必须要在配置文件中配置calculate所在的ip地址才行，由于本文只是作为一个示例，所以没有使用注册中心，在配置中禁用了eureka。最后一行的日志配置，可以发现其实Feign内部其实使用的是现成的http工具：httpclient，okhttp3，可以通过配置替换实现 整体的项目结构如下： 图一 第一种依赖关系结构 再编写一个单元测试类，验证一下Feign是否被正确的配置了 12345678910111213@RestControllerpublic class ConsumerController &#123; @Autowired CalculateApi calculateApi; @RequestMapping(\"/test\") public String test() &#123; Integer result = calculateApi.add(1, 2); System.out.println(\"the result is \" + result); return \"success\"; &#125;&#125; 思考回顾一下我们入门实例，服务提供方使用的是一个RestController暴露计算服务，服务消费方使用http工具（Feign）进行远程调用，这再清晰不过了，也是符合软件设计的，因为Feign接口的定义是存在于消费方，所以是真正的松耦合。但是习惯了使用rpc共享接口的设计，我们也可以将接口定义在服务提供方，这样做的好处是，服务可能被多个消费者使用，不需要每个消费者都定义一次Feign接口。 图2 第二种依赖关系结构在restful-provider创建一个restful-provider-api-definition模块，将CalculateApi.java的定义迁移到服务提供方，相应的restful-provider-app也可以进行改造： 1234567891011121314151617@RestController@RequestMapping(\"/api\")public class CalculateController implements CalculateApi&#123;// @PostMapping(\"/add\") @Override public Integer add(@RequestParam Integer a,@RequestParam Integer b)&#123; return a+b; &#125;// @PostMapping(\"/subtract\") @Override public Integer subtract(@RequestParam Integer a,@RequestParam Integer b)&#123; return a-b; &#125;&#125; 因为接口的定义和服务提供方现在在一个限界上下文中，接口的定义同时也宣告了应该提供什么样的服务，所以直接继承CalculateApi。这里的理解比较绕，现在的设计中，CalculateApi在服务消费者和服务提供者中的定位是不一样的，服务消费者需要在启动类扫描CalculateApi所在的包，生成代理对象，远程调用；而在服务提供方则一定不能扫描CalculateApi所在的包，否则会污染容器中的CalculateApi实现类，要知道，CalculateController 之上有一个@RestController注解，意味着已经有一个本地代理实现了，我们也可以在服务提供方注入CalculateApi，便是进行的本地调用了，这符合我们的初衷：我自己的提供的服务，本地当然可以调用。在服务提供方的启动类上要额外注意@ComponentScan，@EnableFeignClients的扫描。 这样，当我们有多个消费者，只需要让他们配置Feign，并且引入服务提供方的接口定义，扫描，即可进行远程调用。有点类似于RPC的共享接口。 设计原则restful设计以语言无关，松耦合的优势著称。在Spring Cloud Feign的相关文档中有这样的描述： It is generally not advisable to share an interface between a server and a client. It introduces tight coupling, and also actually doesn’t work with Spring MVC in its current form (method parameter mapping is not inherited). 不建议使用上述改进后的共享接口的方式，并且警告我们，springmvc的注解在Feign接口中的定义和实现类中是不可继承的。关于这点，仁者见仁，智者见智。我们现在项目依旧是采用共享接口的方式，这样可以使得开发变得便捷，多个消费者不需要重复定义。 下面是关于耦合和共享接口的一些讨论： 1234https://github.com/spring-cloud/spring-cloud-netflix/issues/951https://github.com/spring-cloud/spring-cloud-netflix/issues/659https://github.com/spring-cloud/spring-cloud-netflix/issues/646https://jmnarloch.wordpress.com/2015/08/19/spring-cloud-designing-feign-client/ 注意事项 当接口定义中出现了实体类时，需要使用@RequestBody注解。多个实体类，则需要用一个大的vo对其进行包裹，要时刻记住，Feign接口最终是会转换成一次http请求。 接口定义中的注解和实现类中的注解要分别写一次，不能继承。 Feign调用一般配合eureka等注册中心使用，并且在客户端可以支持Hystrix机制，本文为了讲解共享接口这一设计，所以重心放在了Feign上，实际开发中，这些spring cloud的其他组件通常配套使用。 对http深入理解，在使用Feign时可以事半功倍。","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/tags/Spring-Cloud/"}]},{"title":"Re：从零开始的领域驱动设计","slug":"Re：从零开始的领域驱动设计","date":"2017-07-28T05:15:46.000Z","updated":"2017-08-22T05:41:09.257Z","comments":true,"path":"2017/07/28/Re：从零开始的领域驱动设计/","link":"","permalink":"http://lexburner.github.io/2017/07/28/Re：从零开始的领域驱动设计/","excerpt":"[TOC] 前言领域驱动的火爆程度不用我赘述，但是即便其如此得耳熟能详，但大多数人对其的认识，还只是停留在知道它的缩写是DDD，知道它是一种软件思想，或者知道它和微服务有千丝万缕的关系。Eric Evans对DDD的诠释是那么地惜字如金，而我所认识的领域驱动设计的专家又都是行业中的资深前辈，他们擅长于对软件设计进行高屋建瓴的论述，如果没有丰富的互联网从业经验，是不能从他们的分享中获取太多的营养的，可以用曲高和寡来形容。1000个互联网从业者，100个懂微服务，10个人懂领域驱动设计。 可能有很多和我一样的读者，在得知DDD如此火爆之后，尝试去读了开山之作《领域驱动设计——软件核心复杂性应对之道》，翻看了几张之后，晦涩的语句，不明所以的专业术语，加上翻译导致的语句流畅性，可以说观看体验并不是很好，特别是对于开发经验不是很多的读者。我总结了一下，为何这本书难以理解： 没有阅读软件设计丛书的习惯，更多人偏向于阅读偏应用层面的书籍，“talk is cheap，show me the code”往往更符合大多数人的习惯。2.没有太多的开发经验支撑。没有踩过坑，就不会意识到设计的重要性，无法产生共情。3.年代有些久远，这本书写于2004年，书中很多软件设计的反例，在当时是非常流行的，但是在现在已经基本绝迹了。大师之所以为大师，是因为其能跨越时代的限制，预见未来的问题，这也是为什么DDD在十几年前就被提出，却在微服务逐渐流行的现阶段才被大家重视。 诚然如标题所示，本文是领域驱动设计的一个入门文章，或者更多的是一个个人理解的笔记，笔者也正在学习DDD的路上，可能会有很多的疏漏。如有理解有偏颇的地方，还望各位指摘。","text":"[TOC] 前言领域驱动的火爆程度不用我赘述，但是即便其如此得耳熟能详，但大多数人对其的认识，还只是停留在知道它的缩写是DDD，知道它是一种软件思想，或者知道它和微服务有千丝万缕的关系。Eric Evans对DDD的诠释是那么地惜字如金，而我所认识的领域驱动设计的专家又都是行业中的资深前辈，他们擅长于对软件设计进行高屋建瓴的论述，如果没有丰富的互联网从业经验，是不能从他们的分享中获取太多的营养的，可以用曲高和寡来形容。1000个互联网从业者，100个懂微服务，10个人懂领域驱动设计。 可能有很多和我一样的读者，在得知DDD如此火爆之后，尝试去读了开山之作《领域驱动设计——软件核心复杂性应对之道》，翻看了几张之后，晦涩的语句，不明所以的专业术语，加上翻译导致的语句流畅性，可以说观看体验并不是很好，特别是对于开发经验不是很多的读者。我总结了一下，为何这本书难以理解： 没有阅读软件设计丛书的习惯，更多人偏向于阅读偏应用层面的书籍，“talk is cheap，show me the code”往往更符合大多数人的习惯。2.没有太多的开发经验支撑。没有踩过坑，就不会意识到设计的重要性，无法产生共情。3.年代有些久远，这本书写于2004年，书中很多软件设计的反例，在当时是非常流行的，但是在现在已经基本绝迹了。大师之所以为大师，是因为其能跨越时代的限制，预见未来的问题，这也是为什么DDD在十几年前就被提出，却在微服务逐渐流行的现阶段才被大家重视。 诚然如标题所示，本文是领域驱动设计的一个入门文章，或者更多的是一个个人理解的笔记，笔者也正在学习DDD的路上，可能会有很多的疏漏。如有理解有偏颇的地方，还望各位指摘。 认识领域驱动设计的意义领域驱动设计并不会绝对地提高项目的开发效率。 图1：复杂性与开发周期关系遵循领域驱动设计的规范使得项目初期的开发甚至不如不使用它来的快，原因有很多，程序员的素质，代码的规范，限界上下文的划分…甚至需求修改后导致需要重新建模。但是遵循领域驱动设计的规范，在项目越来越复杂之后，可以不至于让项目僵死。这也是为什么很多系统不断迭代着，最终就黄了。书名的副标题“软件核心复杂性应对之道”正是阐释了这一点## 模式： smart ui是个反模式可能很多读者还不知道smart ui是什么，但是在这本书写作期间，这种设计风格是非常流行的。在与一位领域驱动设计方面的资深专家的交谈中，他如下感慨到软件发展的历史：&gt;2003年时，正是delphi，vb一类的smart ui程序大行其道，java在那个年代，还在使用jsp来完成大量的业务逻辑操作，4000行的jsp是常见的事；2005年spring hibernate替换了EJB，社区一片欢呼，所有人开始拥护action，service，dao这样的贫血模型（充血模型，贫血模型会在下文论述）；2007年，Rails兴起，有人发现了Rails的activeRecord是涨血模型，引起了一片混战；直到现在的2017年，微服务成为主流系统架构。在现在这个年代，不懂个MVC分层，都不好意思说自己是搞java的，也不会有人在jsp里面写业务代码了（可以说模板技术freemarker,thymeleaf已经取代jsp了），但是在那个年代，还没有现在这么普遍地强调分层架构的重要性。这个章节其实并不重要，因为mvc一类的分层架构已经是大多数java初学者的“起点”了，大多数DDD的文章都不会赘述这一点，我这里列出来是为了让大家知晓这篇文章的时代局限性，在后续章节的理解中，也需要抱有这样的逻辑：这本书写于2004年。## 模式： Entity与Value Object我在不了解DDD时，就对这两个术语早有耳闻。entity又被称为reference object，我们通常所说的java bean在领域中通常可以分为这两类，（可别把value object和常用于前台展示的view object，vo混为一谈）entity的要义在于生命周期和标识，value object的要义在于无标识，通常情况下，entity在通俗意义上可以理解为数据库的实体，（不过不严谨），value object则一般作为一个单独的类，构成entity的一个属性。举两个例子来加深对entity和value object的理解。例1：以电商微服务系统中的商品模块，订单模块为例。将整个电商系统划分出商品和订单两个限界上下文（Bound Context）应该是没有争议的。如果是传统的单体应用，我们可以如何设计这两个模块的实体类呢？会不会是这样？1234567891011121314151617181920212223class Product&#123; String id;//主键 String skuId;//唯一识别号 String productName; Bigdecimal price; Category category;//分类 List&lt;Specification&gt; specifications;//规格 ... &#125;class Order&#123; String id;//主键 String orderNo;//订单号 List&lt;OrderItem&gt; orderItems;//订单明细 BigDecimal orderAmount;//总金额 ...&#125;class OrderItem&#123; String id; Product product;//关联商品 BigDecimal snapshotPrice;//下单时的价格&#125;看似好像没问题，考虑到了订单要保存下单时候的价格（当然，这是常识）但这么设计却存在诸多的问题。在分布式系统中，商品和订单这两个模块必然不在同一个模块，也就意味着不在同一个网段中。上述的类设计中直接将Product的列表存储到了Order中，也就是一对多的外键关联。这会导致，每次访问订单的商品列表，都需要发起n次远程调用。反思我们的设计，其实我们发现，订单BC的Product和商品BC的Product其实并不是同一个entity，在商品模块中，我们更关注商品的规格，种类，实时价格，这最直接地反映了我们想要买什么的欲望。而当生成订单后，我们只关心这个商品买的时候价格是多少，不会关心这个商品之后的价格变动，还有他的名称，仅仅是方便我们在订单的商品列表中定位这个商品。如何改造就变得明了了12345678class OrderItem&#123; String id; String productId;//只记录一个id用于必要的时候发起command操作 String skuId; String productName; ... BigDecimal snapshotPrice;//下单时的价格&#125;是的，我们做了一定的冗余，这使得即使商品模块的商品，名称发生了微调，也不会被订单模块知晓。这么做也有它的业务含义，用户会声称：我买的时候他的确就叫这个名字。记录productId和skuId的用意不是为了查询操作，而是方便申请售后一类的命令操作（command）。在这个例子中，Order 和 Product都是entity，而OrderItem则是value object（想想之前的定义，OrderItem作为一个类，的确是描述了Order这个entity的一个属性集合）。关于标识，我的理解是有两层含义，第一个是作为数据本身存储于数据库，主键id是一个标识，第二是作为领域对象本身，orderNo是一个标识，对于人而言，身份证是一个标识。而OrderItem中的productId，id不能称之为标识，因为整个OrderItem对象是依托于Order存在的，Order不存在，则OrderItem没有意义。例子2： 汽车和轮胎的关系是entity和value object吗？这个例子其实是一个陷阱题，因为他没有交代限界上下文（BC），场景不足以判断。对于用户领域而言，的确可以成立，汽车报废之后，很少有人会关心轮胎。轮胎和发动机，雨刮器，座椅地位一样，只是构成汽车的一些部件，和用户最紧密相关的，只有汽车这个entity，轮胎只是描述这个汽车的属性（value object）；场景切换到汽修厂，无论是汽车，还是轮胎，都是汽修厂密切关心的，每个轮胎都有自己的编号，一辆车报废了，可以安置到其他车上，这里，他们都是entity。这个例子是在说明这么一个道理，同样的事物，在不同的领域中，会有不同的地位。图2：《领域驱动设计》Value Object模式的示例 在单体应用中，可能会有人指出，这直接违背了数据库范式，但是领域驱动设计的思想正如他的名字那样，不是基于数据库的，而是基于领域的。微服务使得数据库发生了隔离，这样的设计思想可以更好的指导我们优化数据库。 模式： Repository 哲学家分析自然规律得出规范，框架编写者根据规范制定框架。有些框架，可能大家一直在用，但是却不懂其中蕴含的哲学。 ——来自于笔者的口胡 记得在刚刚接触mvc模式，常常用DAO层表示持久化层，在JPA+springdata中，抽象出了各式各样的xxxRepository，与DDD的Repository模式同名并不是巧合，jpa所表现出的正是一个充血模型（如果你遵循正确的使用方式的话），可以说是领域驱动设计的一个最佳实践。 开宗明义，在Martin Fowler理论中，有四种领域模型： 失血模型 贫血模型 充血模型 胀血模型详细的概念区别不赘述了，可以参见专门讲解4种模型的博客。他们在数据库开发中分别有不同的实现，用一个修改用户名的例子来分析。12345class User&#123; String id; String name; Integer age;&#125; 失血模型：跳过，可以理解为所有的操作都是直接操作数据库，在smart ui中可能会出现这样的情况。 贫血模型：123456789101112131415161718class UserDao &#123; @Autowired JdbcTemplate jdbcTemplate; public void updateName(String name,String id)&#123; jdbcTemplate.excute(\"update user u set u.name = ? where id=?\",name,id); &#125;&#125;class UserService&#123; @Autowired UserDao userDao; void updateName(String name,String id)&#123; userDao.updateName(name,id); &#125; &#125; 贫血模型中，dao是一类sql的集合，在项目中的表现就是写了一堆sql脚本，与之对应的service层，则是作为Transaction Script的入口。观察仔细的话，会发现整个过程中user对象都没出现过。 充血模型：1234567891011121314interface UserRepository extends JpaRepository&lt;User,String&gt;&#123; //springdata-jpa自动扩展出save findOne findAll方法&#125;class UserService&#123; @Autowoird UserRepository userRepository; void updateName(String name,String id)&#123; User user = userRepository.findOne(id); user.setName(name); userRepository.save(user); &#125;&#125; 充血模型中，整个修改操作是“隐性”的，对内存中user对象的修改直接影响到了数据库最终的结果，不需要关心数据库操作，只需要关注领域对象user本身。Repository模式就是在于此，屏蔽了数据库的实现。与贫血模型中user对象恰恰相反，整个流程没有出现sql语句。 涨血模型：没有具体的实现，可以这么理解：12345void updateName(String name,String id)&#123; User user = new User(id); user.setName(name); user.save();&#125; 我们在Repository模式中重点关注充血模型。为什么前面说：如果你遵循正确的使用方式的话，springdata才是对DDD的最佳实践呢？因为有的使用者会写出下面的代码：1234567interface UserRepository extends JpaRepository&lt;User,String&gt;&#123; @Query(\"update user set name=? where id=?\") @Modifying(clearAutomatically = true) @Transactional void updateName(String name,String id);&#125; 历史的车轮在滚滚倒退。本节只关注模型本身，不讨论使用中的一些并发问题，再来聊聊其他的一些最佳实践。1234567interface UserRepository extends JpaRepository&lt;User,String&gt;&#123; User findById();//√ 然后已经存在findOne了，只是为了做个对比 User findBy身份证号();//可以接受 User findBy名称();//× List&lt;权限&gt; find权限ByUserId();//×&#125; 理论上，一个Repository需要且仅需要包含三类方法loadBy标识，findAll，save（一般findAll（）就包含了分页，排序等多个方法，算作一类方法）。标识的含义和前文中entity的标识是同一个含义，在我个人的理解中，身份证可以作为一个用户的标识（这取决于你的设计，同样的逻辑还有订单中有业务含义的订单编号，保单中的投保单号等等），在数据库中，id也可以作为标识。findBy名称为什么不值得推崇，因为name并不是User的标识，名字可能会重复，只有在特定的现场场景中，名字才能具体对应到人。那应该如何完成“根据姓名查找可能的用户”这一需求呢？最方便的改造是使用Criteria，Predicate来完成视图的查询，哪怕只有一个非标识条件。在更完善的CQRS架构中，视图的查询则应该交由专门的View层去做，可以是数据库，可以是ES。findByUserId不值得推崇则是因为他违背了聚合根模式（下文会介绍），User的Repository只应该返回User对象。 软件设计初期，你是不是还在犹豫：是应该先设计数据库呢，还是应该设计实体呢？在Domain-Driven的指导下，你应当放弃Data-Driven。 模式 聚合和聚合根难住我的还有英文单词，初识这个概念时，忍不住发问：Aggregate是个啥。文中使用聚合的概念，来描述对象之间的关联，采用合适的聚合策略，可以避免一个很长，很深的对象引用路径。对划分模块也有很大的指导意义。 在微服务中我们常说划分服务模块，在领域驱动设计中，我们常说划分限界上下文。在面向对象的世界里，用抽象来封装模型中的引用，聚合就是指一组相关对象的集合，我们把它作为数据修改的单元。每个聚合都有一个聚合根(root)和一个边界(boundary)。边界定义了聚合内部有什么，而根则是一个特定的entity，两个聚合之间，只允许维护根引用，只能通过根引用去向深入引用其他引用变量。 例子还是沿用电商系统中的订单和商品模块。在聚合模式中，订单不能够直接关联到商品的规格信息，如果一定要查询，则应该通过订单关联到的商品，由商品去访问商品规格。在这个例子中，订单和商品分别是两个边界，而订单模块中的订单entity和商品模块中的商品entity就是分别是各自模块的root。遵循这个原则，可以使我们模块关系不那么的盘根错节，这也是众多领域驱动文章中不断强调的划分限界上下文是第一要义。 模式 包结构微服务有诸多的模块，而每个模块并不一定是那么的单一职责，比模块更细的分层，便是包的分层。我在阅读中，隐隐觉得这其中蕴含着一层哲学，但是几乎没有文章尝试解读它。领域驱动设计将其单独作为了一个模式进行了论述，篇幅不小。重点就是论述了一个思想：包结构应当具有高内聚性。 这次以一个真实的案例来介绍一下对高内聚的包结构的理解，项目使用maven多module搭建。我曾经开发过一个短信邮件平台模块，它在整个微服务系统中有两个职责，一：负责为其他模块提供短信邮件发送的远程调用接口，二：有一个后台页面，可以让管理员自定义发送短信，并且可以浏览全部的一，二两种类型发送的短信邮件记录。 在设计包结构之前，先是设计微服务模块。| module名 | 说明 | package类型 | 顶级包名 || ——- | ————— | ————– | ———————— || api | api接口定义，用于暴露服务 | jar | sinosoftgz.message.api || app | api实现者，真正的服务提供者 | executable jar | sinosoftgz.message.app || admin | 管理端应用 | executable jar | sinosoftgz.message.admin || model | 实体 | jar | sinosoftgz.message.model |api层定义了一系列的接口和接口依赖的一些java bean，model层也就是我们的领域层。这两个模块都会打成jar包，外部服务依赖api，api则由app模块使用rpc框架实现远程调用。admin和app连接同一个数据源，可以查询出短信邮件记录，admin需要自定义发送短信也是通过rpc调用。简单介绍完了这个项目后，重点来分析下需求，来看看如何构建包结构。mvc分层天然将controller，service，model，config层分割开，这符合DDD所推崇的分层架构模式（这个模式在原文中有描述，但我觉得和现在耳熟能详的分层结构没有太大的出入，所以没有放到本文中介绍），而我们的业务需求也将短信和邮件这两个领域拆分开了。那么，到底是mvc应该包含业务包结构呢？还是说业务包结构包含mvc呢？ mvc高于业务分层123456789101112131415161718192021//不够好的分层sinosoftgz.message.admin config CommonConfig.java service CommonService.java mail MailTemplateService.java MailMessageService.java sms SmsTemplateService.java SmsMessageService.java web IndexController.java mail MailTemplateController.java MailMessageController.java sms SmsTemplateController.java SmsMessageController.java MessageAdminApp.java 业务分层包含mvc123456789101112131415161718192021222324252627//高内聚的分层sinosoftgz.message.admin config CommonConfig.java service CommonService.java web IndexController.java mail config MailConfig.java service MailTemplateService.java MailMessageService.java web MailTemplateController.java MailMessageController.java sms config Smsconfig.java service SmsTemplateService.java SmsMessageService.java web SmsTemplateController.java SmsMessageController.java MessageAdminApp.java 业务并不是特别复杂，但应该可以发现第二种（业务分层包含mvc）的包结构，才是一种高内聚的包结构。第一种分层会让人有一种将各个业务模块（如mail和sms）的service和controller隔离开了的感觉，当模块更多，每个模块的内容更多，这个“隔得很远”的不适感会逐渐侵蚀你的开发速度。一种更加低内聚的反例是不用包分层，仅仅依赖前缀区分，由于在项目开发中真的发现同事写出了这样的代码，我觉得还是有必要拿出来说一说：12345678910111213141516171819//反例sinosoftgz.message.admin config CommonConfig.java MailConfig.java Smsconfig.java service CommonService.java MailTemplateService.java MailMessageService.java SmsTemplateService.java SmsMessageService.java web IndexController.java MailTemplateController.java MailMessageController.java SmsTemplateController.java SmsMessageController.java MessageAdminApp.java 这样的设计会导致web包越来越庞大，逐渐变得臃肿，是什么使项目僵化，项目经理为何一看到代码就头疼，规范的高内聚的包结构，遵循业务&gt;mvc的原则，可以知道我们的项目庞大却有条理。 其他模式《领域驱动设计》这本书介绍了众多的模式，上面只是介绍了一部分重要的模式，后续我会结合各个模式，尽量采用最佳实践+浅析设计的方式来解读。 微服务之于领域驱动设计的一点思考技术架构诚然重要，但不可忽视领域拆解和业务架构，《领域驱动设计》中的诸多失败，成功案例的总结，是支撑其理论知识的基础，最终汇聚成众多的模式。在火爆的微服务架构潮流下，我也逐渐意识到微服务不仅仅是技术的堆砌，更是一种设计，一门艺术。我的本科论文本想就微服务架构进行论述，奈何功底不够，最后只能改写成一篇分布式网站设计相关的文章，虽然是一个失败的过程，但让我加深了对微服务的认识。如今结合领域驱动设计，更加让我确定，技术方案始终有代替方案，决定微服务的不是框架的选择，不仅仅是restful或者rpc的接口设计风格的抉择，而更应该关注拆解，领域，限界上下文，聚合根等等一系列事物，这便是我所理解的领域驱动设计对微服务架构的指导意义。 参考文章多研究些架构，少谈些框架—-曹祖鹏 DDD领域驱动设计基本理论知识总结 - netfocus","categories":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"http://lexburner.github.io/categories/领域驱动设计/"}],"tags":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"http://lexburner.github.io/tags/领域驱动设计/"}]},{"title":"spring中的懒加载与事务--排坑记录","slug":"spring中的懒加载与事务--排坑记录","date":"2017-06-23T05:37:41.000Z","updated":"2017-09-05T09:35:32.017Z","comments":true,"path":"2017/06/23/spring中的懒加载与事务--排坑记录/","link":"","permalink":"http://lexburner.github.io/2017/06/23/spring中的懒加载与事务--排坑记录/","excerpt":"案例描述本文主要描述了开发中常见的几个与spring懒加载和事务相关的案例，描述常见的使用场景，以及如何规避他们，给出具体的代码。 在新的线程中，访问某个持久化对象的懒加载属性。 在quartz定时任务中，访问某个持久化对象的懒加载属性。 在dubbo，motan一类rpc框架中，远程调用时服务端session关闭的问题。 上面三个案例，其实核心都是一个问题，就是牵扯到spring对事务的管理，而懒加载这个技术，只是比较容易体现出事务出错的一个实践，主要用它来引发问题，进而对问题进行思考。","text":"案例描述本文主要描述了开发中常见的几个与spring懒加载和事务相关的案例，描述常见的使用场景，以及如何规避他们，给出具体的代码。 在新的线程中，访问某个持久化对象的懒加载属性。 在quartz定时任务中，访问某个持久化对象的懒加载属性。 在dubbo，motan一类rpc框架中，远程调用时服务端session关闭的问题。 上面三个案例，其实核心都是一个问题，就是牵扯到spring对事务的管理，而懒加载这个技术，只是比较容易体现出事务出错的一个实践，主要用它来引发问题，进而对问题进行思考。 前期准备为了能直观的暴露出第一个案例的问题，我新建了一个项目，采用传统的mvc分层，一个student.java实体类，一个studentDao.java持久层，一个studentService.java业务层，一个studentController控制层。 12345678910@Entity@Table(name = \"student\")public class Student &#123; @Id @GeneratedValue(strategy = GenerationType.AUTO) private Integer id; private String name; getter..setter..&#125; 持久层使用springdata，框架自动扩展出CURD方法12public interface StudentDao extends JpaRepository&lt;Student, Integer&gt;&#123;&#125; service层，先给出普通的调用方法。用于错误演示。1234567891011@Servicepublic class StudentService &#123; @Autowired StudentDao studentDao; public void testNormalGetOne()&#123; Student student = studentDao.getOne(1); System.out.println(student.getName()); &#125;&#125; 注意：getOne和findOne都是springdata提供的根据id查找单个实体的方法，区别是前者是懒加载，后者是立即加载。我们使用getOne来进行懒加载的实验，就不用大费周章去写懒加载属性，设置多个实体类了。 controller层，不是简简单单的调用，而是在新的线程中调用。使用controller层来代替单元测试（实际项目中，通常使用controller调用service，然后在浏览器或者http工具中调用触发，较为方便）1234567891011@RequestMapping(\"/testNormalGetOne\")@ResponseBodypublic String testNormalGetOne() &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; studentService.testNormalGetOne(); &#125; &#125;).start(); return \"testNormalGetOne\";&#125; 启动项目后，访问localhost:8080/testNormalGetOne报错如下：1Exception in thread \"Thread-6\" org.hibernate.LazyInitializationException: could not initialize proxy - no Session 问题分析no session说明了什么？道理很简单，因为spring的session是和线程绑定的，在整个model-&gt;dao-&gt;service-&gt;controller的调用链中，这种事务和线程绑定的机制非常契合。而我们出现的问题正式由于新开启了一个线程，这个线程与调用链的线程不是同一个。 问题解决我们先使用一种不太优雅的方式解决这个问题。在新的线程中，手动打开session。 12345678910public void testNormalGetOne() &#123; EntityManagerFactory entityManagerFactory = ApplicationContextProvider.getApplicationContext().getBean(EntityManagerFactory.class); EntityManager entityManager = entityManagerFactory.createEntityManager(); EntityManagerHolder entityManagerHolder = new EntityManagerHolder(entityManager); TransactionSynchronizationManager.bindResource(entityManagerFactory, entityManagerHolder); Student student = studentDao.getOne(1); System.out.println(student.getName()); TransactionSynchronizationManager.unbindResource(entityManagerFactory); EntityManagerFactoryUtils.closeEntityManager(entityManager);&#125; 由于我们使用了JPA，所以事务是由EntityManagerFactory这个工厂类生成的EntityManager来管理的。TransactionSynchronizationManager.bindResource(entityManagerFactory, entityManagerHolder);这个方法使用事务管理器绑定session。而ApplicationContextProvider这个工具类是用来获取spring容器中的EntityManagerFactory的，为什么不用注入的方式，下文讲解。它的代码如下：12345678910111213public class ApplicationContextProvider implements ApplicationContextAware &#123; private static ApplicationContext context = null; public static ApplicationContext getApplicationContext() &#123; return context; &#125; @Override public void setApplicationContext(ApplicationContext ac) throws BeansException &#123; context = ac; &#125;&#125; 问题暂时得到了解决。 问题再思考我们一般情况下使用懒加载属性，为什么没有出现no session的问题呢？相信大家都知道@Transactional这个注解，他会帮我们进行事务包裹，当然也会绑定session；以及大家熟知的hiberbate中的OpenSessionInterceptor和OpenSessionInViewFilter以及jpa中的OpenEntityManagerInViewInterceptor都是在没有session的情况下，打开session的过滤器。这种方法开始前依赖事务开启，方法结束后回收资源的操作，非常适合用过滤器拦截器处理，后续的两个未讲解的案例，其实都是使用了特殊的过滤器。 看一下官方文档如何描述这个jpa中的过滤器的： 29.3.4 Open EntityManager in View If you are running a web application, Spring Boot will by default register OpenEntityManagerInViewInterceptor to apply the “Open EntityManager in View” pattern, i.e. to allow for lazy loading in web views. If you don’t want this behavior you should set spring.jpa.open-in-view to false in your application.properties. 我们尝试着关闭这个过滤器：配置application.properties/application.yml文件1spring.jpa.open-in-view=false 再使用正常的方式访问懒加载属性（而不是在一个新的线程中）： 1234567891011 @RequestMapping(\"/testNormalGetOne\") @ResponseBody public String testNormalGetOne() &#123;// new Thread(new Runnable() &#123;// @Override// public void run() &#123; studentService.testNormalGetOne();// &#125;// &#125;).start(); return \"testNormalGetOne\"; &#125; 报错如下： 1&#123;\"timestamp\":1498194914012,\"status\":500,\"error\":\"Internal Server Error\",\"exception\":\"org.hibernate.LazyInitializationException\",\"message\":\"could not initialize proxy - no Session\",\"path\":\"/testNormalGetOne\"&#125; 是的，我们使用spring的controller作为单元测试时，以及我们平时在直接使用jpa的懒加载属性时没有太关注这个jpa的特性，因为springboot帮我们默认开启了这个过滤器。这也解释了，为什么在新的线程中，定时任务线程中，rpc远程调用时session没有打开的原因，因为这些流程没有经过springboot的web调用链。 另外两个实战案例上文已经阐释了，为什么quartz定时任务中访问懒加载属性，rpc框架服务端访问懒加载属性（注意不是客户端，客户端访问懒加载属性那是一种作死的行为，因为是代理对象）为出现问题。我们仿照spring打开session的思路（这取决于你使用hibernate还是jpa，抑或是mybatis），来编写我们的过滤器。 quartz中打开session：使用quartz提供的JobListenerSupport支持，编写一个任务过滤器，用于在每次任务执行时打开session1234567891011121314151617181920212223242526272829303132public class OpenEntityManagerJobListener extends JobListenerSupport implements ApplicationContextAware &#123; @Override public String getName() &#123; return \"OpenEntityManagerJobListener\"; &#125; EntityManagerFactory entityManagerFactory; @Override public void jobToBeExecuted(JobExecutionContext context) &#123; entityManagerFactory = applicationContext.getBean(EntityManagerFactory.class); EntityManager entityManager = entityManagerFactory.createEntityManager(); EntityManagerHolder emHolder = new EntityManagerHolder(entityManager); TransactionSynchronizationManager.bindResource(entityManagerFactory, emHolder); &#125; @Override public void jobWasExecuted(JobExecutionContext context, JobExecutionException jobException) &#123; EntityManagerHolder emHolder = (EntityManagerHolder) TransactionSynchronizationManager.unbindResource(entityManagerFactory); EntityManagerFactoryUtils.closeEntityManager(emHolder.getEntityManager()); &#125; ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; if(this.applicationContext ==null) throw new RuntimeException(\"applicationContext is null\"); &#125;&#125; 配置调度工厂： 12345678//调度工厂 @Bean public SchedulerFactoryBean schedulerFactoryBean() &#123; SchedulerFactoryBean factoryBean = new SchedulerFactoryBean(); factoryBean.setTriggers(triggerFactoryBeans().getObject()); factoryBean.setGlobalJobListeners(openEntityManagerJobListener()); return factoryBean; &#125; 也可以参考我的另一篇描述更为细致的文章(解决Quartz定时器中查询懒加载数据no session的问题)，那是我还是刚刚参加工作，可能有些许疏漏之处，不过参考是够了。 Motan（我现在使用的rpc框架）服务端打开session利用了motan对spi扩展的支持，编写了一个Filter，主要参考了motan的spi过滤器写法和springdata打开session/entityManager的思路。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158@SpiMeta(name = \"openjpasession\")@Activation(sequence = 100)public class OpenEntityManagerInMotanFilter implements Filter &#123; private Logger logger = LoggerFactory.getLogger(OpenEntityManagerInMotanFilter.class); /** * Default EntityManagerFactory bean name: \"entityManagerFactory\". * Only applies when no \"persistenceUnitName\" param has been specified. * * @see #setEntityManagerFactoryBeanName * @see #setPersistenceUnitName */ public static final String DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME = \"entityManagerFactory\"; private String entityManagerFactoryBeanName; private String persistenceUnitName; private volatile EntityManagerFactory entityManagerFactory; /** * Set the bean name of the EntityManagerFactory to fetch from Spring's * root application context. * &lt;p&gt;Default is \"entityManagerFactory\". Note that this default only applies * when no \"persistenceUnitName\" param has been specified. * * @see #setPersistenceUnitName * @see #DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME */ public void setEntityManagerFactoryBeanName(String entityManagerFactoryBeanName) &#123; this.entityManagerFactoryBeanName = entityManagerFactoryBeanName; &#125; /** * Return the bean name of the EntityManagerFactory to fetch from Spring's * root application context. */ protected String getEntityManagerFactoryBeanName() &#123; return this.entityManagerFactoryBeanName; &#125; /** * Set the name of the persistence unit to access the EntityManagerFactory for. * &lt;p&gt;This is an alternative to specifying the EntityManagerFactory by bean name, * resolving it by its persistence unit name instead. If no bean name and no persistence * unit name have been specified, we'll check whether a bean exists for the default * bean name \"entityManagerFactory\"; if not, a default EntityManagerFactory will * be retrieved through finding a single unique bean of type EntityManagerFactory. * * @see #setEntityManagerFactoryBeanName * @see #DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME */ public void setPersistenceUnitName(String persistenceUnitName) &#123; this.persistenceUnitName = persistenceUnitName; &#125; /** * Return the name of the persistence unit to access the EntityManagerFactory for, if any. */ protected String getPersistenceUnitName() &#123; return this.persistenceUnitName; &#125; /** * Look up the EntityManagerFactory that this filter should use. * &lt;p&gt;The default implementation looks for a bean with the specified name * in Spring's root application context. * * @return the EntityManagerFactory to use * @see #getEntityManagerFactoryBeanName */ protected EntityManagerFactory lookupEntityManagerFactory() &#123; String emfBeanName = getEntityManagerFactoryBeanName(); String puName = getPersistenceUnitName(); if (StringUtils.hasLength(emfBeanName)) &#123; return ApplicationContextProvider.getApplicationContext().getBean(emfBeanName, EntityManagerFactory.class); &#125; else if (!StringUtils.hasLength(puName) &amp;&amp; ApplicationContextProvider.getApplicationContext().containsBean(DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME)) &#123; return ApplicationContextProvider.getApplicationContext().getBean(DEFAULT_ENTITY_MANAGER_FACTORY_BEAN_NAME, EntityManagerFactory.class); &#125; else &#123; // Includes fallback search for single EntityManagerFactory bean by type. return EntityManagerFactoryUtils.findEntityManagerFactory(ApplicationContextProvider.getApplicationContext(), puName); &#125; &#125; /** * Create a JPA EntityManager to be bound to a request. * &lt;p&gt;Can be overridden in subclasses. * * @param emf the EntityManagerFactory to use * @see javax.persistence.EntityManagerFactory#createEntityManager() */ protected EntityManager createEntityManager(EntityManagerFactory emf) &#123; return emf.createEntityManager(); &#125; @Override public Response filter(Caller&lt;?&gt; caller, Request request) &#123; if (!(caller instanceof Provider)) &#123; return caller.call(request); &#125; EntityManagerFactory emf = null; try &#123; emf = lookupEntityManagerFactory(); &#125; catch (Exception e) &#123; logger.error(e.getMessage(), e); &#125; //可能没有启用openjpa if (emf == null) &#123; return caller.call(request); &#125; try &#123; //如果没有绑定，绑定到当前线程 if (TransactionSynchronizationManager.getResource(emf) == null) &#123; EntityManager em = createEntityManager(emf); EntityManagerHolder emHolder = new EntityManagerHolder(em); TransactionSynchronizationManager.bindResource(emf, emHolder); &#125; &#125; catch (Exception e) &#123; logger.error(e.getLocalizedMessage(), e); &#125; try &#123; return caller.call(request); &#125; finally &#123; //解除绑定 closeManager(emf); &#125; &#125; /** * 关闭 emf * * @param emf */ private void closeManager(EntityManagerFactory emf) &#123; if (emf == null || TransactionSynchronizationManager.getResource(emf) == null) &#123; return; &#125; EntityManagerHolder emHolder = null; try &#123; emHolder = (EntityManagerHolder) TransactionSynchronizationManager.unbindResource(emf); &#125; catch (IllegalStateException e) &#123; logger.error(e.getLocalizedMessage(), e); &#125; try &#123; if (emHolder != null) &#123; EntityManagerFactoryUtils.closeEntityManager(emHolder.getEntityManager()); &#125; &#125; catch (Exception e) &#123; logger.error(e.getLocalizedMessage(), e); &#125; &#125;&#125; 总结springboot中的事务管理做的永远比我们想的多，事务管理器的使用场景，@Transactional究竟起了哪些作用，以及spring-data这个对DDD最佳的阐释，以及mybatis一类的非j2ee规范在微服务的地位中是否高于jpa，各个层次之间的实体传输，消息传递…都是值得思考的。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"},{"name":"事务","slug":"事务","permalink":"http://lexburner.github.io/tags/事务/"}]},{"title":"使用zipkin做分布式链路监控","slug":"使用zipkin做分布式链路监控","date":"2017-06-11T18:51:51.000Z","updated":"2017-08-22T05:53:51.547Z","comments":true,"path":"2017/06/12/使用zipkin做分布式链路监控/","link":"","permalink":"http://lexburner.github.io/2017/06/12/使用zipkin做分布式链路监控/","excerpt":"介绍 Zipkin 为一个分布式的调用链跟踪系统( distributed tracing system ) ,设计来源于 google dapper paper 官方网站 快速入门 安装方式一：使用zipkin官方提供的jar启动服务zipkin官方提供了一个现成的使用springboot写的zipkin服务端，客户端的链路监控报告可以通过多种方式（下文会讲解具体的方式）向服务端发送报告。 系统需要安装java8 下载地址 配置详解查看源码可知其有4种持久化方式，本文选择使用最熟悉的mysql持久化链路调用信息。 首先建立数据库：默认情况下 zipkin 运行时数据保存在内存中，重启数据会丢失数据库脚本下载 查看与mysql storage相关的配置12345678910111213@ConfigurationProperties(\"zipkin.storage.mysql\")public class ZipkinMySQLStorageProperties implements Serializable &#123; // for Spark jobs private static final long serialVersionUID = 0L; private String host = \"localhost\"; private int port = 3306; private String username; private String password; private String db = \"zipkin\"; private int maxActive = 10; private boolean useSsl; ...&#125; 所以，我们使用mysql作为持久化策略，启动服务端的脚本也就有了1java -server -jar zipkin-server-1.26.0-exec.jar --zipkin.storage.type=mysql --zipkin.storage.mysql.host=localhost --zipkin.storage.mysql.port=3306 --zipkin.storage.mysql.username=root --zipkin.storage.mysql.password=root --zipkin.storage.mysql.db=zipkin 安装方式二springcloud官方按照传输方式分成了三种启动服务端的方式：Sleuth with Zipkin via HTTP，Sleuth with Zipkin via Spring Cloud Stream，Spring Cloud Sleuth Stream Zipkin Collector。只需要添加相应的依赖，之后配置相应的注解，如@EnableZipkinStreamServer即可。具体配置参考Spring Cloud官方文档 项目中，我们使用第一种作为服务端的启动方式，使用mysql作为持久化方案","text":"介绍 Zipkin 为一个分布式的调用链跟踪系统( distributed tracing system ) ,设计来源于 google dapper paper 官方网站 快速入门 安装方式一：使用zipkin官方提供的jar启动服务zipkin官方提供了一个现成的使用springboot写的zipkin服务端，客户端的链路监控报告可以通过多种方式（下文会讲解具体的方式）向服务端发送报告。 系统需要安装java8 下载地址 配置详解查看源码可知其有4种持久化方式，本文选择使用最熟悉的mysql持久化链路调用信息。 首先建立数据库：默认情况下 zipkin 运行时数据保存在内存中，重启数据会丢失数据库脚本下载 查看与mysql storage相关的配置12345678910111213@ConfigurationProperties(\"zipkin.storage.mysql\")public class ZipkinMySQLStorageProperties implements Serializable &#123; // for Spark jobs private static final long serialVersionUID = 0L; private String host = \"localhost\"; private int port = 3306; private String username; private String password; private String db = \"zipkin\"; private int maxActive = 10; private boolean useSsl; ...&#125; 所以，我们使用mysql作为持久化策略，启动服务端的脚本也就有了1java -server -jar zipkin-server-1.26.0-exec.jar --zipkin.storage.type=mysql --zipkin.storage.mysql.host=localhost --zipkin.storage.mysql.port=3306 --zipkin.storage.mysql.username=root --zipkin.storage.mysql.password=root --zipkin.storage.mysql.db=zipkin 安装方式二springcloud官方按照传输方式分成了三种启动服务端的方式：Sleuth with Zipkin via HTTP，Sleuth with Zipkin via Spring Cloud Stream，Spring Cloud Sleuth Stream Zipkin Collector。只需要添加相应的依赖，之后配置相应的注解，如@EnableZipkinStreamServer即可。具体配置参考Spring Cloud官方文档 项目中，我们使用第一种作为服务端的启动方式，使用mysql作为持久化方案 被监控项目配置application.yml 12345678910111213spring: zipkin: #服务端地址 base-url: http://10.19.52.11:9411 #本项目服务名 service: name: $&#123;spring.application.name&#125; sleuth: #监控开关 enabled: true #采样率 sampler: percentage: 1 springboot对zipkin的自动配置可以使得所有RequestMapping匹配到的endpoints得到监控，以及强化了restTemplate，对其加了一层拦截器，使得由他发起的http请求也同样被监控。 motan rpc调用监控Motan通过filter的SPI扩展机制支持OpenTracing，可以支持任何实现了OpenTracing标准的trace实现。使用OpenTracing需要以下步骤。 1.引入filter-opentracing扩展12345&lt;dependency&gt; &lt;groupId&gt;com.weibo&lt;/groupId&gt; &lt;artifactId&gt;filter-opentracing&lt;/artifactId&gt; &lt;version&gt;release&lt;/version&gt;&lt;/dependency&gt; 2.如果第三方trace工具声明了io.opentracing.Tracer的SPI扩展，直接引入第三方trace的jar包即可。如果第三方没有声明，则转第三步。 3.自定义一个TracerFactory实现TracerFactory接口，通过getTracer()来获取不同tracer实现。设置OpenTracingContext的tracerFactory为自定义的TracerFactory即可。 项目中的具体配置MotanConfig.java：12345678910111213141516171819202122232425@Bean(name = \"motanServerBasicConfig\") public BasicServiceConfigBean baseServiceConfig(@Value(\"$&#123;spring.sleuth.enabled:false&#125;\") Boolean tracing ) &#123; BasicServiceConfigBean config = new BasicServiceConfigBean(); ... if(tracing)&#123; config.setFilter(\"sleuth-tracing\"); &#125; ... return config; &#125;@BeanSleuthTracingContext sleuthTracingContext(@Autowired(required = false) org.springframework.cloud.sleuth.Tracer tracer)&#123; SleuthTracingContext context = new SleuthTracingContext(); context.setTracerFactory(new SleuthTracerFactory() &#123; @Override public org.springframework.cloud.sleuth.Tracer getTracer() &#123; return tracer; &#125; &#125;); return context; &#125; 数据查询具体的服务就不列出来了，为了演示依赖关系，service1使用restTemplate调用了service2,service2调用了service3，service4。还有一些现成的motan调用 find a trace当应用正常启动后，可以通过 http://10.19.52.11:9411 查看管理端项目已经成功被监控 Dependencies motan依赖树： http依赖树：","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://lexburner.github.io/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"http://lexburner.github.io/tags/DevOps/"},{"name":"Zipkin","slug":"Zipkin","permalink":"http://lexburner.github.io/tags/Zipkin/"}]},{"title":"JAVA程序员分级，你属于哪一种？","slug":"JAVA程序员分级，你属于哪一级","date":"2017-05-02T11:21:03.000Z","updated":"2017-08-22T05:56:22.455Z","comments":true,"path":"2017/05/02/JAVA程序员分级，你属于哪一级/","link":"","permalink":"http://lexburner.github.io/2017/05/02/JAVA程序员分级，你属于哪一级/","excerpt":"","text":"初级—初 掌握java基础，熟悉常用类库。理解java web中的servlet，jsp，并了解常用的框架对java web的封装原理，能够借助框架完成增删改查功能。理解数据库在web开发中的地位。 初级—中 理解java中较为高级的特性，如反射，动态代理，JVM，内存模型，多线程等等。熟练使用框架，对框架中遇到的bug，能够借助日志和搜索引擎分析出问题的原因。在团队中，能够独立完成普通后台业务功能的开发。了解数据库的高级特性，如索引，存储引擎等等。 初级—高 理解java分布式架构，微服务架构，了解其与集中式架构的区别，并能保证分布式代码质量。熟练使用各个中间件如redis，mq，zookeeper等等，并了解其工作原理和使用场景。能够在中级或高级程序员的带领之下，完成非核心功能的研发。能够关注开源，并且具有阅读源码的能力。 中级 具备一定的项目开发经验（3年之上一线互联网产品研发经验），拥有线上bug的处理能力，JVM调优能力，以及完成核心业务功能的开发。并且带领团队的新人，能够按能力分配任务。 高级 团队的核心人物，把控整个项目的质量，包括代码漏洞和规范问题。具有5年以上项目开发经验，2年以上架构搭建的经验，能够根据业务选择不同的架构类型；根据团队组成，分配不同的任务。具有将自己的知识分享出去的能力，带领初级程序员走向中级，中级程序员走向高级的能力。","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/categories/技术杂谈/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"},{"name":"杂谈","slug":"杂谈","permalink":"http://lexburner.github.io/tags/杂谈/"}]},{"title":"drools用户指南----Cross Products","slug":"drools用户指南----Cross Products","date":"2017-04-11T05:44:54.000Z","updated":"2017-08-22T06:11:59.582Z","comments":true,"path":"2017/04/11/drools用户指南----Cross Products/","link":"","permalink":"http://lexburner.github.io/2017/04/11/drools用户指南----Cross Products/","excerpt":"","text":"Cross Products之前提到“Cross Products”一词，其实就是一个join操作（译者注：可以理解为笛卡尔积）。想象一下，火灾报警示例的数据与以下规则结合使用，其中没有字段约束： 1234567rule \"Show Sprinklers\" when $room : Room() $sprinkler : Sprinkler()then System.out.println( \"room:\" + $room.getName() + \" sprinkler:\" + $sprinkler.getRoom().getName() );end 在SQL术语中，这就像是执行了select * from Room, Sprinkler，Sprinkler 表中的每一行将与Room表中的每一行相连接，从而产生以下输出： 12345678910111213141516room:office sprinkler:officeroom:office sprinkler:kitchenroom:office sprinkler:livingroomroom:office sprinkler:bedroomroom:kitchen sprinkler:officeroom:kitchen sprinkler:kitchenroom:kitchen sprinkler:livingroomroom:kitchen sprinkler:bedroomroom:livingroom sprinkler:officeroom:livingroom sprinkler:kitchenroom:livingroom sprinkler:livingroomroom:livingroom sprinkler:bedroomroom:bedroom sprinkler:officeroom:bedroom sprinkler:kitchenroom:bedroom sprinkler:livingroomroom:bedroom sprinkler:bedroom 这些连接结果显然会变得巨大，它们必然包含冗余数据。 cross products的大小通常是新规则引擎产品性能问题的根源。 从这可以看出，我们希望约束cross products，这便是用可变约束（the variable constraint）完成的。 12345678rulewhen $room : Room() $sprinkler : Sprinkler( room == $room )then System.out.println( \"room:\" + $room.getName() + \" sprinkler:\" + $sprinkler.getRoom().getName() );end 这就使得筛选结果只有寥寥几行, 这就为每一个Room筛选出了正确的Sprinkler. 在sql中(实际上是HQL) 这样的查询约等于select * from Room, Sprinkler where Room == Sprinkler.room.","categories":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/categories/规则引擎/"}],"tags":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/tags/规则引擎/"},{"name":"drools","slug":"drools","permalink":"http://lexburner.github.io/tags/drools/"}]},{"title":"drools用户指南----Methods vs Rules","slug":"drools用户指南----Methods vs Rules","date":"2017-04-11T05:28:44.000Z","updated":"2017-08-22T06:10:20.571Z","comments":true,"path":"2017/04/11/drools用户指南----Methods vs Rules/","link":"","permalink":"http://lexburner.github.io/2017/04/11/drools用户指南----Methods vs Rules/","excerpt":"","text":"Methods vs Rules人们经常混淆方法和规则，初学者经常会问：“我如何理解规则的含义？“ 在最后一节之后，你会对规则的使用得心应手，答案也变得显而易见的，但在这之前，先让我们总结一下方法判断和规则的差异。 12345public void helloWorld(Person person) &#123; if ( person.getName().equals( \"Chuck\" ) ) &#123; System.out.println( \"Hello Chuck\" ); &#125;&#125; 方法是被直接调用的 需要传递具体的实例 一个调用导致一次执行（One call results in a single execution）。 12345rule \"Hello World\" when Person( name == \"Chuck\" )then System.out.println( \"Hello Chuck\" );end 只要将其插入引擎，就可以通过匹配任何数据执行规则。 规则永远无法被直接调用，而只能触发 无法将特定的实例传递给规则 根据匹配，一个规则可能会触发一次或多次，或根本不被触发。","categories":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/categories/规则引擎/"}],"tags":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/tags/规则引擎/"},{"name":"drools","slug":"drools","permalink":"http://lexburner.github.io/tags/drools/"}]},{"title":"drools用户指南----stateless session（无状态会话）的使用","slug":"drools用户指南----stateless session（无状态会话）的使用","date":"2017-04-11T04:51:59.000Z","updated":"2017-08-22T06:22:12.445Z","comments":true,"path":"2017/04/11/drools用户指南----stateless session（无状态会话）的使用/","link":"","permalink":"http://lexburner.github.io/2017/04/11/drools用户指南----stateless session（无状态会话）的使用/","excerpt":"stateless session 无状态会话Drools规则引擎中有如此多的用例和诸多功能，它变得令人难以置信。不过不用担心，复杂性是分层的，你可以用简单的用例来逐步了解drools。 无状态会话，不使用推理，形成最简单的用例。无状态会话可以被称为函数传递一些数据，然后再接收一些结果。无状态会话的一些常见用例有以下但不限于： 验证这个人有资格获得抵押吗？ 计算计算抵押保费。 路由和过滤将传入的邮件（如电子邮件）过滤到文件夹中。将传入的邮件发送到目的地。 所以让我们从使用驾驶执照应用程序的一个非常简单的例子开始吧。 123456public class Applicant &#123; private String name; private int age; private boolean valid; // getter and setter methods here&#125; 现在我们有了我们的数据模型，我们可以写出我们的第一个规则。我们假设应用程序使用规则来拒绝不符合规则的申请。由于这是一个简单的验证用例，我们将添加一条规则来取消任何18岁以下的申请人的资格。 12345678package com.company.licenserule \"Is of valid age\"when $a : Applicant( age &lt; 18 )then $a.setValid( false );end","text":"stateless session 无状态会话Drools规则引擎中有如此多的用例和诸多功能，它变得令人难以置信。不过不用担心，复杂性是分层的，你可以用简单的用例来逐步了解drools。 无状态会话，不使用推理，形成最简单的用例。无状态会话可以被称为函数传递一些数据，然后再接收一些结果。无状态会话的一些常见用例有以下但不限于： 验证这个人有资格获得抵押吗？ 计算计算抵押保费。 路由和过滤将传入的邮件（如电子邮件）过滤到文件夹中。将传入的邮件发送到目的地。 所以让我们从使用驾驶执照应用程序的一个非常简单的例子开始吧。 123456public class Applicant &#123; private String name; private int age; private boolean valid; // getter and setter methods here&#125; 现在我们有了我们的数据模型，我们可以写出我们的第一个规则。我们假设应用程序使用规则来拒绝不符合规则的申请。由于这是一个简单的验证用例，我们将添加一条规则来取消任何18岁以下的申请人的资格。 12345678package com.company.licenserule \"Is of valid age\"when $a : Applicant( age &lt; 18 )then $a.setValid( false );end 为了使引擎了解数据，所以可以根据规则进行处理，我们必须插入数据，就像数据库一样。当申请人实例插入到引擎中时，将根据规则的约束进行评估，在这种情况下，这只是一个规则的两个约束条件。我们说两个，因为申请人类型是第一个对象类型约束，而age &lt;18是第二个字段约束。对象类型约束及其零个或多个字段约束被称为模式。当插入的实例同时满足对象类型约束和所有字段约束时，它被称为匹配。$a是一个绑定变量，它允许我们引用匹配的对象。其属性可以更新。美元字符（’$’）是可选的，但它有助于区分变量名称和字段名称。匹配模式与插入数据的过程并不奇怪，通常被称为模式匹配。 要使用这个规则，有必要把它放在一个Drools文件中，只是一个带有.drl扩展名的纯文本文件，简称为“Drools Rule Language”。我们来调用licenseApplication.drl这个文件，并将其存储在Kie Project中。 Kie项目具有正常的Maven项目的结构，并附加一个可以创建的KieBase和KieSession文件（kmodule.xml）。该文件必须放在Maven项目的resources/META-INF文件夹中，而所有其他Drools工件（如包含前一规则的licenseApplication.drl）必须存储在资源文件夹或其下的任何其他子文件夹中。 由于为所有配置方面提供了有意义的默认值，所以最简单的kmodule.xml文件只能包含一个空的kmodule标签，如下所示： 12&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;kmodule xmlns=\"http://www.drools.org/xsd/kmodule\"/&gt; 此时，可以从类路径创建一个KieContainer来读取要构建的文件。 12KieServices kieServices = KieServices.Factory.get();KieContainer kContainer = kieServices.getKieClasspathContainer(); 上面的代码段编译了类路径中找到的所有DRL文件，并将该编译结果KieModule放在KieContainer中。如果没有错误，我们现在可以从KieContainer创建我们的会话并执行一些数据： 12345StatelessKieSession kSession = kContainer.newStatelessKieSession();Applicant applicant = new Applicant( \"Mr John Smith\", 16 );assertTrue( applicant.isValid() );ksession.execute( applicant );assertFalse( applicant.isValid() ); 上述代码根据规则执行数据。由于申请人年龄未满18岁，申请被标记为无效。 到目前为止，我们只使用了一个实例，但是如果我们想要使用多个实例呢？我们可以执行任何实现Iterable的对象，如集合。我们再添加一个名为Application的类，它有应用程序的日期，我们还将布尔有效字段移到Application类。 1234567891011public class Applicant &#123; private String name; private int age; // getter and setter methods here&#125;public class Application &#123; private Date dateApplied; private boolean valid; // getter and setter methods here&#125; 我们还将添加另一条规则来验证申请是否在一段时间内进行。 12345678910111213141516package com.company.licenserule \"Is of valid age\"when Applicant( age &lt; 18 ) $a : Application() then $a.setValid( false );endrule \"Application was made this year\"when $a : Application( dateApplied &gt; \"01-jan-2009\" ) then $a.setValid( false );end 不幸的是，Java数组不实现Iterable接口，所以我们必须使用JDK转换器方法Arrays.asList（…）。下面显示的代码针对一个可迭代列表执行，其中在触发任何匹配的规则之前插入所有集合元素。 123456StatelessKieSession kSession = kContainer.newStatelessKieSession();Applicant applicant = new Applicant( \"Mr John Smith\", 16 );Application application = new Application();assertTrue( application.isValid() );ksession.execute( Arrays.asList( new Object[] &#123; application, applicant &#125; ) );assertFalse( application.isValid() ); 执行的两个执行方法（Object object）和execute（Iterable对象）实际上是接口BatchExecutor的方法execute（Command命令）的便利方法。 KieCommands命令工厂可以像KIE A​​PI的所有其他工厂一样从KieServices获取，用于创建命令，以便以下操作相当于执行（Iterable it）： 1ksession.execute( kieServices.getCommands().newInsertElements( Arrays.asList( new Object[] &#123; application, applicant &#125; ) ); 批处理执行器和命令工厂在使用多个命令和输出标识符以获取结果时特别有用。123456KieCommands kieCommands = kieServices.getCommands();List&lt;Command&gt; cmds = new ArrayList&lt;Command&gt;();cmds.add( kieCommands.newInsert( new Person( \"Mr John Smith\" ), \"mrSmith\", true, null ) );cmds.add( kieCommands.newInsert( new Person( \"Mr John Doe\" ), \"mrDoe\", true, null ) );BatchExecutionResults results = ksession.execute( kieCommands.newBatchExecution( cmds ) );assertEquals( new Person( \"Mr John Smith\" ), results.getValue( \"mrSmith\" ) );","categories":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/categories/规则引擎/"}],"tags":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/tags/规则引擎/"},{"name":"drools","slug":"drools","permalink":"http://lexburner.github.io/tags/drools/"}]},{"title":"drools用户指南----stateful session（有状态会话）的使用","slug":"drools用户指南----stateful session（有状态会话）的使用","date":"2017-04-11T04:37:22.000Z","updated":"2017-08-22T06:05:22.304Z","comments":true,"path":"2017/04/11/drools用户指南----stateful session（有状态会话）的使用/","link":"","permalink":"http://lexburner.github.io/2017/04/11/drools用户指南----stateful session（有状态会话）的使用/","excerpt":"stateful session 有状态会话有状态会话长期存在，并允许随着时间的推移进行迭代更改。 有状态会话的一些常见用例包括但不限于： 监测半自动买入股票市场监控与分析。 诊断故障查找，医疗诊断 物流包裹跟踪和送货配置 合规验证市场交易的合法性。 与无状态会话相反，必须先调用dispose()方法，以确保没有内存泄漏，因为KieBase包含创建状态知识会话时的引用。 由于状态知识会话是最常用的会话类型，所以它只是在KIE API中命名为KieSession。 KieSession还支持BatchExecutor接口，如StatelessKieSession，唯一的区别是FireAllRules命令在有状态会话结束时不被自动调用。 我们举例说明了用于提高火灾报警器的监控用例。 只使用四个类，我们假设Room代表房子里的房间，每个Room都有一个喷头Sprinkler。 如果在房间里发生火灾，我们用一个Fire实例来表示,用Alarm代表警报 。 123456789101112131415161718public class Room &#123; private String name // getter and setter methods here&#125;public class Sprinkler &#123; private Room room; private boolean on; // getter and setter methods here&#125;public class Fire &#123; private Room room; // getter and setter methods here&#125;public class Alarm &#123;&#125; 在上一节无状态会话中介绍了插入和匹配数据的概念。 这个例子假设每个对象类型的都是单个实例被插入的，因此只使用了字面约束。 然而，房子有许多房间，因此rules必须表达实体类之间的关系，例如在某个房间内的喷洒器。 这最好通过使用绑定变量作为模式中的约束来完成。 这种“加入”过程产生了所谓的“cross products”，这在下一节中将会介绍。","text":"stateful session 有状态会话有状态会话长期存在，并允许随着时间的推移进行迭代更改。 有状态会话的一些常见用例包括但不限于： 监测半自动买入股票市场监控与分析。 诊断故障查找，医疗诊断 物流包裹跟踪和送货配置 合规验证市场交易的合法性。 与无状态会话相反，必须先调用dispose()方法，以确保没有内存泄漏，因为KieBase包含创建状态知识会话时的引用。 由于状态知识会话是最常用的会话类型，所以它只是在KIE API中命名为KieSession。 KieSession还支持BatchExecutor接口，如StatelessKieSession，唯一的区别是FireAllRules命令在有状态会话结束时不被自动调用。 我们举例说明了用于提高火灾报警器的监控用例。 只使用四个类，我们假设Room代表房子里的房间，每个Room都有一个喷头Sprinkler。 如果在房间里发生火灾，我们用一个Fire实例来表示,用Alarm代表警报 。 123456789101112131415161718public class Room &#123; private String name // getter and setter methods here&#125;public class Sprinkler &#123; private Room room; private boolean on; // getter and setter methods here&#125;public class Fire &#123; private Room room; // getter and setter methods here&#125;public class Alarm &#123;&#125; 在上一节无状态会话中介绍了插入和匹配数据的概念。 这个例子假设每个对象类型的都是单个实例被插入的，因此只使用了字面约束。 然而，房子有许多房间，因此rules必须表达实体类之间的关系，例如在某个房间内的喷洒器。 这最好通过使用绑定变量作为模式中的约束来完成。 这种“加入”过程产生了所谓的“cross products”，这在下一节中将会介绍。 当发生火灾时，会为该类别创建Fire类的实例，并将其插入到会话中。 该规则使用Fire对象的房间字段上的绑定来约束与当前关闭的房间的喷水灭火器的匹配。 当此规则触发并且执行结果时，喷头被打开。 12345678rule \"When there is a fire turn on the sprinkler\"when Fire($room : room) $sprinkler : Sprinkler( room == $room, on == false )then modify( $sprinkler ) &#123; setOn( true ) &#125;; System.out.println( \"Turn on the sprinkler for room \" + $room.getName() );end 而无状态会话使用标准Java语法来修改字段，在上述规则中，我们使用modify语句，它作为一种“with”语句。 它可以包含一系列逗号分隔的Java表达式，即对由modify语句的控制表达式选择的对象的setter的调用。 这将修改数据，并使引擎意识到这些更改，以便它可以再次对其进行推理。 这个过程被称为推理，对于有状态会话的工作至关重要。 无状态会话通常不使用推理，因此引擎不需要意识到数据的更改。 也可以通过使用顺序模式显式地关闭推理。 到目前为止，我们有规则告诉我们匹配数据是否存在，但是当它不存在时呢？ 我们如何确定火已经熄灭了，即没有Fire对象呢？ 以前的约束是根据命题逻辑的句子，其中引擎限制个别的实例。 Drools还支持First Order Logic，允许您查看数据集。 当某个不存在时，关键字下的模式不匹配。 一旦这个房间的火灾消失，下面给出的规则会使喷水灭火。 123456789rule \"When the fire is gone turn off the sprinkler\"when $room : Room( ) $sprinkler : Sprinkler( room == $room, on == true ) not Fire( room == $room )then modify( $sprinkler ) &#123; setOn( false ) &#125;; System.out.println( \"Turn off the sprinkler for room \" + $room.getName() );end 每个room有一个喷水灭火器，house只有一个警报。 当发生火灾时，会创建一个alrm对象，而不管发生多少火灾，整个建筑物都只需要一个警报alrm。 1234567rule \"Raise the alarm when we have one or more fires\"when exists Fire()then insert( new Alarm() ); System.out.println( \"Raise the alarm\" );end 同样，当没有火灾时，我们想要删除警报，所以可以再次使用not关键字。 12345678rule \"Cancel the alarm when all the fires have gone\"when not Fire() $alarm : Alarm()then delete( $alarm ); System.out.println( \"Cancel the alarm\" );end 最后，当应用程序首次启动并且在报警消除并且所有喷头已关闭后，都会打印Everything is ok。 1234567rule \"Status output when things are ok\"when not Alarm() not Sprinkler( on == true ) then System.out.println( \"Everything is ok\" );end 正如我们在无状态会话示例中所做的那样，上述规则应放在单个DRL文件中，并保存到Maven项目或其任何子文件夹的资源文件夹中。 如前所述，我们可以从KieContainer获得KieSession。 唯一的区别是，这次我们创建一个有状态会话，而之前我们创建的是一个无状态会话。 123KieServices kieServices = KieServices.Factory.get();KieContainer kContainer = kieServices.getKieClasspathContainer();KieSession ksession = kContainer.newKieSession(); 创建会话后，现在可以随着时间的推移迭代地使用它。 创建和插入四个房间对象，每个房间的对应一个Sprinkler对象。 此时，规则引擎已经完成了所有的匹配，但并没有触发。 调用ksession.fireAllRules（）使得匹配的规则触发，但因为没有火灾，所以输出结果是Everything is ok。 1234567891011String[] names = new String[]&#123;\"kitchen\", \"bedroom\", \"office\", \"livingroom\"&#125;;Map&lt;String,Room&gt; name2room = new HashMap&lt;String,Room&gt;();for( String name: names )&#123; Room room = new Room( name ); name2room.put( name, room ); ksession.insert( room ); Sprinkler sprinkler = new Sprinkler( room ); ksession.insert( sprinkler );&#125;ksession.fireAllRules(); Everything is ok 我们现在创造两个Fire并插入它们， 随着发动机内部的火灾，一旦调用了fireAllRules（），报警器就会升高，并且相应的喷水灭火器打开。 123456Fire kitchenFire = new Fire( name2room.get( \"kitchen\" ) );Fire officeFire = new Fire( name2room.get( \"office\" ) );FactHandle kitchenFireHandle = ksession.insert( kitchenFire );FactHandle officeFireHandle = ksession.insert( officeFire );ksession.fireAllRules(); Raise the alarmTurn on the sprinkler for room kitchenTurn on the sprinkler for room office 一段时间之后，火灾将熄灭，并且Fire实例被撤回。 这导致喷头关闭，报警被取消，最后再次打印Everything is ok。 1234ksession.delete( kitchenFireHandle );ksession.delete( officeFireHandle );ksession.fireAllRules(); Cancel the alarmTurn off the sprinkler for room officeTurn off the sprinkler for room kitchenEverything is ok","categories":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/categories/规则引擎/"}],"tags":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://lexburner.github.io/tags/规则引擎/"},{"name":"drools","slug":"drools","permalink":"http://lexburner.github.io/tags/drools/"}]},{"title":"Zuul性能测试","slug":"Zuul性能测试","date":"2017-04-08T07:27:52.000Z","updated":"2017-08-22T06:26:32.079Z","comments":true,"path":"2017/04/08/Zuul性能测试/","link":"","permalink":"http://lexburner.github.io/2017/04/08/Zuul性能测试/","excerpt":"环境准备采用三台阿里云服务器作为测试10.19.52.8 部署网关应用-gateway10.19.52.9, 10.19.52.10 部署用于测试的业务系统 压测工具准备选用ab作为压力测试的工具，为了方便起见，直接将ab工具安装在10.19.52.8这台机测试命令如下：1ab -n 10000 -c 100 http://10.19.52.8:8080/hello/testOK?access_token=e0345712-c30d-4bf8-ae61-8cae1ec38c52 其中－n表示请求数，－c表示并发数,上面一条命令也就意味着，100个用户并发对http://10.19.52.8/hello/testOK累计发送了10000次请求。 服务器,网关配置由于我们使用的tomcat容器，关于tomcat的一点知识总结如下：","text":"环境准备采用三台阿里云服务器作为测试10.19.52.8 部署网关应用-gateway10.19.52.9, 10.19.52.10 部署用于测试的业务系统 压测工具准备选用ab作为压力测试的工具，为了方便起见，直接将ab工具安装在10.19.52.8这台机测试命令如下：1ab -n 10000 -c 100 http://10.19.52.8:8080/hello/testOK?access_token=e0345712-c30d-4bf8-ae61-8cae1ec38c52 其中－n表示请求数，－c表示并发数,上面一条命令也就意味着，100个用户并发对http://10.19.52.8/hello/testOK累计发送了10000次请求。 服务器,网关配置由于我们使用的tomcat容器，关于tomcat的一点知识总结如下： Tomcat的最大并发数是可以配置的，实际运用中，最大并发数与硬件性能和CPU数量都有很大关系的。更好的硬件，更多的处理器都会使Tomcat支持更多的并发。​Tomcat 默认的HTTP实现是采用阻塞式的Socket通信，每个请求都需要创建一个线程处理，当一个进程有500个线程在跑的话，那性能已经是很低很低了。Tomcat默认配置的最大请求数是150，也就是说同时支持150个并发。具体能承载多少并发，需要看硬件的配置，CPU越多性能越高，分配给JVM的内存越多性能也就越高，但也会加重GC的负担。当某个应用拥有 250个以上并发的时候，应考虑应用服务器的集群。操作系统对于进程中的线程数有一定的限制： Windows 每个进程中的线程数不允许超过 2000Linux 每个进程中的线程数不允许超过 1000在Java中每开启一个线程需要耗用1MB的JVM内存空间用于作为线程栈之用，此处也应考虑。 所以我们修改配置tomcat的默认配置，如下：12345server: tomcat: accept-count: 1000 max-threads: 1000 max-connections: 2000 无论是网关应用，还是用于测试的业务系统的tomcat，我们都需要如上配置，否则会引起木桶效应，整个调用流程会受到配置最差的应用的干扰。zuul内部路由可以理解为使用一个线程池去发送路由请求，所以我们也需要扩大这个线程池的容量，配置如下：1234zuul: host: max-per-route-connections: 1000 max-total-connections: 1000 监控工具为了确保上述配置真正起作用，我们使用Java VisualVM这个工具监控这几台服务器上部署的tomcat的线程以及内存使用情况。启动脚本加上如下参数，之后通过工具连接2099端口即可监控1-Dcom.sun.management.jmxremote.port=2099 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Djava.rmi.server.hostname=10.19.52.8 开始测试 测试一1.通过访问网关，由网关转发，应用端接口延迟200ms后返回一个字符串，模拟真实接口的业务处理延迟2.300个线程并发请求，共计100000 次1ab -n 100000 -c 300 http://10.19.52.8:8080/hello/testOK?access_token=e0345712-c30d-4bf8-ae61-8cae1ec38c52 1234567891011121314151617181920212223242526272829303132Document Path: /hello/testOK?access_token=e0345712-c30d-4bf8-ae61-8cae1ec38c52Document Length: 2 bytesConcurrency Level: 300Time taken for tests: 151.026 secondsComplete requests: 100000Failed requests: 0Write errors: 0Total transferred: 42200844 bytesHTML transferred: 200004 bytes**Requests per second: 662.14 [#/sec] (mean)**Time per request: 453.078 [ms] (mean)Time per request: 1.510 [ms] (mean, across all concurrent requests)Transfer rate: 272.88 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 5 7.0 2 98Processing: 206 447 478.7 230 3171Waiting: 197 445 478.7 227 3165Total: 206 451 478.8 236 3177Percentage of the requests served within a certain time (ms) 50% 236 66% 250 75% 273 80% 322 90% 1408 95% 1506 98% 1684 99% 1764 100% 3177 (longest request) 测试二：1.直接访问应用，应用端接口延迟200ms后返回一个字符串，模拟真实接口的业务处理延迟2.300个线程并发请求，共计100000 次 1ab -n 100000 -c 300 http://10.19.52.9:9091/testOK 1234567891011121314151617181920212223242526272829303132333435Server Hostname: 10.19.52.9Server Port: 9091Document Path: /testOKDocument Length: 2 bytesConcurrency Level: 300Time taken for tests: 69.003 secondsComplete requests: 100000Failed requests: 0Write errors: 0Total transferred: 13400000 bytesHTML transferred: 200000 bytes**Requests per second: 1449.21 [#/sec] (mean)**Time per request: 207.009 [ms] (mean)Time per request: 0.690 [ms] (mean, across all concurrent requests)Transfer rate: 189.64 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 0 0.8 0 10Processing: 200 206 7.7 202 286Waiting: 200 205 7.7 202 286Total: 201 206 7.9 203 295Percentage of the requests served within a certain time (ms) 50% 203 66% 205 75% 207 80% 209 90% 215 95% 220 98% 229 99% 240 100% 295 (longest request) 经过网关路由之后的性能下降是不可避免的，在测试过程中，查看监控端的线程变化，如下图： 我们的配置的确产生了作用。 我们再来分析一下上面测试结果的一个重要指标：Requests per second，我们的网关经过了鉴权之后，性能仍然可以达到600+每秒的响应，是完全可以接受的，峰值时内存情况，使用top指令，如下所示：ab测试命令也占用了一定的cpu使用率，总应用接近70%的cpu使用率，这估计也是单个tomcat实例的瓶颈了。因为我们的应用服务器会单独部署网关，并且可以在多个服务器上部署多个实例，所以这个结果可以接受。 为了避免单次响应带来的偶然因素，我们重复进行测试一（更改为10000次请求，并发量200），看看Requests per second的变化。 123451. 799.452. 818.863. 838.674. 833.905. 973.65 总结有一些其他的数据没有整理到博客中，但是也顺便把结论写一下。 这次的测试有几个注意点： 是在应用服务器端模拟200ms的延时，因为实际请求不可能不伴随着耗时的业务操作，实际发现对ab的测试影响还是较大的，毕竟线程阻塞着，不延迟时request per second能达到2000，加了200ms延迟之后下降到1000+。 模拟总请求数和线程数的变化会引起QPS/TPS的抖动，即使是在多核CPU的承受范围之内，也并不是说线程越多，QPS/TPS就越高，因为启动线程的开销，以及线程上下文切换的耗时，开辟线程带来的内存损耗都会影响性能。钱总说单个tomcat实例的并发度理论值200就可以接受了，经过参数调优后的tomcat使用zuul做网关能达到如上的测试结果，完全可以投入生产环境使用了。而tomcat默认的150线程，如果使用200的并发度测试就显然是“不公平的”。 测试注意点有几个，例如ab部署在了api-gateway本机会影响性能，tomcat参数以及zuul参数应当尽可能放开，不让其默认配置影响测试。 本文还有些遗漏的数据，后续会补上…","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud Zuul","slug":"Spring-Cloud-Zuul","permalink":"http://lexburner.github.io/tags/Spring-Cloud-Zuul/"}]},{"title":"springcloud----Zuul动态路由","slug":"springcloud----Zuul动态路由","date":"2017-04-01T06:11:52.000Z","updated":"2017-08-22T06:34:10.169Z","comments":true,"path":"2017/04/01/springcloud----Zuul动态路由/","link":"","permalink":"http://lexburner.github.io/2017/04/01/springcloud----Zuul动态路由/","excerpt":"前言Zuul 是Netflix 提供的一个开源组件,致力于在云平台上提供动态路由，监控，弹性，安全等边缘服务的框架。也有很多公司使用它来作为网关的重要组成部分，碰巧今年公司的架构组决定自研一个网关产品，集动态路由，动态权限，限流配额等功能为一体，为其他部门的项目提供统一的外网调用管理，最终形成产品(这方面阿里其实已经有成熟的网关产品了，但是不太适用于个性化的配置，也没有集成权限和限流降级)。 不过这里并不想介绍整个网关的架构，而是想着重于讨论其中的一个关键点，并且也是经常在交流群中听人说起的：动态路由怎么做？ 再阐释什么是动态路由之前，需要介绍一下架构的设计。 传统互联网架构图上图是没有网关参与的一个最典型的互联网架构(本文中统一使用book代表应用实例，即真正提供服务的一个业务系统) 加入eureka的架构图book注册到eureka注册中心中，zuul本身也连接着同一个eureka，可以拉取book众多实例的列表。服务中心的注册发现一直是值得推崇的一种方式，但是不适用与网关产品。因为我们的网关是面向众多的其他部门的已有或是异构架构的系统，不应该强求其他系统都使用eureka，这样是有侵入性的设计。 最终架构图要强调的一点是，gateway最终也会部署多个实例，达到分布式的效果，在架构图中没有画出，请大家自行脑补。 本博客的示例使用最后一章架构图为例，带来动态路由的实现方式，会有具体的代码。","text":"前言Zuul 是Netflix 提供的一个开源组件,致力于在云平台上提供动态路由，监控，弹性，安全等边缘服务的框架。也有很多公司使用它来作为网关的重要组成部分，碰巧今年公司的架构组决定自研一个网关产品，集动态路由，动态权限，限流配额等功能为一体，为其他部门的项目提供统一的外网调用管理，最终形成产品(这方面阿里其实已经有成熟的网关产品了，但是不太适用于个性化的配置，也没有集成权限和限流降级)。 不过这里并不想介绍整个网关的架构，而是想着重于讨论其中的一个关键点，并且也是经常在交流群中听人说起的：动态路由怎么做？ 再阐释什么是动态路由之前，需要介绍一下架构的设计。 传统互联网架构图上图是没有网关参与的一个最典型的互联网架构(本文中统一使用book代表应用实例，即真正提供服务的一个业务系统) 加入eureka的架构图book注册到eureka注册中心中，zuul本身也连接着同一个eureka，可以拉取book众多实例的列表。服务中心的注册发现一直是值得推崇的一种方式，但是不适用与网关产品。因为我们的网关是面向众多的其他部门的已有或是异构架构的系统，不应该强求其他系统都使用eureka，这样是有侵入性的设计。 最终架构图要强调的一点是，gateway最终也会部署多个实例，达到分布式的效果，在架构图中没有画出，请大家自行脑补。 本博客的示例使用最后一章架构图为例，带来动态路由的实现方式，会有具体的代码。 动态路由动态路由需要达到可持久化配置，动态刷新的效果。如架构图所示，不仅要能满足从spring的配置文件properties加载路由信息，还需要从数据库加载我们的配置。另外一点是，路由信息在容器启动时就已经加载进入了内存，我们希望配置完成后，实施发布，动态刷新内存中的路由信息，达到不停机维护路由信息的效果。 zuul–HelloWorldDemo项目结构123456789101112131415161718192021222324252627&lt;groupId&gt;com.sinosoft&lt;/groupId&gt; &lt;artifactId&gt;zuul-gateway-demo&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;modules&gt; &lt;module&gt;gateway&lt;/module&gt; &lt;module&gt;book&lt;/module&gt; &lt;/modules&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Camden.SR6&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; tip：springboot-1.5.2对应的springcloud的版本需要使用Camden.SR6，一开始想专门写这个demo时，只替换了springboot的版本1.4.0-&gt;1.5.2，结果启动就报错了，最后发现是版本不兼容的锅。 gateway项目：启动类：GatewayApplication.java123456789@EnableZuulProxy@SpringBootApplicationpublic class GatewayApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GatewayApplication.class, args); &#125;&#125; 配置：application.properties 1234567#配置在配置文件中的路由信息zuul.routes.books.url=http://localhost:8090zuul.routes.books.path=/books/**#不使用注册中心,会带来侵入性ribbon.eureka.enabled=false#网关端口server.port=8080 book项目：启动类：BookApplication.java 12345678910111213141516171819@RestController@SpringBootApplicationpublic class BookApplication &#123; @RequestMapping(value = \"/available\") public String available() &#123; System.out.println(\"Spring in Action\"); return \"Spring in Action\"; &#125; @RequestMapping(value = \"/checked-out\") public String checkedOut() &#123; return \"Spring Boot in Action\"; &#125; public static void main(String[] args) &#123; SpringApplication.run(BookApplication.class, args); &#125;&#125; 配置类：application.properties 1server.port=8090 测试访问：http://localhost:8080/books/available 上述demo是一个简单的静态路由，简单看下源码，zuul是怎么做到转发，路由的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140@Configuration@EnableConfigurationProperties(&#123; ZuulProperties.class &#125;)@ConditionalOnClass(ZuulServlet.class)@Import(ServerPropertiesAutoConfiguration.class)public class ZuulConfiguration &#123; @Autowired //zuul的配置文件,对应了application.properties中的配置信息 protected ZuulProperties zuulProperties; @Autowired protected ServerProperties server; @Autowired(required = false) private ErrorController errorController; @Bean public HasFeatures zuulFeature() &#123; return HasFeatures.namedFeature(\"Zuul (Simple)\", ZuulConfiguration.class); &#125; //核心类，路由定位器，最最重要 @Bean @ConditionalOnMissingBean(RouteLocator.class) public RouteLocator routeLocator() &#123; //默认配置的实现是SimpleRouteLocator.class return new SimpleRouteLocator(this.server.getServletPrefix(), this.zuulProperties); &#125; //zuul的控制器，负责处理链路调用 @Bean public ZuulController zuulController() &#123; return new ZuulController(); &#125; //MVC HandlerMapping that maps incoming request paths to remote services. @Bean public ZuulHandlerMapping zuulHandlerMapping(RouteLocator routes) &#123; ZuulHandlerMapping mapping = new ZuulHandlerMapping(routes, zuulController()); mapping.setErrorController(this.errorController); return mapping; &#125; //注册了一个路由刷新监听器，默认实现是ZuulRefreshListener.class，这个是我们动态路由的关键 @Bean public ApplicationListener&lt;ApplicationEvent&gt; zuulRefreshRoutesListener() &#123; return new ZuulRefreshListener(); &#125; @Bean @ConditionalOnMissingBean(name = \"zuulServlet\") public ServletRegistrationBean zuulServlet() &#123; ServletRegistrationBean servlet = new ServletRegistrationBean(new ZuulServlet(), this.zuulProperties.getServletPattern()); // The whole point of exposing this servlet is to provide a route that doesn't // buffer requests. servlet.addInitParameter(\"buffer-requests\", \"false\"); return servlet; &#125; // pre filters @Bean public ServletDetectionFilter servletDetectionFilter() &#123; return new ServletDetectionFilter(); &#125; @Bean public FormBodyWrapperFilter formBodyWrapperFilter() &#123; return new FormBodyWrapperFilter(); &#125; @Bean public DebugFilter debugFilter() &#123; return new DebugFilter(); &#125; @Bean public Servlet30WrapperFilter servlet30WrapperFilter() &#123; return new Servlet30WrapperFilter(); &#125; // post filters @Bean public SendResponseFilter sendResponseFilter() &#123; return new SendResponseFilter(); &#125; @Bean public SendErrorFilter sendErrorFilter() &#123; return new SendErrorFilter(); &#125; @Bean public SendForwardFilter sendForwardFilter() &#123; return new SendForwardFilter(); &#125; @Configuration protected static class ZuulFilterConfiguration &#123; @Autowired private Map&lt;String, ZuulFilter&gt; filters; @Bean public ZuulFilterInitializer zuulFilterInitializer() &#123; return new ZuulFilterInitializer(this.filters); &#125; &#125; //上面提到的路由刷新监听器 private static class ZuulRefreshListener implements ApplicationListener&lt;ApplicationEvent&gt; &#123; @Autowired private ZuulHandlerMapping zuulHandlerMapping; private HeartbeatMonitor heartbeatMonitor = new HeartbeatMonitor(); @Override public void onApplicationEvent(ApplicationEvent event) &#123; if (event instanceof ContextRefreshedEvent || event instanceof RefreshScopeRefreshedEvent || event instanceof RoutesRefreshedEvent) &#123; //设置为脏,下一次匹配到路径时，如果发现为脏，则会去刷新路由信息 this.zuulHandlerMapping.setDirty(true); &#125; else if (event instanceof HeartbeatEvent) &#123; if (this.heartbeatMonitor.update(((HeartbeatEvent) event).getValue())) &#123; this.zuulHandlerMapping.setDirty(true); &#125; &#125; &#125; &#125;&#125; 我们要解决动态路由的难题，第一步就得理解路由定位器的作用。很失望，因为从接口关系来看，spring考虑到了路由刷新的需求，但是默认实现的SimpleRouteLocator没有实现RefreshableRouteLocator接口，看来我们只能借鉴DiscoveryClientRouteLocator去改造SimpleRouteLocator使其具备刷新能力。123public interface RefreshableRouteLocator extends RouteLocator &#123; void refresh();&#125; DiscoveryClientRouteLocator比SimpleRouteLocator多了两个功能，第一是从DiscoveryClient（如Eureka）发现路由信息，之前的架构图已经给大家解释清楚了，我们不想使用eureka这种侵入式的网关模块，所以忽略它，第二是实现了RefreshableRouteLocator接口，能够实现动态刷新。对SimpleRouteLocator.class的源码加一些注释，方便大家阅读： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165public class SimpleRouteLocator implements RouteLocator &#123; //配置文件中的路由信息配置 private ZuulProperties properties; //路径正则配置器,即作用于path:/books/** private PathMatcher pathMatcher = new AntPathMatcher(); private String dispatcherServletPath = \"/\"; private String zuulServletPath; private AtomicReference&lt;Map&lt;String, ZuulRoute&gt;&gt; routes = new AtomicReference&lt;&gt;(); public SimpleRouteLocator(String servletPath, ZuulProperties properties) &#123; this.properties = properties; if (servletPath != null &amp;&amp; StringUtils.hasText(servletPath)) &#123; this.dispatcherServletPath = servletPath; &#125; this.zuulServletPath = properties.getServletPath(); &#125; //路由定位器和其他组件的交互，是最终把定位的Routes以list的方式提供出去,核心实现 @Override public List&lt;Route&gt; getRoutes() &#123; if (this.routes.get() == null) &#123; this.routes.set(locateRoutes()); &#125; List&lt;Route&gt; values = new ArrayList&lt;&gt;(); for (String url : this.routes.get().keySet()) &#123; ZuulRoute route = this.routes.get().get(url); String path = route.getPath(); values.add(getRoute(route, path)); &#125; return values; &#125; @Override public Collection&lt;String&gt; getIgnoredPaths() &#123; return this.properties.getIgnoredPatterns(); &#125; //这个方法在网关产品中也很重要，可以根据实际路径匹配到Route来进行业务逻辑的操作，进行一些加工 @Override public Route getMatchingRoute(final String path) &#123; if (log.isDebugEnabled()) &#123; log.debug(\"Finding route for path: \" + path); &#125; if (this.routes.get() == null) &#123; this.routes.set(locateRoutes()); &#125; if (log.isDebugEnabled()) &#123; log.debug(\"servletPath=\" + this.dispatcherServletPath); log.debug(\"zuulServletPath=\" + this.zuulServletPath); log.debug(\"RequestUtils.isDispatcherServletRequest()=\" + RequestUtils.isDispatcherServletRequest()); log.debug(\"RequestUtils.isZuulServletRequest()=\" + RequestUtils.isZuulServletRequest()); &#125; String adjustedPath = adjustPath(path); ZuulRoute route = null; if (!matchesIgnoredPatterns(adjustedPath)) &#123; for (Entry&lt;String, ZuulRoute&gt; entry : this.routes.get().entrySet()) &#123; String pattern = entry.getKey(); log.debug(\"Matching pattern:\" + pattern); if (this.pathMatcher.match(pattern, adjustedPath)) &#123; route = entry.getValue(); break; &#125; &#125; &#125; if (log.isDebugEnabled()) &#123; log.debug(\"route matched=\" + route); &#125; return getRoute(route, adjustedPath); &#125; private Route getRoute(ZuulRoute route, String path) &#123; if (route == null) &#123; return null; &#125; String targetPath = path; String prefix = this.properties.getPrefix(); if (path.startsWith(prefix) &amp;&amp; this.properties.isStripPrefix()) &#123; targetPath = path.substring(prefix.length()); &#125; if (route.isStripPrefix()) &#123; int index = route.getPath().indexOf(\"*\") - 1; if (index &gt; 0) &#123; String routePrefix = route.getPath().substring(0, index); targetPath = targetPath.replaceFirst(routePrefix, \"\"); prefix = prefix + routePrefix; &#125; &#125; Boolean retryable = this.properties.getRetryable(); if (route.getRetryable() != null) &#123; retryable = route.getRetryable(); &#125; return new Route(route.getId(), targetPath, route.getLocation(), prefix, retryable, route.isCustomSensitiveHeaders() ? route.getSensitiveHeaders() : null); &#125; //注意这个类并没有实现refresh接口，但是却提供了一个protected级别的方法,旨在让子类不需要重复维护一个private AtomicReference&lt;Map&lt;String, ZuulRoute&gt;&gt; routes = new AtomicReference&lt;&gt;();也可以达到刷新的效果 protected void doRefresh() &#123; this.routes.set(locateRoutes()); &#125; //具体就是在这儿定位路由信息的，我们之后从数据库加载路由信息，主要也是从这儿改写 /** * Compute a map of path pattern to route. The default is just a static map from the * &#123;@link ZuulProperties&#125;, but subclasses can add dynamic calculations. */ protected Map&lt;String, ZuulRoute&gt; locateRoutes() &#123; LinkedHashMap&lt;String, ZuulRoute&gt; routesMap = new LinkedHashMap&lt;String, ZuulRoute&gt;(); for (ZuulRoute route : this.properties.getRoutes().values()) &#123; routesMap.put(route.getPath(), route); &#125; return routesMap; &#125; protected boolean matchesIgnoredPatterns(String path) &#123; for (String pattern : this.properties.getIgnoredPatterns()) &#123; log.debug(\"Matching ignored pattern:\" + pattern); if (this.pathMatcher.match(pattern, path)) &#123; log.debug(\"Path \" + path + \" matches ignored pattern \" + pattern); return true; &#125; &#125; return false; &#125; private String adjustPath(final String path) &#123; String adjustedPath = path; if (RequestUtils.isDispatcherServletRequest() &amp;&amp; StringUtils.hasText(this.dispatcherServletPath)) &#123; if (!this.dispatcherServletPath.equals(\"/\")) &#123; adjustedPath = path.substring(this.dispatcherServletPath.length()); log.debug(\"Stripped dispatcherServletPath\"); &#125; &#125; else if (RequestUtils.isZuulServletRequest()) &#123; if (StringUtils.hasText(this.zuulServletPath) &amp;&amp; !this.zuulServletPath.equals(\"/\")) &#123; adjustedPath = path.substring(this.zuulServletPath.length()); log.debug(\"Stripped zuulServletPath\"); &#125; &#125; else &#123; // do nothing &#125; log.debug(\"adjustedPath=\" + path); return adjustedPath; &#125;&#125; 重写过后的自定义路由定位器如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169public class CustomRouteLocator extends SimpleRouteLocator implements RefreshableRouteLocator&#123; public final static Logger logger = LoggerFactory.getLogger(CustomRouteLocator.class); private JdbcTemplate jdbcTemplate; private ZuulProperties properties; public void setJdbcTemplate(JdbcTemplate jdbcTemplate)&#123; this.jdbcTemplate = jdbcTemplate; &#125; public CustomRouteLocator(String servletPath, ZuulProperties properties) &#123; super(servletPath, properties); this.properties = properties; logger.info(\"servletPath:&#123;&#125;\",servletPath); &#125; //父类已经提供了这个方法，这里写出来只是为了说明这一个方法很重要！！！// @Override// protected void doRefresh() &#123;// super.doRefresh();// &#125; @Override public void refresh() &#123; doRefresh(); &#125; @Override protected Map&lt;String, ZuulRoute&gt; locateRoutes() &#123; LinkedHashMap&lt;String, ZuulRoute&gt; routesMap = new LinkedHashMap&lt;String, ZuulRoute&gt;(); //从application.properties中加载路由信息 routesMap.putAll(super.locateRoutes()); //从db中加载路由信息 routesMap.putAll(locateRoutesFromDB()); //优化一下配置 LinkedHashMap&lt;String, ZuulRoute&gt; values = new LinkedHashMap&lt;&gt;(); for (Map.Entry&lt;String, ZuulRoute&gt; entry : routesMap.entrySet()) &#123; String path = entry.getKey(); // Prepend with slash if not already present. if (!path.startsWith(\"/\")) &#123; path = \"/\" + path; &#125; if (StringUtils.hasText(this.properties.getPrefix())) &#123; path = this.properties.getPrefix() + path; if (!path.startsWith(\"/\")) &#123; path = \"/\" + path; &#125; &#125; values.put(path, entry.getValue()); &#125; return values; &#125; private Map&lt;String, ZuulRoute&gt; locateRoutesFromDB()&#123; Map&lt;String, ZuulRoute&gt; routes = new LinkedHashMap&lt;&gt;(); List&lt;ZuulRouteVO&gt; results = jdbcTemplate.query(\"select * from gateway_api_define where enabled = true \",new BeanPropertyRowMapper&lt;&gt;(ZuulRouteVO.class)); for (ZuulRouteVO result : results) &#123; if(org.apache.commons.lang3.StringUtils.isBlank(result.getPath()) || org.apache.commons.lang3.StringUtils.isBlank(result.getUrl()) )&#123; continue; &#125; ZuulRoute zuulRoute = new ZuulRoute(); try &#123; org.springframework.beans.BeanUtils.copyProperties(result,zuulRoute); &#125; catch (Exception e) &#123; logger.error(\"=============load zuul route info from db with error==============\",e); &#125; routes.put(zuulRoute.getPath(),zuulRoute); &#125; return routes; &#125; public static class ZuulRouteVO &#123; /** * The ID of the route (the same as its map key by default). */ private String id; /** * The path (pattern) for the route, e.g. /foo/**. */ private String path; /** * The service ID (if any) to map to this route. You can specify a physical URL or * a service, but not both. */ private String serviceId; /** * A full physical URL to map to the route. An alternative is to use a service ID * and service discovery to find the physical address. */ private String url; /** * Flag to determine whether the prefix for this route (the path, minus pattern * patcher) should be stripped before forwarding. */ private boolean stripPrefix = true; /** * Flag to indicate that this route should be retryable (if supported). Generally * retry requires a service ID and ribbon. */ private Boolean retryable; private Boolean enabled; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public String getPath() &#123; return path; &#125; public void setPath(String path) &#123; this.path = path; &#125; public String getServiceId() &#123; return serviceId; &#125; public void setServiceId(String serviceId) &#123; this.serviceId = serviceId; &#125; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; public boolean isStripPrefix() &#123; return stripPrefix; &#125; public void setStripPrefix(boolean stripPrefix) &#123; this.stripPrefix = stripPrefix; &#125; public Boolean getRetryable() &#123; return retryable; &#125; public void setRetryable(Boolean retryable) &#123; this.retryable = retryable; &#125; public Boolean getEnabled() &#123; return enabled; &#125; public void setEnabled(Boolean enabled) &#123; this.enabled = enabled; &#125; &#125;&#125; 配置这个自定义的路由定位器： 123456789101112131415161718@Configurationpublic class CustomZuulConfig &#123; @Autowired ZuulProperties zuulProperties; @Autowired ServerProperties server; @Autowired JdbcTemplate jdbcTemplate; @Bean public CustomRouteLocator routeLocator() &#123; CustomRouteLocator routeLocator = new CustomRouteLocator(this.server.getServletPrefix(), this.zuulProperties); routeLocator.setJdbcTemplate(jdbcTemplate); return routeLocator; &#125;&#125; 现在容器启动时，就可以从数据库和配置文件中一起加载路由信息了，离动态路由还差最后一步，就是实时刷新，前面已经说过了，默认的ZuulConfigure已经配置了事件监听器，我们只需要发送一个事件就可以实现刷新了。 1234567891011121314public class RefreshRouteService &#123; @Autowired ApplicationEventPublisher publisher; @Autowired RouteLocator routeLocator; public void refreshRoute() &#123; RoutesRefreshedEvent routesRefreshedEvent = new RoutesRefreshedEvent(routeLocator); publisher.publishEvent(routesRefreshedEvent); &#125;&#125; 具体的刷新流程其实就是从数据库重新加载了一遍，有人可能会问，为什么不自己是手动重新加载Locator.dorefresh？非要用事件去刷新。这牵扯到内部的zuul内部组件的工作流程，不仅仅是Locator本身的一个变量，具体想要了解的还得去看源码。 到这儿我们就实现了动态路由了，所以的实例代码和建表语句我会放到github上，下载的时候记得给我star QAQ github地址","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://lexburner.github.io/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud Zuul","slug":"Spring-Cloud-Zuul","permalink":"http://lexburner.github.io/tags/Spring-Cloud-Zuul/"}]},{"title":"分布式限流","slug":"分布式限流","date":"2017-03-18T05:52:00.000Z","updated":"2017-08-22T06:40:13.761Z","comments":true,"path":"2017/03/18/分布式限流/","link":"","permalink":"http://lexburner.github.io/2017/03/18/分布式限流/","excerpt":"","text":"前言最近正在为本科论文的事感到心烦，一方面是在调研期间，发现大部分的本科论文都是以MVC为架构，如果是使用了java作为开发语言则又是千篇一律的在使用SSH，二方面是自己想就微服务，分布式方面写一篇论文，讲述一些技术点的实现，和一些中间件的使用，看到如八股文般的模板格式..不免让人望文生怯。退一步，投入模板化ssh-web项目的怀抱，落入俗套，可以省去自己不少时间，因为在外实习，琐事并不少；进一步，需要投入大量时间精力去研究，而且不成体系，没有论文参考。 突然觉得写博客，比写论文爽多了，可以写自己想写的，记录自己最真实的想法。可能会逐渐将之前博客维护的自己的一些想法，纳入到本科论文中去。 经典限流算法说回正题，补上之前分布式限流的实现。先介绍一些现有的限流方案。 核心的算法主要就是四种：A类：计数器法，滑动窗口法B类：令牌桶法，漏桶法 这里的四种算法通常都是在应用级别讨论的，这里不重复介绍这四种算法的实现思路了，只不过我人为的将他们分成了A，B两类。 A类算法，是否决式限流。即如果系统设定限流方案是1分钟允许100次调用，那么真实请求1分钟调用200次的话，意味着超出的100次调用，得到的是空结果或者调用频繁异常。 B类算法，是阻塞式限流。即如果系统设定限流方案是1分钟允许100次调用，那么真实请求1分钟调用200次的话，意味着超出的100次调用，会均匀安排到下一分钟返回。（当然B类算法，也可以立即返回失败，也可以达到否决式限流的效果） B类算法，如Guava包提供的RateLimiter，内部其实就是一个阻塞队列，达到阻塞限流的效果。然后分布式场景下，有一些思路悄悄的发生了变化。多个模块之间不能保证相互阻塞，共享的变量也不在一片内存空间中。为了使用阻塞限流的算法，我们不得不将统计流量放到redis一类的共享内存中，如果操作是一系列复合的操作，我们还不能使用redis自带的CAS操作(CAS操作只能保证单个操作的原子性)或者使用中间件级别的队列来阻塞操作，显示加分布式锁的开销又是非常的巨大。最终选择放弃阻塞式限流，而在分布式场景下，仅仅使用redis+lua脚本的方式来达到分布式-否决式限流的效果。redis执行lua脚本是一个单线程的行为，所以不需要显示加锁，这可以说避免了加锁导致的线程切换开销。 锁的演变下面记录一下这个设计的演变过程。 单体式应用中显示加锁首先还是回到单体应用中对共享变量进行+1的例子。12345678910111213141516171819Integer count = 0;//sychronized锁public synchronized void synchronizedIncrement()&#123; count++; &#125;//juc中的lockLock lock = new ReentrantLock(); public void incrementByLock()&#123; lock.lock(); try&#123; count++; &#125;finally &#123; lock.unlock(); &#125; &#125; 用synchronized或者lock同步的方式进行统计，当单位时间内到达限定次数后否决执行。限制：单体应用下有效，分布式场景失效，显示加锁，开销大。 单体式应用中CAS操作 12345public AtomicInteger atomicInteger = new AtomicInteger(0);public increamt()&#123; atomicInteger.incrementAndGet();&#125; 虽然没有显示加锁，但是CAS操作有一定的局限性，限流中不仅要对计数器进行+1，而且还要记录时间段，所以复合操作，还是无法避免加锁。 分布式应用中显示加锁 1234567891011RedisDistributeLock lock = new RedisDistributeLock();public void incrementByLock()&#123; lock.lock(); try&#123; count++; &#125;finally &#123; lock.unlock(); &#125;&#125; 分布式阻塞锁的实现，可以参考我之前的博客。虽然能达到多个模块之间的同步，但还是开销过大。不得已时才会考虑使用。 redis+lua脚本限流（最终方案） 12345678910111213local key = KEYS[1] --限流KEY（一秒一个）local limit = tonumber(ARGV[1]) --限流大小local current = tonumber(redis.call(&apos;get&apos;, key) or &quot;0&quot;)if current + 1 &gt; limit then --如果超出限流大小 redis.call(&quot;INCRBY&quot;, key,&quot;1&quot;) -- 如果不需要统计真是访问量可以不加这行 return 0else --请求数+1，并设置2秒过期 redis.call(&quot;INCRBY&quot;, key,&quot;1&quot;) if tonumber(ARGV[2]) &gt; -1 then redis.call(&quot;expire&quot;, key,tonumber(ARGV[2])) --时间窗口最大时间后销毁键 end return 1end lua脚本返回值比较奇怪，用java客户端接受返回值，只能使用Long，没有去深究。这个脚本只需要传入key（url+时间戳/预设时间窗口大小），便可以实现限流。这里也贴下java中配套的工具类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package sinosoftgz.apiGateway.utils;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.core.script.RedisScript;import org.springframework.util.Assert;import java.util.Arrays;/** * Created by xujingfeng on 2017/3/13. * &lt;p&gt; * 基于redis lua脚本的线程安全的计数器限流方案 * &lt;/p&gt; */public class RedisRateLimiter &#123; /** * 限流访问的url */ private String url; /** * 单位时间的大小,最大值为 Long.MAX_VALUE - 1,以秒为单位 */ final Long timeUnit; /** * 单位时间窗口内允许的访问次数 */ final Integer limit; /** * 需要传入一个lua script,莫名其妙redisTemplate返回值永远是个Long */ private RedisScript&lt;Long&gt; redisScript; private RedisTemplate redisTemplate; /** * 配置键是否会过期， * true：可以用来做接口流量统计，用定时器去删除 * false：过期自动删除，时间窗口过小的话会导致键过多 */ private boolean isDurable = false; public void setRedisScript(RedisScript&lt;Long&gt; redisScript) &#123; this.redisScript = redisScript; &#125; public void setRedisTemplate(RedisTemplate redisTemplate) &#123; this.redisTemplate = redisTemplate; &#125; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; public boolean isDurable() &#123; return isDurable; &#125; public void setDurable(boolean durable) &#123; isDurable = durable; &#125; public RedisRateLimiter(Integer limit, Long timeUnit) &#123; this.timeUnit = timeUnit; Assert.isTrue(timeUnit &lt; Long.MAX_VALUE - 1); this.limit = limit; &#125; public RedisRateLimiter(Integer limit, Long timeUnit, boolean isDurable) &#123; this(limit, timeUnit); this.isDurable = isDurable; &#125; public boolean acquire() &#123; return this.acquire(this.url); &#125; public boolean acquire(String url) &#123; StringBuffer key = new StringBuffer(); key.append(&quot;rateLimiter&quot;).append(&quot;:&quot;) .append(url).append(&quot;:&quot;) .append(System.currentTimeMillis() / 1000 / timeUnit); Integer expire = limit + 1; String convertExpire = isDurable ? &quot;-1&quot; : expire.toString(); return redisTemplate.execute(redisScript, Arrays.asList(key.toString()), limit.toString(), convertExpire).equals(1l); &#125;&#125; 由此可以见，分布式场景下，一个小小的统计次数的需求，如果真想在分布式下做到最完善，需要花很大的精力。","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/categories/架构设计/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://lexburner.github.io/tags/redis/"},{"name":"lua","slug":"lua","permalink":"http://lexburner.github.io/tags/lua/"}]},{"title":"DevOps的八荣八耻","slug":"DevOps的八荣八耻","date":"2017-03-13T16:43:52.000Z","updated":"2017-08-22T06:46:24.382Z","comments":true,"path":"2017/03/14/DevOps的八荣八耻/","link":"","permalink":"http://lexburner.github.io/2017/03/14/DevOps的八荣八耻/","excerpt":"","text":"前言被群里的好友安利了一发，周日跑去参加了一个技术讲座《云上开发与运维最佳实践》，听完两个人的演讲之后才发现主题竟然是讲运维，好在有一个人干货不少，在此记录下所得。简单追溯了一下这个DevOps才发现并不是一个新的概念，早在2010年就能看到有相关的人在追捧这个概念了。DevOps 就是开发（Development）和运维（Operations）这两个领域的合并。（如果没错的话，DevOps还包括产品管理、QA、winces 甚至销售等领域）。这种理念和现如今流行的微服务架构以及分布式特性的相关理念不谋而合。这篇文章主要就是转载记录了当时又拍云运维总监的演讲稿。 DevOps的八荣八耻DevOps这个思想提出来已经五六年了，一直都是呼声很高，落地很难，为什么呢？这可能与各个公司的业务情况和技术发展路线有或多或少的关系，比如说创业的最早技术合伙人是运维出身或者技术出身，但是水平不高，为了公司持续发展，引入新鲜血液时，就会存在技术的先进性跟解决遗留烂摊子的矛盾。又或者业务本身偏向于用户，导致技术被边缘化，产品又没有好的架构，限制了快速发展等；所以，DevOps的推进一定要自上而下，凭借挑战自我，颠覆传统的勇气才能去落实。 以可配置为荣，以硬编码为耻 △ 以可配置为荣，以硬编码为耻 hardcoding一时爽，真正要做改动时，需要定位代码，做出调整，甚至可能会破坏功能。以下可以说是配置的一个进化史 • 本地配置,程序⽣生成 (txt/ini/cfg)• 集中配置, 动态⽣生成(Yaml/Json)• 环境变量量(代码⽆无侵⼊入&amp;语⾔言⽆无关性)• 服务⾃自动发现,⾃自动注册(zookeeper/consul) 以互备为荣，以单点为耻 △ 以互备为荣，以单点为耻 互容互备一直是优良架构的设计重点。 又拍云早期做架构设计，使用了LVS+Keeplived+VRRP做转换，这样可以方便负载均衡，动态升级，隔离故障。现在的又拍云第二代，已经在部分大节点使用OSPF和Quagga做等价路由的负载均衡和冗余保障。 Nginx可以加Haproxy或LVS做负载均衡。MySQL可以做主从切换，或者是MMM的高可用成熟解决方案。我们的消息队列之前用rabbitmq做，现在主要是redis和kafka集群化，其中kafka已经迁到了Mesos容器平台里。 服务的自动发现、注册，我们可以使用consul、etcd、doozer（Heroku公司产品），还有zookeeper。主要区别是算法不一样，zookeeper用的是paxos算法，而consul用的是raft算法。目前看来consul比较流行，因为consul的自动发现和自动注册更加容易使用。etcd主要是CoreOS在主推，CoreOS本身就是一个滚动发布的针对分布式部署的操作系统，大家可以去关注一下它。还有一个是hadoop和elk，大数据平台的可扩展性是标配，很容易互备。 上面是举了一些常见互备的软件组件的造型，那我们如何是设计一个无单点的架构呢？主要掌握以下几点： 1.无状态 无状态意味着没有竞争，很容易做负载均衡，负载均衡的方式有很多种，F5，LVS，Haproxy，总能找到一种适合你的方式。 2.无共享 以前我们很喜欢用内存来保持临时信息，如进程间的交换，这种方式虽然效率很高，但是对程序的扩展性没什么好处，尤其是现在的互联网体量，光靠单机或者高性能机器是明显玩不转的。所以我们现在就需要使用类似消息队列的组件，把数据共享出去，利用多台机器把负载给承担下来。 3.松耦合/异步处理 以前我们用Gearman这样的任务框架。大家可以把任务丢进任务池里，生成多个消费者去取任务。当我的消费不够用时，可以平滑增加我的work资源，让他从更快的去拿任务。运维平台这边以python/celery的组合使用更多。 4.分布式/集群协作 像Hadoop这样的天生大数据/数据仓库解决方案，由于先前设计比较成熟，一般都是通过很多台机器扩容来实现map/reduce的扩展计算能力。 以随时重启为荣，以不能迁移为耻 △ 以随时重启为荣，以不能迁移为耻 关于这个点，我们讲三个方面： 1.Pet到Cow观念的转变 以前我们说机器是pet，也就是宠物模式，然后花了几万块钱去买的服务器，当宝一般供奉。但事实上却并不是这样，任何电子设备、服务器只要一上线，便开始了一个衰老的过程，你根本不知道在运行过程中会发生什么事，比如说质量差的电容会老化爆浆，电子元器件在机房的恶劣环境里会加速损坏，这些变化都是我们无法参与控制的，所以无论我们怎么努力，都无法保障机器有多么的牢靠。 谷歌指出的Cow模式就是指农场模式。就是要把机器发生故障当做常态，打个比方，比如说这头牛死了，那我就不要了，因为我有很多这样的牛，或者是再拉一头新的牛。这就是我们软件开发和运维需要做的转变，去适应这种变化。 2.OpenStack虚拟机的编排 虚拟化是个好东西，通过OpenStack我们很容易就可以做出一些存储或者迁移的操作，但是在实施的过程中，也是一波三折的。 又拍云从2014年开始在内部推动OpenStack，当然我们也踩过OpenStack网络的坑，那时候我们用双千兆的卡做内网通讯，因为使用OpenStack实现虚拟化后，一切都变成了文件，在网络上传输的话，对网络的压力会非常大，结果就导致部分服务响应缓慢（因为本身就是实验性质，所以在硬件上没有足够投入，内测时也没有推广，所以影响不大）。 2015年又拍云再上的OpenStack，全部都用双万兆的网卡做bonding，交换机也是做了端口聚合和堆叠。目前来说，只有云存储没有上线，其它云处理，云网络的使用还是能够满足要求。 3.Docker的导入导出 Docker是更轻量级的资源隔离和复用技术，从2016年开始，又拍云同时也在尝试使用Mesos/Docker来实现云处理的业务迁移。 以整体交付为荣，以部分交付为耻 △ 以整体交付为荣，以部分交付为耻 以往开发运维要安装一个机器，首先要去申请采购，购买完了还要等待运输，在运输中要花去一天的时间，之后还需要配交换机和网络。在这个过程中你会发现，简单的给开发配台机器，光上架就涉及到运维的很多环节，更不要说系统安装，优化，软件配置等剩余工作了，所以大多数情况下你只能做到部分交付。 要如何解决这些问题？通过OpenStack可以做到云计算、云网络、云存储这三块搭建完成之后，进行整体交付。 根据一些经验总结，在整个云平台当中，云存储的坑最多，云计算、云网络相对来说比较成熟。现在云计算的硬件基本上是基于英特尔CPU的虚拟化技术来硬件指令穿透的，损耗大概2%～5%，这是可以接受的。至于云网络，刚才胡凯（B站运维总监）提到内网包转发效率，我做过一个测试，在OpenStack的内网中，如果MTU默认是1500，万兆网卡的转发率大概为6.7xxGbps。后来我在优化的过程中，也翻查一些文档，看到的数据是可以达到9.5xxGbps，通过不断的摸索，对比测试后发现，如果把内网的MTU搞成大包，如9000时，万兆网卡的存储量直接达到了9.72Gbps左右的。不过，这个MTU需要提前在宿主机上调整好，需要重启生效。所以，这个问题发现得越早越好，这样就可以做到统一调度，分配资源。 Docker的好处是可以做到Build、Shipand Run，一气呵成。无论是对开发，测试，还是运维来说，Docker都是同一份Dockerfile清单，所以使用Docker在公司里的推动就很顺畅。虽然OpenStack也可以一站式交付，整体交付，使用时非常方便。但是对开发来说，他还是拿到一台机器，还是需要去安装软件环境，配置，上线，运行，除了得到机器快一些，对上线服务没有什么大的帮助，所以又拍云现在的Openstack集群一般对内申请开发测试用，外网生产环境还是以Docker容器化部署为主，这也是大家都喜闻乐见的方式，但前提是开发那边能够适应编写Dockerfile（目前是我在内部推动这种变革，如新的项目就强制要求用docker）。 以无状态为荣，以有状态为耻 △ 以无状态为荣，以有状态为耻 有状态的服务真的很麻烦，无论是存在数据库、磁盘开销，还有各种锁等资源的竞争，横向扩展也很差，不能重启，也不能互备。所以，有姿态的服务对于扩展原则来说，就是一场恶梦。如果是说我们解决这个问题，那就要使用解耦和负载均衡的方法去解决问题。 1.使用可靠的中间件 中间件其实最早出现在金融公司、证券公司，后来随着互联网行业不断壮大以后，就用一些高可靠性的号称工业级的消息队列出现，如RabbitMQ，一出来以后，就把中间件拉下神坛。随着中间件民用化，互联网蓬勃发展，是可以把一些服务变成无状态，方便扩展。 2.公共资源池 我们可以通过各种云，容器云、弹性云，做计算单元的弹性扩展。 3.能够被计算 如果你不想存状态，那也可以被计算，比如说Ceph存储，它的创新在于每个数据块都是可计算出来的，这就类似无状态的，每次都算，反正现在的cpu都这么强悍了，所以，无状态是一个命题，在做架构的时候，你脑海里一定要有这个意念，然后再看你用什么样的方式开动脑筋，预先的跟开发，运维沟通好，把应用拆分成一种无状态的最佳组合。 以标准化为荣，以特殊化为耻 △ 以标准化为荣，以特殊化为耻 在标准化方面，我们在这几个方面改良： 1.统一输入输出 统一入口是我加入又拍云后做的第一件事情，我们用一个统一的文本，到现在也在用，然后推送到所有的边缘，服务器上面的组件，要用到的参数，都能从配置里读出来。代码管理方面我们也使用git，git wiki，批量部署我们用ansible（早在2012年，我做了一些比较后，就在公司里推行ansible，看来还是很明智的决定）。 2.统一的流程管理 运维中使用python最多，所以我们使用了yaml和playbook。又拍云有自己的跳板机，通过VPN登陆，目前我们也在试用一个带有审计功能的堡垒机，可以把每个人的操作录制下来，然后再去回放观察，改进我们的工作流程。 3.抽象底层设计和复用组件 如果是开发者的话，就会写很多的复用函数，对于优秀的运维人员来说，也要有优秀的抽象业务的能力，也要去做一些重复工作的复用准备，如频繁的，繁琐易出错的手工操作抽象成若干运维的脚本化。 最后是巧妙的利用虚拟化、容器服务、server-less微服务，这些服务是可以被备份，还原的，可以保持一个相对稳定的状态，我们要拒绝多的特殊管理操作。香农-信息熵理论里说，变量的不确定性越大，熵就越大，把它搞清楚所需要的信息量也就越大。理论上来说，如果是一个孤立的系统，他就会变得越来越乱。 以自动化工具为荣，以手动和人肉为耻 △ 以自动化工具为荣，以手动和人肉为耻 又拍云早期，用的是bash、sed、awk，因为我之前有搞嵌入式的背景和经验，对一个十几兆的嵌入式系统来说，上面是不可能有python/perl/nodejs等环境。所以我们把服务器批量安装，部署，上线，做成了嵌入式的系统后，只要点亮以后，运行一个硬件检测的程序，会把机器的CPU、内存、硬盘大小等都打印出来，供货商截图给我看，这个机器是否合格。合格的机器可以直接发到机房去，在机器到了机房通上网线以后会有一个ansibleplaybook的推动。 自从用了这种方法以后，我们在公司里面基本上没有见到服务器，一般直接产线上检测通过后发到机房。然后又拍云的运维人员就可以连上去远程管理，在过去的三年里我们服务器平均每年翻了三倍，节点翻了六倍多，但是人手并没有增加。 关于tgz、rpm、pkg的打包部署，我们用的是tgz的打包及docker镜像。优势在于，又拍云自有CDN网络，软件通过推动到CDN网络下可以加速下发。 关于集成测试、自动测试的发布，像ELK集中日志的分析、大数据的分析，我们现在使用ELK以后，只要有基础的运维技术知识便可看懂，不需要高深的运维知识和脚本编辑知识，大多数人都可以完成这份工作，好处就是你多了好多眼睛帮你一起来发现问题，定位问题。 最后是不要图形，不要交互，不要终端。一旦有了图形以后，很难实现自动化。原则就是，不要手工hack，最好是用程序生成程序的方式去完成这个步骤。 以无人值守为荣，以人工介入为耻 △ 以无人值守为荣，以人工介入为耻 运维部门要做的事情有三件： 1.运维自动化 要有一定的业务抽象能力，要有标准化的流程。没有好的自动化，就很难把运维的工作效率提升了，只要做好这些，就可以节省时间，从容应对业务增长。而且运维自动化的另一个好处就是运维不会因为人的喜怒哀乐而受到影响稳定性，比如说我今天心情不好，你让我装一台机器我还可以忍，你让我装十台一百台就不行了。但如果公司有了运维自动化的流程，这个事情就可以避免，因为谁做都一样。 2.监控要常态 2016年年初，又拍云特别成立大数据分析部门，我们把日志做了采样收集和过滤，通过大数据平台做日志的同构数据分析，重点关注4xx/5xx/2xx比例，响应时间分析如100毫秒、200毫秒、500毫秒，还有区域性的速率分布，讲真，这真是一个好东西。 3.性能可视化 数据的有效展示。现在ELK对我们的帮助很大，从监控图上来看相关的数据指标，一目了然。这里就不反复赘述了。 DevOps的本质最后，我们谈一谈DevOps的本质。 弹性 像亚马逊推云时，那个单词叫elastic，意思是，你要能够扩展，如横向扩展；你要能负载均衡，如果你是基于openstack/docker资源池，你的资源就可以复用，可以编排回滚。比如说OpenStack有模板，我打一个镜像包，稍微重了一点，Docker的就轻一点，Docker可以做一个滚动发布，可以保留原来的程序、原来的容器，你可以做快速切换，这也是一种变化的弹性。 无关性 如果是虚拟化资源，一切都可以在模板里面设置，可以把底层的硬件、系统、网络抚平差异，比如说不管物理磁盘是1T(市面上缺货)/4T/6T的盘，都可以划分100G容量，所以当把一切变成按需申请的服务，无论是开发还是运维，工作都会比较简单，因为它的无关性。 不可变的基础设施 这个对传统运维可能是一种打击，因为基础镜像可能已经做的足够安全，足够完美，足够精干，不需要基础运维过多的人工参与。但我认为恰恰能帮助传统运维减轻工作量，反而有更多的精力去迎接虚拟化、容器化，SDN的挑战，掌握了新技能后，就可以随取随用。","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://lexburner.github.io/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"http://lexburner.github.io/tags/DevOps/"}]},{"title":"java并发实践--ConcurrentHashMap与CAS","slug":"java并发实践--ConcurrentHashMap与CAS","date":"2017-03-11T16:02:00.000Z","updated":"2017-08-22T06:56:00.534Z","comments":true,"path":"2017/03/12/java并发实践--ConcurrentHashMap与CAS/","link":"","permalink":"http://lexburner.github.io/2017/03/12/java并发实践--ConcurrentHashMap与CAS/","excerpt":"前言最近在做接口限流时涉及到了一个有意思问题，牵扯出了关于concurrentHashMap的一些用法，以及CAS的一些概念。限流算法很多，我主要就以最简单的计数器法来做引。先抽象化一下需求：统计每个接口访问的次数。一个接口对应一个url，也就是一个字符串，每调用一次对其进行加一处理。可能出现的问题主要有三个： 多线程访问，需要选择合适的并发容器 分布式下多个实例统计接口流量需要共享内存 流量统计应该尽可能不损耗服务器性能 但这次的博客并不是想描述怎么去实现接口限流，而是主要想描述一下遇到的问题，所以，第二点暂时不考虑，即不使用redis。 说到并发的字符串统计，立即让人联想到的数据结构便是ConcurrentHashpMap&lt;String,Long&gt; urlCounter;","text":"前言最近在做接口限流时涉及到了一个有意思问题，牵扯出了关于concurrentHashMap的一些用法，以及CAS的一些概念。限流算法很多，我主要就以最简单的计数器法来做引。先抽象化一下需求：统计每个接口访问的次数。一个接口对应一个url，也就是一个字符串，每调用一次对其进行加一处理。可能出现的问题主要有三个： 多线程访问，需要选择合适的并发容器 分布式下多个实例统计接口流量需要共享内存 流量统计应该尽可能不损耗服务器性能 但这次的博客并不是想描述怎么去实现接口限流，而是主要想描述一下遇到的问题，所以，第二点暂时不考虑，即不使用redis。 说到并发的字符串统计，立即让人联想到的数据结构便是ConcurrentHashpMap&lt;String,Long&gt; urlCounter; 如果你刚刚接触并发可能会写出如代码清单1的代码 代码清单112345678910111213141516171819202122232425262728293031323334353637383940414243444546public class CounterDemo1 &#123; private final Map&lt;String, Long&gt; urlCounter = new ConcurrentHashMap&lt;&gt;(); //接口调用次数+1 public long increase(String url) &#123; Long oldValue = urlCounter.get(url); Long newValue = (oldValue == null) ? 1L : oldValue + 1; urlCounter.put(url, newValue); return newValue; &#125; //获取调用次数 public Long getCount(String url)&#123; return urlCounter.get(url); &#125; public static void main(String[] args) &#123; ExecutorService executor = Executors.newFixedThreadPool(10); final CounterDemo1 counterDemo = new CounterDemo1(); int callTime = 100000; final String url = \"http://localhost:8080/hello\"; CountDownLatch countDownLatch = new CountDownLatch(callTime); //模拟并发情况下的接口调用统计 for(int i=0;i&lt;callTime;i++)&#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; counterDemo.increase(url); countDownLatch.countDown(); &#125; &#125;); &#125; try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; executor.shutdown(); //等待所有线程统计完成后输出调用次数 System.out.println(\"调用次数：\"+counterDemo.getCount(url)); &#125;&#125;console output：调用次数：96526 都说concurrentHashMap是个线程安全的并发容器，所以没有显示加同步，实际效果呢并不如所愿。 问题就出在increase方法，concurrentHashMap能保证的是每一个操作（put，get,delete…）本身是线程安全的，但是我们的increase方法，对concurrentHashMap的操作是一个组合，先get再put，所以多个线程的操作出现了覆盖。如果对整个increase方法加锁，那么又违背了我们使用并发容器的初衷，因为锁的开销很大。我们有没有方法改善统计方法呢？代码清单2罗列了concurrentHashMap父接口concurrentMap的一个非常有用但是又常常被忽略的方法。 代码清单21234567891011121314/** * Replaces the entry for a key only if currently mapped to a given value. * This is equivalent to * &lt;pre&gt; &#123;@code * if (map.containsKey(key) &amp;&amp; Objects.equals(map.get(key), oldValue)) &#123; * map.put(key, newValue); * return true; * &#125; else * return false; * &#125;&lt;/pre&gt; * * except that the action is performed atomically. */ boolean replace(K key, V oldValue, V newValue); 这其实就是一个最典型的CAS操作，except that the action is performed atomically.这句话真是帮了大忙，我们可以保证比较和设置是一个原子操作，当A线程尝试在increase时，旧值被修改的话就回导致replace失效，而我们只需要用一个循环，不断获取最新值，直到成功replace一次，即可完成统计。 改进后的increase方法如下 代码清单31234567891011121314151617181920212223public long increase2(String url) &#123; Long oldValue, newValue; while (true) &#123; oldValue = urlCounter.get(url); if (oldValue == null) &#123; newValue = 1l; //初始化成功，退出循环 if (urlCounter.putIfAbsent(url, 1l) == null) break; //如果初始化失败，说明其他线程已经初始化过了 &#125; else &#123; newValue = oldValue + 1; //+1成功，退出循环 if (urlCounter.replace(url, oldValue, newValue)) break; //如果+1失败，说明其他线程已经修改过了旧值 &#125; &#125; return newValue;&#125;console output：调用次数：100000 再次调用后获得了正确的结果，上述方案看上去比较繁琐，因为第一次调用时需要进行一次初始化，所以多了一个判断，也用到了另一个CAS操作putIfAbsent，他的源代码描述如下： 代码清单412345678910111213141516171819202122232425/** * If the specified key is not already associated * with a value, associate it with the given value. * This is equivalent to * &lt;pre&gt; &#123;@code * if (!map.containsKey(key)) * return map.put(key, value); * else * return map.get(key); * &#125;&lt;/pre&gt; * * except that the action is performed atomically. * * @implNote This implementation intentionally re-abstracts the * inappropriate default provided in &#123;@code Map&#125;. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with the specified key, or * &#123;@code null&#125; if there was no mapping for the key. * (A &#123;@code null&#125; return can also indicate that the map * previously associated &#123;@code null&#125; with the key, * if the implementation supports null values.) */ V putIfAbsent(K key, V value); 简单翻译如下：“如果（调用该方法时）key-value 已经存在，则返回那个 value 值。如果调用时 map 里没有找到 key 的 mapping，返回一个 null 值”。值得注意点的一点就是concurrentHashMap的value是不能存在null值的。实际上呢，上述的方案也可以把Long替换成AtomicLong，可以简化实现， ConcurrentHashMap。 juc包下的各类Atomic类也提供了大量的CAS操作，可以不用加锁，也可以实现原子操作，以后看到其他类库有类似比较后设值，不存在即设值，加一并获取返回值等等一系列的组合操作合并成了一个接口的，都应该意识到很有可能是CAS操作。如redis的IncreamtAndGet，setIfAbsent，Atomic类的一系列api，以及上述描述的concurrentHashMap中相关的api（不同api的CAS组合接口可能名称类似，但是返回值含义不大相同，我们使用CAS的api很大程度需要获取其返回值来进行分支处理，所以一定要搞清楚每个接口的特性。如redistemplate提供的setIfAbsent，当设置成功时返回的是true，而与之名称类似的ConcurrentHashMap的putIfAbsent在设置成功后返回的是null，要足够小心，加以区分）。凡事没有绝对，但是一个大体上正确的编程建议便是能使用编程类库并发容器（线程安全的类）完成的操作，尽量不要显示加锁同步。 再扯一句关于CAS的知识点，CAS不能代替同步，由它引出了一个经典的ABA问题，即修改过一次之后，第二次修改又变为了原值，可能会在一些逻辑中出现问题。不过对于计数这个逻辑而言，只是单调的增，不会受到影响。 最后介绍一个和主题非常贴切的并发容器：Guava包中AtomicLongMap，使用他来做计数器非常容易。 代码清单51234567891011private AtomicLongMap&lt;String&gt; urlCounter3 = AtomicLongMap.create();public long increase3(String url) &#123; long newValue = urlCounter3.incrementAndGet(url); return newValue;&#125;public Long getCount3(String url) &#123; return urlCounter3.get(url);&#125; 看一下他的源码就会发现，其实和代码清单3思路差不多，只不过功能更完善了一点。 和CAS很像的操作，我之前的博客中提到过数据库的乐观锁，用version字段来进行并发控制，其实也是一种compare and swap的思想。 杂谈：网上很多对ConcurrentHashMap的介绍，众所周知，这是一个用分段锁实现的一个线程安全的map容器，但是真正对他的使用场景有介绍的少之又少。面试中能知道这个容器的人也确实不少，问出去，也就回答一个分段锁就没有下文了，但我觉得吧，有时候一知半解反而会比不知道更可怕。 参考 https://my.oschina.net/mononite/blog/144329 http://www.tuicool.com/articles/zuui6z","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"},{"name":"多线程","slug":"多线程","permalink":"http://lexburner.github.io/tags/多线程/"}]},{"title":"volatile疑问记录","slug":"volatile疑问记录","date":"2017-03-07T11:26:52.000Z","updated":"2017-08-22T07:12:44.354Z","comments":true,"path":"2017/03/07/volatile疑问记录/","link":"","permalink":"http://lexburner.github.io/2017/03/07/volatile疑问记录/","excerpt":"","text":"对java中volatile关键字的描述，主要是可见性和有序性两方面。 一个很广泛的应用就是使得多个线程对共享资源的改动变得互相可见，如下： 123456789101112131415161718192021222324252627282930313233343536public class TestVolatile extends Thread &#123; /*A*/// public volatile boolean runFlag = true; public boolean runFlag = true; public boolean isRunFlag() &#123; return runFlag; &#125; public void setRunFlag(boolean runFlag) &#123; this.runFlag = runFlag; &#125; @Override public void run() &#123; System.out.println(\"进入run\"); while (isRunFlag()) &#123; /*B*/// System.out.println(\"running\"); &#125; System.out.println(\"退出run\"); &#125; public static void main(String[] args) throws InterruptedException &#123; TestVolatile testVolatile = new TestVolatile(); testVolatile.start(); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; testVolatile.setRunFlag(false); System.out.println(\"main already set runflag to false\"); new CountDownLatch(1).await(); &#125;&#125; 在A处如果不将运行标记（runflag）设置成volatile，那么main线程对runflag的修改对于testVolatile线程将不可见。导致其一直不打印“退出run”这句。 但是如果在testVolatile线程的while()增加一句：B处打印语句，程序却达到了不使用volatile，修改也变得可见，不知道到底是什么原理。 只能大概估计是while()的执行过程中线程上下文进行了切换，使得重新去主存获取了runflag的最新值，从而退出了循环，暂时记录… 2017/3/8日更新和群里面的朋友讨论了一下，发现同一份代码，不同的机器运行出了不一样的效果。又仔细翻阅了一下《effective java》，依稀记得当时好像遇到过这个问题，果然，在并发的第一张就对这个现象做出了解释。关键就在于HotSpot Server VM对编译进行了优化，这种优化称之为提升(hoisting)，结果导致了活性失败（liveness failure） 1while (isRunFlag()) &#123;&#125; 会被优化成 123if(isRunFlag())&#123; while(true)...&#125; 引用effective java这一节的原话： 简而言之，当多个线程共享可变数据的时候，每个读或者写数据的线程都必须执行同步如果没有同步，就无法保证一个线程所做的修改可以被另一个线程获知。未能同步共享可变数据会造成程序的活性失败和安全性失败。这样的失败是难以调式的。他们可能是间歇性的，且与时间相关，程序的行为在不同的VM上可能根本不同，如果只需要线程之间的交互通信，而不需要互斥，volatile修饰符就是一种可以接受的同步形式，但是正确的使用它可能需要一些技巧。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"},{"name":"多线程","slug":"多线程","permalink":"http://lexburner.github.io/tags/多线程/"}]},{"title":"浅析java内存模型（JMM）","slug":"浅析java内存模型（JMM）","date":"2017-02-24T05:07:52.000Z","updated":"2017-08-22T07:16:41.143Z","comments":true,"path":"2017/02/24/浅析java内存模型（JMM）/","link":"","permalink":"http://lexburner.github.io/2017/02/24/浅析java内存模型（JMM）/","excerpt":"","text":"并发编程模型的分类在并发编程中，我们需要处理两个关键问题：线程之间如何通信及线程之间如何同步（这里的线程是指并发执行的活动实体）。通信是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。 在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信。 同步是指程序用于控制不同线程之间操作发生相对顺序的机制。在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。如果编写多线程程序的Java程序员不理解隐式进行的线程之间通信的工作机制，很可能会遇到各种奇怪的内存可见性问题。 Java内存模型的抽象在java中，所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享（本文使用“共享变量”这个术语代指实例域，静态域和数组元素）。局部变量（Local variables），方法定义参数（java语言规范称之为formal method parameters）和异常处理器参数（exception handler parameters）不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。 Java线程之间的通信由Java内存模型（本文简称为JMM）控制，JMM决定一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。Java内存模型的抽象示意图如下： 从上图来看，线程A与线程B之间如要通信的话，必须要经历下面2个步骤： 首先，线程A把本地内存A中更新过的共享变量刷新到主内存中去。 然后，线程B到主内存中去读取线程A之前已更新过的共享变量。 下面通过示意图来说明这两个步骤： 如上图所示，本地内存A和B有主内存中共享变量x的副本。假设初始时，这三个内存中的x值都为0。线程A在执行时，把更新后的x值（假设值为1）临时存放在自己的本地内存A中。当线程A和线程B需要通信时，线程A首先会把自己本地内存中修改后的x值刷新到主内存中，此时主内存中的x值变为了1。随后，线程B到主内存中去读取线程A更新后的x值，此时线程B的本地内存的x值也变为了1。 从整体来看，这两个步骤实质上是线程A在向线程B发送消息，而且这个通信过程必须要经过主内存。JMM通过控制主内存与每个线程的本地内存之间的交互，来为java程序员提供内存可见性保证。 重排序在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型： 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从java源代码到最终实际执行的指令序列，会分别经历下面三种重排序： 上述的1属于编译器重排序，2和3属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM的处理器重排序规则会要求java编译器在生成指令序列时，插入特定类型的内存屏障（memory barriers，intel称之为memory fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。 JMM属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 处理器重排序与内存屏障指令现代的处理器使用写缓冲区来临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，可以减少对内存总线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读/写操作的执行顺序，不一定与内存实际发生的读/写操作顺序一致！为了具体说明，请看下面示例： Processor A Processor B a = 1; //A1x = b; //A2 b = 2; //B1y = a; //B2 初始状态：a = b = 0处理器允许执行后得到结果：x = y = 0 假设处理器A和处理器B按程序的顺序并行执行内存访问，最终却可能得到x = y = 0的结果。具体的原因如下图所示： 这里处理器A和处理器B可以同时把共享变量写入自己的写缓冲区（A1，B1），然后从内存中读取另一个共享变量（A2，B2），最后才把自己写缓存区中保存的脏数据刷新到内存中（A3，B3）。当以这种时序执行时，程序就可以得到x = y = 0的结果。 从内存操作实际发生的顺序来看，直到处理器A执行A3来刷新自己的写缓存区，写操作A1才算真正执行了。虽然处理器A执行内存操作的顺序为：A1-&gt;A2，但内存操作实际发生的顺序却是：A2-&gt;A1。此时，处理器A的内存操作顺序被重排序了（处理器B的情况和处理器A一样，这里就不赘述了）。 这里的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操做重排序。 下面是常见处理器允许的重排序类型的列表： Load-Load Load-Store Store-Store Store-Load 数据依赖 sparc-TSO N N N Y N x86 N N N Y N ia64 Y Y Y Y N PowerPC Y Y Y Y N 上表单元格中的“N”表示处理器不允许两个操作重排序，“Y”表示允许重排序。 从上表我们可以看出：常见的处理器都允许Store-Load重排序；常见的处理器都不允许对存在数据依赖的操作做重排序。sparc-TSO和x86拥有相对较强的处理器内存模型，它们仅允许对写-读操作做重排序（因为它们都使用了写缓冲区）。 ※注1：sparc-TSO是指以TSO(Total Store Order)内存模型运行时，sparc处理器的特性。 ※注2：上表中的x86包括x64及AMD64。 ※注3：由于ARM处理器的内存模型与PowerPC处理器的内存模型非常类似，本文将忽略它。 ※注4：数据依赖性后文会专门说明。 为了保证内存可见性，java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM把内存屏障指令分为下列四类： 屏障类型 指令示例 说明 LoadLoad Barriers Load1; LoadLoad; Load2 确保Load1数据的装载，之前于Load2及所有后续装载指令的装载。 StoreStore Barriers Store1; StoreStore; Store2 确保Store1数据对其他处理器可见（刷新到内存），之前于Store2及所有后续存储指令的存储。 LoadStore Barriers Load1; LoadStore; Store2 确保Load1数据装载，之前于Store2及所有后续的存储指令刷新到内存。 StoreLoad Barriers Store1; StoreLoad; Load2 确保Store1数据对其他处理器变得可见（指刷新到内存），之前于Load2及所有后续装载指令的装载。StoreLoad Barriers会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。 StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他三个屏障的效果。现代的多处理器大都支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（buffer fully flush）。 happens-before从JDK5开始，java使用新的JSR -133内存模型（本文除非特别说明，针对的都是JSR- 133内存模型）。JSR-133提出了happens-before的概念，通过这个概念来阐述操作之间的内存可见性。如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在happens-before关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 与程序员密切相关的happens-before规则如下： 程序顺序规则：一个线程中的每个操作，happens- before 于该线程中的任意后续操作。 监视器锁规则：对一个监视器锁的解锁，happens- before 于随后对这个监视器锁的加锁。 volatile变量规则：对一个volatile域的写，happens- before 于任意后续对这个volatile域的读。 传递性：如果A happens- before B，且B happens- before C，那么A happens- before C。 注意，两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。happens- before的定义很微妙，后文会具体说明happens-before为什么要这么定义。 happens-before与JMM的关系如下图所示： 如上图所示，一个happens-before规则通常对应于多个编译器重排序规则和处理器重排序规则。对于java程序员来说，happens-before规则简单易懂，它避免程序员为了理解JMM提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现。 原文地址","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"},{"name":"JMM","slug":"JMM","permalink":"http://lexburner.github.io/tags/JMM/"}]},{"title":"浅析项目中的并发","slug":"浅析项目中的并发","date":"2017-02-22T03:31:52.000Z","updated":"2017-08-22T07:46:51.708Z","comments":true,"path":"2017/02/22/浅析项目中的并发/","link":"","permalink":"http://lexburner.github.io/2017/02/22/浅析项目中的并发/","excerpt":"前言控制并发的方法很多，我之前的两篇博客都有过介绍，从最基础的synchronized，juc中的lock，到数据库的行级锁，乐观锁，悲观锁，再到中间件级别的redis，zookeeper分布式锁。今天主要想讲的主题是“根据并发出现的具体业务场景，使用合理的控制并发手段”。 什么是并发由一个大家都了解的例子引入我们今天的主题：并发 123456789101112131415161718192021222324252627public class Demo1 &#123; public Integer count = 0; public static void main(String[] args) &#123; final Demo1 demo1 = new Demo1(); Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++)&#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; demo1.count++; &#125; &#125;); &#125; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"final count value:\"+demo1.count); &#125;&#125;console:final count value:973 这个过程中，类变量count就是共享资源，而++操作并不是线程安全的，而多个线程去对count执行++操作，并没有happens-before原则保障执行的先后顺序，导致了最终结果并不是想要的1000","text":"前言控制并发的方法很多，我之前的两篇博客都有过介绍，从最基础的synchronized，juc中的lock，到数据库的行级锁，乐观锁，悲观锁，再到中间件级别的redis，zookeeper分布式锁。今天主要想讲的主题是“根据并发出现的具体业务场景，使用合理的控制并发手段”。 什么是并发由一个大家都了解的例子引入我们今天的主题：并发 123456789101112131415161718192021222324252627public class Demo1 &#123; public Integer count = 0; public static void main(String[] args) &#123; final Demo1 demo1 = new Demo1(); Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++)&#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; demo1.count++; &#125; &#125;); &#125; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"final count value:\"+demo1.count); &#125;&#125;console:final count value:973 这个过程中，类变量count就是共享资源，而++操作并不是线程安全的，而多个线程去对count执行++操作，并没有happens-before原则保障执行的先后顺序，导致了最终结果并不是想要的1000 下面，我们把并发中的共享资源从类变量转移到数据库中。先来看看使用框架的情况，以JPA为例（充血模型） 1234567891011121314151617181920212223242526272829303132@Componentpublic class Demo2 &#123; @Autowired TestNumDao testNumDao; @Transactional public void test()&#123; TestNum testNum = testNumDao.findOne(\"1\"); testNum.setCount(testNum.getCount()+1); testNumDao.save(testNum); &#125;&#125;controller: @Autowired Demo2 demo2; @RequestMapping(\"test\") @ResponseBody public String test()&#123; Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++)&#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; demo2.test(); &#125; &#125;); &#125; return \"test\"; &#125; 数据库的记录 id count 1 344 初窥门径的程序员会认为事务最基本的ACID中便包含了原子性，但是事务的原子性和今天所讲的并发中的原子操作仅仅是名词上有点类似。而有点经验的程序员都能知道这中间发生了什么（下面细说），这只是暴露了项目中并发问题的冰山一角。 改成直接用sql如何呢（贫血模型）？ 1234567891011121314151617181920@RequestMapping(\"testSql\") @ResponseBody public String testSql() throws InterruptedException &#123; final CountDownLatch countDownLatch = new CountDownLatch(1000); long start = System.currentTimeMillis(); Executor executor = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++)&#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; jdbcTemplate.execute(\"update test_num set count = count + 1 where id = '1'\"); countDownLatch.countDown(); &#125; &#125;); &#125; countDownLatch.await(); long costTime =System.currentTimeMillis() - start; System.out.println(\"共花费：\"+costTime+\" s\"); return \"testSql\"; &#125; 数据库结果： count ： 1000 达到了预期效果这个例子我顺便记录了耗时,控制台打印:共花费：113 ms简单对比一下二，三两个例子，都是想对数据库的count进行+1操作，唯一的区别就是，后者的+1计算发生在数据库，而前者的计算依赖于事先查出来的值，并且计算发生在程序的内存中。而现在大部分的ORM框架的兴起，导致了写第二种代码的程序员变多，不注意并发的话，就会出现问题。下面我们来看看具体的业务场景。 业务场景 修改个人信息 修改商品信息 扣除账户余额，扣减库存 业务场景分析第一个场景，互联网如此众多的用户修改个人信息，这算不算并发？答案是：算也不算。算，从程序员角度来看，每一个用户请求进来，都是调用的同一个修改入口，具体一点，就是映射到controller层的同一个requestMapping，所以一定是并发的。不算，虽然程序是并发的，但是从用户角度来分析，每个人只可以修改自己的信息，所以，不同用户的操作其实是隔离的，所以不算“并发”。这也是为什么很多开发者，在日常开发中一直不注意并发控制，却也没有发生太大问题的原因，大多数初级程序员开发的还都是CRM，OA，CMS系统。 回到我们的并发，第一种业务场景，是可以使用如上模式的，对于一条用户数据的修改，我们允许程序员读取数据到内存中，内存计算修改（耗时操作），提交更改，提交事务。 1234567//Transaction startUser user = userDao.findById(\"1\");user.setName(\"newName\");user.setAge(user.getAge()+1);...//其他耗时操作userDao.save(user);//Transaction commit 这个场景变现为：几乎不存在并发，不需要控制，场景乐观。 为了严谨，也可以选择控制并发，但我觉得这需要交给写这段代码的同事，让他自由发挥。第二个场景已经有所不同了，同样是修改一个记录，但是系统中可能有多个操作员来维护，此时，商品数据表现为一个共享数据，所以存在微弱的并发，通常表现为数据的脏读，例如操作员A，B同时对一个商品信息维护，我们希望只能有一个操作员修改成功，另外一个操作员得到错误提示（该商品信息已经发生变化），否则，两个人都以为自己修改成功了，但是其实只有一个人完成了操作，另一个人的操作被覆盖了。 这个场景表现为：存在并发，需要控制，允许失败，场景乐观。 通常我建议这种场景使用乐观锁，即在商品属性添加一个version字段标记修改的版本，这样两个操作员拿到同一个版本号，第一个操作员修改成功后版本号变化，另一个操作员的修改就会失败了。 1234567891011121314151617class Goods&#123; @Version int version;&#125;//Transaction starttry&#123; Goods goods = goodsDao.findById(\"1\"); goods.setName(\"newName\"); goods.setPrice(goods.getPrice()+100.00); ...//其他耗时操作 goodsDao.save(goods);&#125;catch(org.hibernate.StaleObjectStateException e)&#123; //返回给前台&#125;//Transaction commit springdata配合jpa可以自动捕获version异常，也可以自动手动对比。 第三个场景这个场景表现为：存在频繁的并发，需要控制，不允许失败，场景悲观。 强调一下，本例不应该使用在项目中，只是为了举例而设置的一个场景，因为这种贫血模型无法满足复杂的业务场景，而且依靠单机事务来保证一致性，并发性能和可扩展性能不好。 一个秒杀场景，大量请求在短时间涌入，是不可能像第二种场景一样，100个并发请求，一个成功，其他99个全部异常的。 设计方案应该达到的效果是：有足够库存时，允许并发，库存到0时，之后的请求全部失败；有足够金额时，允许并发，金额不够支付时立刻告知余额不足。 可以利用数据库的行级锁，update set balance = balance - money where userId = ? and balance &gt;= money;update stock = stock - number where goodsId = ? and stock &gt;= number ; 然后在后台 查看返回值是否影响行数为1，判断请求是否成功，利用数据库保证并发。 需要补充一点，我这里所讲的秒杀，并不是指双11那种级别的秒杀，那需要多层架构去控制并发，前端拦截，负载均衡….不能仅仅依赖于数据库的，会导致严重的性能问题。为了留一下一个直观的感受，这里对比一下oracle，mysql的两个主流存储引擎：innodb，myisam的性能问题。123456oracle:10000个线程共计1000000次并发请求：共花费：101017 ms =&gt;101sinnodb:10000个线程共计1000000次并发请求：共花费：550330 ms =&gt;550smyisam:10000个线程共计1000000次并发请求：共花费：75802 ms =&gt;75s 可见，如果真正有大量请求到达数据库，光是依靠数据库解决并发是不现实的，所以仅仅只用数据库来做保障而不是完全依赖。需要根据业务场景选择合适的控制并发手段。 后续，待补充分布式锁控制并发…浅析队列在并发场景中的地位…","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/categories/架构设计/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"},{"name":"多线程","slug":"多线程","permalink":"http://lexburner.github.io/tags/多线程/"}]},{"title":"聊聊IT行业应届生求职","slug":"聊聊IT行业应届生求职","date":"2017-02-19T16:57:52.000Z","updated":"2017-08-22T07:50:12.532Z","comments":true,"path":"2017/02/20/聊聊IT行业应届生求职/","link":"","permalink":"http://lexburner.github.io/2017/02/20/聊聊IT行业应届生求职/","excerpt":"前言回首大三下的暑假，那时候刚开始出来找实习，如今已经即将进入大四下学期，恍惚间，已经过去了8，9个月。写这篇文章的初衷就是想结合自己的经验给即将要出来找工作的应届生一些建议，想当初自己刚出来时，也得到过热心学长的教导，权当一种传递吧。 个人经历坐标上海，目前在一家IT软件公司从事电子商务，金融保险类的网站开发，主要使用的语言是JAVA。从任职的3-4个月起，开始担任项目小组长协同项目经理进行开发。期间由于技术总监常驻广州的原因，我兼任了上海分部这一块的面试工作，主要负责技术部分的面试（TMD工资却没涨T__T）。所以对广大来面试者的水平，以及公司想要的人才都有了更深的了解；有了面试经验后，一些观念也有了转变。 面试杂谈大四肯定很多人想出来找实习，但是又完全没有任何经验，这就很尴尬了，我先来说一些一定要注意的点。 不要乱投简历，现在互联网上有很多培训机构，中介机构，打着招聘的牌子，背后却干着培训的勾当。通常是对一些基础不太好的同学进行技术面试，对他们的信心造成碾压，而后，提出培训后入职的建议。通常这类公司就是通过这种手段去拉人培训，招人根本不是初衷。所以，要问清楚公司的情况，有必要面试之前先去百度搜一搜公司的基本情况和评价。 紧接着上面那点，可以通过一些业界信誉比较高的app或者网站去筛选公司。如BOSS直聘，拉钩，51job，前程无忧…特别是前面两个，是专门给程序员招聘使用的，针对性很强，对自己能力有了解的同学也可以量力而行，挑选适合自己的岗位。 投简历之前搞清楚公司的性质。IT行业目前大方向就分为两类：软件公司，互联网公司。我当初刚进公司的时候甲方乙方都搞不清楚，大家可能一下子也不知道这两种公司性质有什么区别。可以参照知乎这个问题的讨论https://www.zhihu.com/question/20274106/answer/40996303，简单来说同样的能力：软件公司轻松，钱少；互联网公司累，钱多。软件公司中又有外企，民营，国资等划分，工作性质又分为外包，自营...外包又分为人力外包和项目外包...互联网公司一说，大家肯定都知道BAT，京东，谷歌...还有一个层面的划分就是，软件公司大多提供的是服务，互联网公司通常都有自己的产品，不过这么说不够严谨，权当个参考吧。","text":"前言回首大三下的暑假，那时候刚开始出来找实习，如今已经即将进入大四下学期，恍惚间，已经过去了8，9个月。写这篇文章的初衷就是想结合自己的经验给即将要出来找工作的应届生一些建议，想当初自己刚出来时，也得到过热心学长的教导，权当一种传递吧。 个人经历坐标上海，目前在一家IT软件公司从事电子商务，金融保险类的网站开发，主要使用的语言是JAVA。从任职的3-4个月起，开始担任项目小组长协同项目经理进行开发。期间由于技术总监常驻广州的原因，我兼任了上海分部这一块的面试工作，主要负责技术部分的面试（TMD工资却没涨T__T）。所以对广大来面试者的水平，以及公司想要的人才都有了更深的了解；有了面试经验后，一些观念也有了转变。 面试杂谈大四肯定很多人想出来找实习，但是又完全没有任何经验，这就很尴尬了，我先来说一些一定要注意的点。 不要乱投简历，现在互联网上有很多培训机构，中介机构，打着招聘的牌子，背后却干着培训的勾当。通常是对一些基础不太好的同学进行技术面试，对他们的信心造成碾压，而后，提出培训后入职的建议。通常这类公司就是通过这种手段去拉人培训，招人根本不是初衷。所以，要问清楚公司的情况，有必要面试之前先去百度搜一搜公司的基本情况和评价。 紧接着上面那点，可以通过一些业界信誉比较高的app或者网站去筛选公司。如BOSS直聘，拉钩，51job，前程无忧…特别是前面两个，是专门给程序员招聘使用的，针对性很强，对自己能力有了解的同学也可以量力而行，挑选适合自己的岗位。 投简历之前搞清楚公司的性质。IT行业目前大方向就分为两类：软件公司，互联网公司。我当初刚进公司的时候甲方乙方都搞不清楚，大家可能一下子也不知道这两种公司性质有什么区别。可以参照知乎这个问题的讨论https://www.zhihu.com/question/20274106/answer/40996303，简单来说同样的能力：软件公司轻松，钱少；互联网公司累，钱多。软件公司中又有外企，民营，国资等划分，工作性质又分为外包，自营...外包又分为人力外包和项目外包...互联网公司一说，大家肯定都知道BAT，京东，谷歌...还有一个层面的划分就是，软件公司大多提供的是服务，互联网公司通常都有自己的产品，不过这么说不够严谨，权当个参考吧。 下面说一说这么多公司，怎么挑选适合自己的岗位。有很多的参考项，个人的能力，期望的工作地点以及地域的工资水准，未来的职业规划，房价，对象，水土气候，人脉等等诸多因素。本人是干java的，所以就以java求职来做例子，其他职业，专业请结合自己的专业知识做好对比即可。全部以上海为准，上海的起薪大概是2.8K左右，这叫基本工资，其他城市，例如无锡，苏州，大概在2.3k左右，视经济发展程度而定，先有个大概了解。 下面来看看具体招聘需求A类： Java 6K-12K职位描述 人品过硬。愿意追随项目长期发展。有能力。 有阅历。 有学历。符合PSD原则，即出身贫寒、渴望成功、聪明机智。 不需要我吐槽了吧，这种明明是招技术岗，却对技术没有要求的，估计能骗一些小白去面试，只有技术一无所知，才会退而去要求人品，试想一下，你啥都不会，也只能要求你人品过关了。 B类： 职位描述 任职要求：1) 大专或以上学历，计算机相关专业，1-3年以上软件开发经验；2) 熟练掌握Java开发技术，j2ee平台的核心技术的原理：jsp、ajax、servlet，jdbc等；3) 熟练掌握一种主流数据库：MySQL/sql,server/oracle/DB2，熟悉一种应用服务器的配置：tomcat/jboss/weblogic/websphere；4) 熟悉和理解Java开发各层次框架，如struts、spring、hiberate等，掌握基本Web前台技术；5) 热爱开发工作，具备良好的程序开发驾驭能力，需求分析把握能力；6) 好的沟通和解能力，善于团队合作，逻辑思维强，能够独立思考。 此文我是想写给应届生的，1-3年的工作经验没那么恐怖，大多数情况下，你的能力够了，公司不会跟你较真，用年限压你，所以看到自己技术水平能够达到，资历却不符合的岗位，也可以尝试着投一投。这类公司其实已经算是对技术有了要求了，而且技术细节都明确了出来，但是，看到只对jsp，servlet这些技术有所要求，明眼人都知道，这是在招初级开发，了解一点框架，懂计算机基础，这样的新手，公司还是可以接受的，上海这边针对可以独立开发的应届生，或者培训班出来可以直接上手的非科班生：软件公司，实习开价大概在4-5k，转正开价大概在7-8k；互联网公司实习大概在5-6k，转正开价9-10k起步。985/211或者能力不错能够入职的高校生，在互联网名企的开价，就以阿里为例，我了解到的情况大概是12k14 or 1216。这里都是说一个上海地区价格，不适用与全国。北京的情况是IT非常发达，很多互联网公司都在北京，而上海，深圳，广州其次，注意，上海是金融之都，并非IT之都。 C类 ： Java工程师 13K-21K任职资格 1)大学本科或以上学历，计算机相关专业；2)熟练掌握core java以及主流java框架，3)熟悉HTML5、CSS3、JAVASCRIPT、JQUERY等前端技术；4)熟练掌握面向对象的设计原则，熟悉JAVA设计模式，具备一定的系统架构设计能力；5)熟悉常用的互联网相关技术产品和中间件，例如redis，elasticSearch，activeMq，Dubbo等；6)能够带领开发小组独立完成产品功能的模块设计和研发；7)熟悉面向服务的开发，有大型互联网项目的开发/设计经验优先；8)较强的上进心和求知欲，善于学习和运用新知识，善于沟通和逻辑表达，有强烈的团队意识和执行力。 没找到特别适合本科生的描述，简单概括下这类公司，按照招聘要求来说吧。对计算机专业做要求，说明希望应聘者的专业素质有所保障，懂得基本的操作系统原理，数据结构，编译原理…因为这些都是本科期间必学的。对core java有掌握，说明是要招java岗位，基础必须牢固。前端知识有所了解，说明要懂得如果跟前端人员交互，不是完全的服务端开发设计模式和架构，说明不是要招只能够写增删改查的业务人员，更希望是那种能驱动团队的人才一系列中间件的要求说明企业比较正规，跟的上互联网的步伐，通常这类公司的技术总监是比较厉害的，发展前景不错dubbo一出来，说明该公司还是搞得分布式框架，微服务架构，对程序员的要求更上了一个档次 综合来看，具备以上素质的人当然配得上高一点的工资。 简历简历不要弄虚作假，什么东西是自己做的，什么东西不是自己做的，面试官一句话就能问出来。我面试过的很多人把自己的项目技能写的天花乱坠，随便问一个东西，都不能说个所以然出来，你还写了干嘛，徒增尴尬。 简历不要写与应聘岗位相差太大的描述，如果写了，也要能自圆其说，为什么体现出了自己的才能。我看过一个应聘JAVA后端的“人才”写着有普通话证书，来，我现场让你说一段绕口令？还有诸如“参加XXX比赛，虽然没得奖，但是自己得到了锻炼”之类的话，真的有必要写在简历上面吗？ 真是没得写的，可以说一说自己大学里面参加的活动体现出怎么样的能力，自己的优异表现，学分绩点，专业课程知识等等。要是实在一无可写…算了，那还是写普通话证书吧。 有项目经验，比赛经历，专业技能证书，英语考级证书的务必要写上（排名分先后）。都是应届生吗，注意一些技巧，如果你其他方面很突出，但是英语不行，只过了4级，那就别写英语4级了，因为会暴露你没有过6级。用其他证书掩盖过去。这不是欺骗，而是扬长避短。 简历得体大方，模板到处有，关于应届生求职简历的事，可以到知乎好好看看。 公司的诉求普通公司找人，一是看人的基础水平符不符合岗位需求，二是看人的素质符不符合团队的理念，再者就是追求一个性价比。 不是说你能力够了我就要招你，有些时候，公司就是要招基础的业务人员，你技术太厉害，要价太高，完全没必要招你。一个公司的垂直分层，必然是金字塔结构。所以讲究一个对号入座，搞清楚自己的能力，搞清楚自己想要什么样的一份岗位，投简历之前好好看看岗位的描述，公司的诉求。 我面了前前后后也快30多个人了，有很多培训班出来的非科班生，很多应届或者一年经验的人，985/211也有，工作了12年的人也有，说实话，能力也就这样，能力很强的人要么出国了，要么内推进了名企，我就只能从我接触到的这些人，说出一些看法。资历在我看来不是很重要，仅仅作为一个参考的位面，好几个工作了3-4年的人我感觉好不如咱们应届生，不追求技术的突破，一直干着增删改查操作，问一些JAVA基础性的知识又一无所知，要价有得太低，体现出对自己的不自信，有得太高，不清楚自己的定位，入职率很低。再加上现在公司都是对分布式架构的开发，需要的从业者的素质越来越高。整个互联网的趋势也是如此，没有什么人是突然就变得很厉害的，我司技术总监拥有着这么厉害的技术，在我所知也是靠着毕业后依旧数年如一日的对技术的热忱追求。所以，特别是IT互联网行业，更希望找到的，是有一颗学习的心，具备终身学习能力的人，以应对日新月异的互联网技术变更。 最后大多数人还是需要有自己的思考，此文谨代表个人看法供大家参考。","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/categories/技术杂谈/"}],"tags":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"http://lexburner.github.io/tags/技术杂谈/"},{"name":"求职","slug":"求职","permalink":"http://lexburner.github.io/tags/求职/"}]},{"title":"《微服务》九大特性笔记","slug":"《微服务》九大特性笔记","date":"2017-02-18T17:05:52.000Z","updated":"2017-08-22T07:52:39.139Z","comments":true,"path":"2017/02/19/《微服务》九大特性笔记/","link":"","permalink":"http://lexburner.github.io/2017/02/19/《微服务》九大特性笔记/","excerpt":"","text":"服务组件化组件，是一个可以独立更换和升级的单元。就像PC中的CPU、内存、显卡、硬盘一样，独立且可以更换升级而不影响其他单元。 在“微服务”架构中，需要我们对服务进行组件化分解。服务，是一种进程外的组件，它通过http等通信协议进行协作，而不是传统组件以嵌入的方式协同工作。服务都独立开发、部署，可以有效的避免一个服务的修改引起整个系统的重新部署。 打一个不恰当的比喻，如果我们的PC组件以服务的方式构建，我们只维护主板和一些必要外设之后，计算能力通过一组外部服务实现，我们只需要告诉PC我们从哪个地址来获得计算能力，通过服务定义的计算接口来实现我们使用过程中的计算需求，从而实现CPU组件的服务化。这样我们原本复杂的PC服务得到了更轻量化的实现，我们甚至只需要更换服务地址就能升级我们PC的计算能力。 按业务组织团队当我们开始决定如何划分“微服务”时，通常也意味着我们要开始对团队进行重新规划与组织。按以往的方式，我们往往会以技术的层面去划分多个不同的团队，比如：DBA团队、运维团队、后端团队、前端团队、设计师团队等等。若我们继续按这种方式组织团队来实施“微服务”架构开发时，当有一个有问题需要更改，可能是一个非常简单的变动，比如：对人物描述增加一个字段，这就需要从数据存储开始考虑一直到设计和前端，虽然大家的修改都非常小，但这会引起跨团队的时间和预算审批。 在实施“微服务”架构时，需要采用不同的团队分割方法。由于每一个微服务都是针对特定业务的宽栈或是全栈实现，既要负责数据的持久化存储，又要负责用户的接口定义等各种跨专业领域的职能。因此，面对大型项目时候，对于微服务团队拆分更加建议按业务线的方式进行拆分，一方面可以有效减少服务内部修改所产生的内耗；另一方面，团队边界可以变得更为清晰。 做“产品”的态度实施“微服务”架构的团队中，每个小团队都应该以做产品的方式，对其产品的整个生命周期负责。而不是以项目的模式，以完成开发与交付并将成果交接给维护者为最终目标。 开发团队通过了解服务在具体生产环境中的情况，可以增加他们对具体业务的理解，比如：很多时候一些业务中发生的特殊或异常情况，很可能产品经理都并不知晓，但细心的开发者很容易通过生产环境发现这些特殊的潜在问题或需求。 所以，我们需要用做“产品”的态度来对待每一个“微服务”，持续关注服务的运作情况，并不断地分析帮助用户来提升业务功能。 智能端点与哑管道在单体应用中，组件间直接通过函数调用的方式进行交互协作。而在“微服务”架构中，服务由于不在一个进程中，组件间的通信模式发生了改变，若仅仅将原本在进程内的方法调用改成RPC方式的调用，会导致微服务之间产生繁琐的通信，使得系统表现更为糟糕，所以，我们需要更粗粒度的通信协议。 在“微服务”架构中，通常会使用这两个服务调用方式： 第一种，使用HTTP协议的RESTful API或轻量级的消息发送协议，来实现信息传递与服务调用的触发。第二种，通过在轻量级消息总线上传递消息，类似RabbitMQ等一些提供可靠异步交换的结构。 在极度强调性能的情况下，有些团队会使用二进制的消息发送协议，例如：protobuf。即使是这样，这些系统仍然会呈现出“智能端点和哑管道”的特点，为了在易读性与高效性之间取得平衡。当然大多数Web应用或企业系统并不需要作出在这两者间做出选择，能够获得易读性就已经是一个极大的胜利了。——Martin Fowler 去中心化治理当我们采用集中化的架构治理方案时，通常在技术平台上都会做同一的标准，但是每一种技术平台都有其短板，这会导致在碰到短板时，不得不花费大力气去解决，并且可能还是因为其底层原因解决的不是很好。 在实施“微服务”架构时，通过采用轻量级的契约定义接口，使得我们对于服务本身的具体技术平台不再那么敏感，这样我们整个“微服务”架构的系统中的组件就能针对其不同的业务特点选择不同的技术平台，终于不会出现杀鸡用牛刀或是杀牛用指甲钳的尴尬处境了。 不是每一个问题都是钉子，不是每一个解决方案都是锤子 去中心化管理数据我们在实施“微服务”架构时，都希望可以让每一个服务来管理其自有的数据库，这就是数据管理的去中心化。 在去中心化过程中，我们除了将原数据库中的存储内容拆分到新的同平台的其他数据库实例中之外（如：把原本存储在MySQL中的表拆分后，存储多几个不同的MySQL实例中），也可以针对一些具有特殊结构或业务特性的数据存储到一些其他技术的数据库实例中（如：把日志信息存储到MongoDB中、把用户登录信息存储到Redis中）。 虽然，数据管理的去中心化可以让数据管理更加细致化，通过采用更合适的技术来让数据存储和性能达到最优。但是，由于数据存储于不同的数据库实例中后，数据一致性也成为“微服务”架构中急需解决的问题之一。分布式事务的实现，本身难度就非常大，所以在“微服务”架构中，我们更强调在各服务之间进行“无事务”的调用，而对于数据一致性，只要求数据在最后的处理状态是一致的效果；若在过程中发现错误，通过补偿机制来进行处理，使得错误数据能够达到最终的一致性。 基础设施自动化近年来云计算服务与容器化技术的不断成熟，运维基础设施的工作变得越来越不那么难了。但是，当我们实施“微服务”架构时，数据库、应用程序的个头虽然都变小了，但是因为拆分的原因，数量成倍的增长。这使得运维人员需要关注的内容也成倍的增长，并且操作性任务也会成倍的增长，这些问题若没有得到妥善的解决，必将成为运维人员的噩梦。 所以，在“微服务”架构中，请务必从一开始就构建起“持续交付”平台来支撑整个实施过程，该平台需要两大内容，不可或缺： 自动化测试：每次部署前的强心剂，尽可能的获得对正在运行软件的信心。自动化部署：解放繁琐枯燥的重复操作以及对多环境的配置管理。 容错设计在单体应用中，一般不存在单个组件故障而其他还在运行的情况，通常是一挂全挂。而在“微服务”架构中，由于服务都运行在独立的进程中，所以是存在部分服务出现故障，而其他服务都正常运行的情况，比如：当正常运作的服务B调用到故障服务A时，因故障服务A没有返回，线程挂起开始等待，直到超时才能释放，而此时若触发服务B调用服务A的请求来自服务C，而服务C频繁调用服务B时，由于其依赖服务A，大量线程被挂起等待，最后导致服务A也不能正常服务，这时就会出现故障的蔓延。 所以，在“微服务”架构中，快速的检测出故障源并尽可能的自动恢复服务是必须要被设计和考虑的。通常，我们都希望在每个服务中实现监控和日志记录的组件，比如：服务状态、断路器状态、吞吐量、网络延迟等关键数据的仪表盘等。 演进式设计通过上面的几点特征，我们已经能够体会到，要实施一个完美的“微服务”架构，需要考虑的设计与成本并不小，对于没有足够经验的团队来说，甚至要比单体应用发付出更多的代价。 所以，很多情况下，架构师们都会以演进的方式进行系统的构建，在初期系统以单体系统的方式来设计和实施，一方面系统体量初期并不会很大，构建和维护成本都不高。另一方面，初期的核心业务在后期通常也不会发生巨大的改变。随着系统的发展或者业务的需要，架构师们会将一些经常变动或是有一定时间效应的内容进行“微服务”处理，并逐渐地将原来在单体系统中多变的模块逐步拆分出来，而稳定不太变化的就形成了一个核心“微服务”存在于整个架构之中。 原文由 程序猿DD-翟永超 创作转载自《微服务》九大特性笔记","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/categories/架构设计/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://lexburner.github.io/tags/微服务/"}]},{"title":"ThreadLocal的最佳实践","slug":"ThreadLocal的最佳实践","date":"2017-02-14T09:38:52.000Z","updated":"2017-08-22T07:56:03.082Z","comments":true,"path":"2017/02/14/ThreadLocal的最佳实践/","link":"","permalink":"http://lexburner.github.io/2017/02/14/ThreadLocal的最佳实践/","excerpt":"","text":"SimpleDateFormat众所周知是线程不安全的，多线程中如何保证线程安全又同时兼顾性能问题呢？那就是使用ThreadLocal维护SimpleDateFormat 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class SimpleDateFormatThreadTest &#123; static volatile AtomicInteger n = new AtomicInteger(-1); static ThreadLocal&lt;DateFormat&gt; sdf ; static &#123; sdf =new ThreadLocal&lt;DateFormat&gt;() &#123; @Override protected DateFormat initialValue() &#123; return new SimpleDateFormat(\"yyyy-MM-dd\"); &#125; &#125;; &#125; public static void main(String[] args) throws ParseException, InterruptedException &#123; Set&lt;String&gt; dateSet = new ConcurrentHashSet&lt;&gt;(); Set&lt;Integer&gt; numberSet = new ConcurrentHashSet&lt;&gt;(); Date[] dates = new Date[1000]; for (int i = 0; i &lt; 1000; i++) &#123; dates[i] = sdf.get().parse(i + 1000 + \"-11-22\"); &#125; ExecutorService executorService = Executors.newFixedThreadPool(10); for(int i=0;i&lt;1000;i++)&#123; executorService.execute(new Runnable() &#123; @Override public void run() &#123; int number = n.incrementAndGet(); String date = sdf.get().format(dates[number]); numberSet.add(number); dateSet.add(date); System.out.println(number+\" \"+date); &#125; &#125;); &#125; executorService.shutdown(); Thread.sleep(5000); System.out.println(dateSet.size()); System.out.println(numberSet.size()); &#125;&#125; 实践证明sdf的parse（String to Date）有严重的线程安全问题，format（Date to String）有轻微的线程安全问题，虽然不太明显，但还是会出现问题，这和内部的实现有关。 简单分析下使用ThreadLocal的好处，1000次转换操作，10个线程争抢执行，如果每次都去new 一个sdf，可见其效率之低，而使用ThreadLocal，是对每个线程维护一个sdf，所以最多就只会出现10个sdf，真正项目中，由于操作系统线程分片执行，所以线程不会非常的多，使用ThreadLocal的好处也就立竿见影了。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"},{"name":"多线程","slug":"多线程","permalink":"http://lexburner.github.io/tags/多线程/"}]},{"title":"简单了解RPC实现原理","slug":"简单了解RPC实现原理","date":"2017-02-10T07:11:52.000Z","updated":"2017-08-22T08:19:51.166Z","comments":true,"path":"2017/02/10/简单了解RPC实现原理/","link":"","permalink":"http://lexburner.github.io/2017/02/10/简单了解RPC实现原理/","excerpt":"时下很多企业应用更新换代到分布式，一篇文章了解什么是RPC。原作者梁飞，在此记录下他非常简洁的rpc实现思路。","text":"时下很多企业应用更新换代到分布式，一篇文章了解什么是RPC。原作者梁飞，在此记录下他非常简洁的rpc实现思路。 核心框架类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (\"Confidential * Information\"). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.framework;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.net.ServerSocket;import java.net.Socket;/** * RpcFramework * * @author william.liangf */public class RpcFramework &#123; /** * 暴露服务 * * @param service 服务实现 * @param port 服务端口 * @throws Exception */ public static void export(final Object service, int port) throws Exception &#123; if (service == null) throw new IllegalArgumentException(\"service instance == null\"); if (port &lt;= 0 || port &gt; 65535) throw new IllegalArgumentException(\"Invalid port \" + port); System.out.println(\"Export service \" + service.getClass().getName() + \" on port \" + port); ServerSocket server = new ServerSocket(port); for(;;) &#123; try &#123; final Socket socket = server.accept(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; try &#123; ObjectInputStream input = new ObjectInputStream(socket.getInputStream()); try &#123; String methodName = input.readUTF(); Class&lt;?&gt;[] parameterTypes = (Class&lt;?&gt;[])input.readObject(); Object[] arguments = (Object[])input.readObject(); ObjectOutputStream output = new ObjectOutputStream(socket.getOutputStream()); try &#123; Method method = service.getClass().getMethod(methodName, parameterTypes); Object result = method.invoke(service, arguments); output.writeObject(result); &#125; catch (Throwable t) &#123; output.writeObject(t); &#125; finally &#123; output.close(); &#125; &#125; finally &#123; input.close(); &#125; &#125; finally &#123; socket.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 引用服务 * * @param &lt;T&gt; 接口泛型 * @param interfaceClass 接口类型 * @param host 服务器主机名 * @param port 服务器端口 * @return 远程服务 * @throws Exception */ @SuppressWarnings(\"unchecked\") public static &lt;T&gt; T refer(final Class&lt;T&gt; interfaceClass, final String host, final int port) throws Exception &#123; if (interfaceClass == null) throw new IllegalArgumentException(\"Interface class == null\"); if (! interfaceClass.isInterface()) throw new IllegalArgumentException(\"The \" + interfaceClass.getName() + \" must be interface class!\"); if (host == null || host.length() == 0) throw new IllegalArgumentException(\"Host == null!\"); if (port &lt;= 0 || port &gt; 65535) throw new IllegalArgumentException(\"Invalid port \" + port); System.out.println(\"Get remote service \" + interfaceClass.getName() + \" from server \" + host + \":\" + port); return (T) Proxy.newProxyInstance(interfaceClass.getClassLoader(), new Class&lt;?&gt;[] &#123;interfaceClass&#125;, new InvocationHandler() &#123; public Object invoke(Object proxy, Method method, Object[] arguments) throws Throwable &#123; Socket socket = new Socket(host, port); try &#123; ObjectOutputStream output = new ObjectOutputStream(socket.getOutputStream()); try &#123; output.writeUTF(method.getName()); output.writeObject(method.getParameterTypes()); output.writeObject(arguments); ObjectInputStream input = new ObjectInputStream(socket.getInputStream()); try &#123; Object result = input.readObject(); if (result instanceof Throwable) &#123; throw (Throwable) result; &#125; return result; &#125; finally &#123; input.close(); &#125; &#125; finally &#123; output.close(); &#125; &#125; finally &#123; socket.close(); &#125; &#125; &#125;); &#125;&#125; 定义服务接口12345678910111213141516171819/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (\"Confidential * Information\"). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.test;/** * HelloService * * @author william.liangf */public interface HelloService &#123; String hello(String name);&#125; 实现服务123456789101112131415161718192021/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (\"Confidential * Information\"). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.test;/** * HelloServiceImpl * * @author william.liangf */public class HelloServiceImpl implements HelloService &#123; public String hello(String name) &#123; return \"Hello \" + name; &#125;&#125; 暴露服务123456789101112131415161718192021222324/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (\"Confidential * Information\"). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.test;import com.alibaba.study.rpc.framework.RpcFramework;/** * RpcProvider * * @author william.liangf */public class RpcProvider &#123; public static void main(String[] args) throws Exception &#123; HelloService service = new HelloServiceImpl(); RpcFramework.export(service, 1234); &#125;&#125; 引用服务12345678910111213141516171819202122232425262728/* * Copyright 2011 Alibaba.com All right reserved. This software is the * confidential and proprietary information of Alibaba.com (\"Confidential * Information\"). You shall not disclose such Confidential Information and shall * use it only in accordance with the terms of the license agreement you entered * into with Alibaba.com. */package com.alibaba.study.rpc.test;import com.alibaba.study.rpc.framework.RpcFramework;/** * RpcConsumer * * @author william.liangf */public class RpcConsumer &#123; public static void main(String[] args) throws Exception &#123; HelloService service = RpcFramework.refer(HelloService.class, \"127.0.0.1\", 1234); for (int i = 0; i &lt; Integer.MAX_VALUE; i ++) &#123; String hello = service.hello(\"World\" + i); System.out.println(hello); Thread.sleep(1000); &#125; &#125; &#125; 总结这个简单的例子的实现思路是使用阻塞的socket IO流来进行server和client的通信，也就是rpc应用中服务提供方和服务消费方。并且是端对端的，用端口号来直接进行通信。方法的远程调用使用的是jdk的动态代理，参数的序列化也是使用的最简单的objectStream。 真实的rpc框架会对上面的实现方式进行替换，采用更快更稳定，更高可用易扩展，更适宜分布式场景的中间件，技术来替换。例如使用netty的nio特性达到非阻塞的通信，使用zookeeper统一管理服务注册与发现，解决了端对端不灵活的劣势。代理方式有cglib字节码技术。序列化方式有hession2，fastjson等等。不过梁飞大大的博客使用原生的jdk api就展现给各位读者一个生动形象的rpc demo，实在是强。rpc框架解决的不仅仅是技术层面的实现，还考虑到了rpc调用中的诸多问题，重试机制，超时配置…这些就需要去了解成熟的rpc框架是如果考虑这些问题的了。 推荐一个轻量级的rpc框架：motan。weibo团队在github开源的一个rpc框架，有相应的文档，用起来感觉比dubbo要轻量级，易上手。","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://lexburner.github.io/categories/架构设计/"}],"tags":[{"name":"RPC","slug":"RPC","permalink":"http://lexburner.github.io/tags/RPC/"}]},{"title":"java trick--String.intern()","slug":"java trick--String.intern()","date":"2016-11-07T15:16:52.000Z","updated":"2017-08-22T08:24:18.638Z","comments":true,"path":"2016/11/07/java trick--String.intern()/","link":"","permalink":"http://lexburner.github.io/2016/11/07/java trick--String.intern()/","excerpt":"","text":"《深入理解java虚拟机》第二版中对String.intern()方法的讲解中所举的例子非常有意思 不了解String.intern()的朋友要理解他其实也很容易，它返回的是一个字符串在字符串常亮池中的引用。直接看下面的demo 123456789public class Main &#123; public static void main(String[] args) &#123; String str1 = new StringBuilder(\"计算机\").append(\"软件\").toString(); System.out.println(str1.intern() == str1); String str2 = new StringBuilder(\"ja\").append(\"va\").toString(); System.out.println(str2.intern() == str2); &#125;&#125; 两者输出的结果如下： 12truefalse 我用的jdk版本为Oracle JDK7u45。简单来说，就是一个很奇怪的现象，为什么java这个字符串在类加载之前就已经加载到常量池了？ 我在知乎找到了具体的说明，如下： 1234567891011package sun.misc;import java.io.PrintStream;public class Version &#123; private static final String launcher_name = \"java\"; private static final String java_version = \"1.7.0_79\"; private static final String java_runtime_name = \"Java(TM) SE Runtime Environment\"; private static final String java_runtime_version = \"1.7.0_79-b15\"; ...&#125; 而HotSpot JVM的实现会在类加载时先调用： 123456789public final class System&#123; ... private static void initializeSystemClass() &#123; ... sun.misc.Version.init(); ... &#125; ...&#125; 原来是sun.misc.Version这个类在起作用。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"}]},{"title":"java trick--String.intern()","slug":"java trick--intergerCache","date":"2016-11-07T15:00:52.000Z","updated":"2017-08-22T08:27:03.061Z","comments":true,"path":"2016/11/07/java trick--intergerCache/","link":"","permalink":"http://lexburner.github.io/2016/11/07/java trick--intergerCache/","excerpt":"","text":"看一段代码： 1234567public class Main &#123; public static void main(String[] args) &#123; Integer a=100,b=100,c=150,d=150; System.out.println(a==b); System.out.println(c==d); &#125;&#125; 这段代码会输出什么？ 不加留意的人可能会理所当然的认为两个答案会是一致的，但结果却是： 12truefalse 下面一个很好解释，因为自动拆装箱机制，比较的是两者的引用，而不是值，所以为false，那么为什么前者是同一个引用呢？ 来看看Integer这个类，首先是自动拆装箱会调用valueOf()方法 123456public static Integer valueOf(int i) &#123; assert IntegerCache.high &gt;= 127; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; 这里并不是简单的返回new Integer(i) 而是判断了一下int的数值，Integer的存在一个缓存机制，默认用一个IntegerCache缓存了[IntegerCache.low,IntegerCache.high]的引用,其中IntegerCache这个内部类真正在做缓存 1234567891011121314151617181920212223242526private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(\"java.lang.Integer.IntegerCache.high\"); if (integerCacheHighPropValue != null) &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); &#125; private IntegerCache() &#123;&#125; &#125; 所以就出现了最开始的一个小trick","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"}]},{"title":"java trick--String.intern()","slug":"java trick--system.out.println","date":"2016-11-07T14:03:52.000Z","updated":"2017-08-22T08:28:34.717Z","comments":true,"path":"2016/11/07/java trick--system.out.println/","link":"","permalink":"http://lexburner.github.io/2016/11/07/java trick--system.out.println/","excerpt":"","text":"多线程在使用system.out.println时要留一个有意思的地方 123456789101112131415161718192021public class Main &#123; public static void main(String[] args) &#123; Thread thread = new MyThread(); thread.start(); System.out.println(\"end\"); &#125;&#125;class MyThread extends Thread &#123; private int i = 0; @Override public void run() &#123; while (true) &#123; i++; System.out.println(i); &#125; &#125;&#125; 主线程另起一个线程，然后在主线程最后打印一个end，猜猜看结果是什么？end会不会打印？主线程一直被Mythread占用原因就在于system.out.println是一个同步方法 12345678910111213/** * Prints an integer and then terminate the line. This method behaves as * though it invokes &lt;code&gt;&#123;@link #print(int)&#125;&lt;/code&gt; and then * &lt;code&gt;&#123;@link #println()&#125;&lt;/code&gt;. * * @param x The &lt;code&gt;int&lt;/code&gt; to be printed. */ public void println(int x) &#123; synchronized (this) &#123; print(x); newLine(); &#125; &#125;","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/tags/JAVA/"}]},{"title":"使用zkclient操作zookeeper的学习过程记录","slug":"使用zkclient操作zookeeper的学习过程记录","date":"2016-08-16T07:52:52.000Z","updated":"2017-08-22T08:19:15.173Z","comments":true,"path":"2016/08/16/使用zkclient操作zookeeper的学习过程记录/","link":"","permalink":"http://lexburner.github.io/2016/08/16/使用zkclient操作zookeeper的学习过程记录/","excerpt":"前言最近开发的分布式(使用motan)项目中使用zookeeper作为服务中心来提供注册服务(@MotanService)和发现服务(@MotanRefer),虽然motan这个rpc框架对服务模块进行了很好的封装，但是以防以后会出现定制化的需求，以及对服务更好的监控，所以有必要了解一下zookeeper的基本知识和使用方法。关于zookeeper的知识点，网上很多的博客都已经介绍的很详尽了，我写这篇的博客的用意其实也就是将一些零散的却很精妙的博客整理出来，方便以后查阅。短篇以cp的方式，长篇的以url的方式。 zookeeper是什么？ ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。ZooKeeper包含一个简单的原语集，提供Java和C的接口。 ZooKeeper代码版本中，提供了分布式独享锁、选举、队列的接口。 —-百度百科 一开始看的云里雾里的，幸好我之前搞过一点hadoop，对他的生态体系有所了解，这才大概知道他想说什么。提炼几个关键词，并且加入我后面学习的理解，总结一下就是– zookeeper是一个组件，需要安装客户端和服务端，一般用于解决分布式开发下的一些问题。化抽象为具体，你可以把整个zookeeper理解成一个树形数据结构，也可以理解为一个文件系统的结构，每个叶子节点都会携带一些信息(data)，并且也可能会携带一些操作(op)。分布式场景中，每一个客户端都可以访问到这些叶子节点，并且进行一些操作。我们所有使用zookeeper的场景几乎都是在CRUD某一个或者某些叶子节点，然后会触发对应的操作…即zookeeper本身可以理解为一个shareData。—-来自于博主的口胡 zookeeper怎么学？学一个新的中间件的最好方法是先在脑子里面有一个想法：我为什么要学他，是想解决什么问题，他大概是个什么东西，我觉得打开思路的最好方式是看几篇博客(大多数情况你一开始看不懂，但是混个眼熟)，然后看视频，这里我自己是了解过了zookeeper原生的api之后看了极客学院的视频","text":"前言最近开发的分布式(使用motan)项目中使用zookeeper作为服务中心来提供注册服务(@MotanService)和发现服务(@MotanRefer),虽然motan这个rpc框架对服务模块进行了很好的封装，但是以防以后会出现定制化的需求，以及对服务更好的监控，所以有必要了解一下zookeeper的基本知识和使用方法。关于zookeeper的知识点，网上很多的博客都已经介绍的很详尽了，我写这篇的博客的用意其实也就是将一些零散的却很精妙的博客整理出来，方便以后查阅。短篇以cp的方式，长篇的以url的方式。 zookeeper是什么？ ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。ZooKeeper包含一个简单的原语集，提供Java和C的接口。 ZooKeeper代码版本中，提供了分布式独享锁、选举、队列的接口。 —-百度百科 一开始看的云里雾里的，幸好我之前搞过一点hadoop，对他的生态体系有所了解，这才大概知道他想说什么。提炼几个关键词，并且加入我后面学习的理解，总结一下就是– zookeeper是一个组件，需要安装客户端和服务端，一般用于解决分布式开发下的一些问题。化抽象为具体，你可以把整个zookeeper理解成一个树形数据结构，也可以理解为一个文件系统的结构，每个叶子节点都会携带一些信息(data)，并且也可能会携带一些操作(op)。分布式场景中，每一个客户端都可以访问到这些叶子节点，并且进行一些操作。我们所有使用zookeeper的场景几乎都是在CRUD某一个或者某些叶子节点，然后会触发对应的操作…即zookeeper本身可以理解为一个shareData。—-来自于博主的口胡 zookeeper怎么学？学一个新的中间件的最好方法是先在脑子里面有一个想法：我为什么要学他，是想解决什么问题，他大概是个什么东西，我觉得打开思路的最好方式是看几篇博客(大多数情况你一开始看不懂，但是混个眼熟)，然后看视频，这里我自己是了解过了zookeeper原生的api之后看了极客学院的视频 zkclient的使用学完原生api之后一般我们不直接使用，类比redis的客户端jedis，再到spring提供的redisTemplate;类比jdbc到dbutils，再到orm框架。所以作为小白，我建议使用这个比较简单的客户端zkclient，当后期需求需要一些定制化需求时使用原生的api自己重写，或者使用更高级一点的其他客户端。 zkclient我学完之后觉得非常轻量级，设计也很规范，大概可以参考以下的博客。博客园-房继诺原作者非常用心，里面给出了一张zkclient的uml类图，如下顺便也复习一下uml类图的知识，理解清楚图中用到的聚合，组合，关联，泛化，实现的箭头含义。uml建模没有学好的同学的移步这个链接，里面对应了java讲解，还算详细。掌握这个客户端之后，还需要补充一些注意点 1. create方法:创建节点时,如果节点已经存在,仍然抛出NodeExistException,可是我期望它不在抛出此异常. 2. retryUtilConnected: 如果向zookeeper请求数据时(create,delete,setData等),此时链接不可用,那么调用者将会被阻塞直到链接建立成功;不过我仍然需要一些方法是非阻塞的,如果链接不可用,则抛出异常,或者直接返回. 3. create方法: 创建节点时,如果节点的父节点不存在,我期望同时也要创建父节点,而不是抛出异常. 4. data监测: 我需要提供一个额外的功能来补充watch的不足,开启一个线程,间歇性的去zk server获取指定的path的data,并缓存起来..归因与watch可能丢失,以及它不能持续的反应znode数据的每一次变化,所以只能手动去同步获取. 回到开始这个时候看看你当初为啥要学习zookeeper，看看能不能解决你当时遇到的问题。如果你有兴趣，可以自己去试试zookeeper前面提到的那些可以实现的功能：分布式锁、选举、队列等等","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://lexburner.github.io/tags/zookeeper/"}]},{"title":"使用JPA实现乐观锁","slug":"使用JPA实现乐观锁","date":"2016-08-16T07:52:52.000Z","updated":"2017-08-22T08:18:33.986Z","comments":true,"path":"2016/08/16/使用JPA实现乐观锁/","link":"","permalink":"http://lexburner.github.io/2016/08/16/使用JPA实现乐观锁/","excerpt":"乐观锁的概念就不再赘述了，不了解的朋友请自行百度谷歌之，今天主要说的是在项目中如何使用乐观锁，做成一个小demo。 持久层使用jpa时，默认提供了一个注解@Version先看看源码怎么描述这个注解的 1234@Target(&#123; METHOD, FIELD &#125;)@Retention(RUNTIME)public @interface Version &#123;&#125; 简单来说就是用一个version字段来充当乐观锁的作用。先来设计实体类 123456789101112131415161718192021/** * Created by xujingfeng on 2017/1/30. */@Entity@Table(name = \"t_student\")public class Student &#123; @Id @GenericGenerator(name = \"PKUUID\", strategy = \"uuid2\") @GeneratedValue(generator = \"PKUUID\") @Column(length = 36) private String id; @Version private int version; private String name; //getter()... //setter()...&#125;","text":"乐观锁的概念就不再赘述了，不了解的朋友请自行百度谷歌之，今天主要说的是在项目中如何使用乐观锁，做成一个小demo。 持久层使用jpa时，默认提供了一个注解@Version先看看源码怎么描述这个注解的 1234@Target(&#123; METHOD, FIELD &#125;)@Retention(RUNTIME)public @interface Version &#123;&#125; 简单来说就是用一个version字段来充当乐观锁的作用。先来设计实体类 123456789101112131415161718192021/** * Created by xujingfeng on 2017/1/30. */@Entity@Table(name = \"t_student\")public class Student &#123; @Id @GenericGenerator(name = \"PKUUID\", strategy = \"uuid2\") @GeneratedValue(generator = \"PKUUID\") @Column(length = 36) private String id; @Version private int version; private String name; //getter()... //setter()...&#125; Dao层 12345678910/** * Created by xujingfeng on 2017/1/30. */public interface StudentDao extends JpaRepository&lt;Student,String&gt;&#123; @Query(\"update Student set name=?1 where id=?2\") @Modifying @Transactional int updateNameById(String name,String id);&#125; Controller层充当单元测试的作用，通过访问一个requestMapping来触发我们想要测试的方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Created by xujingfeng on 2017/1/30. */@Controllerpublic class StudentController &#123; @Autowired StudentDao studentDao; @RequestMapping(\"student.html\") @ResponseBody public String student()&#123; Student student = new Student(); student.setName(\"xujingfeng\"); studentDao.save(student); return \"student\"; &#125; @RequestMapping(\"testVersion.html\") @ResponseBody public String testVersion() throws InterruptedException &#123; Student student = studentDao.findOne(\"6ed16acc-61df-4a66-add9-d17c88b69755\"); student.setName(\"xuxuan\"); new Thread(new Runnable() &#123; @Override public void run() &#123; studentDao.findOne(\"6ed16acc-61df-4a66-add9-d17c88b69755\"); student.setName(\"xuxuanInThread\"); studentDao.save(student); &#125; &#125;).start(); Thread.sleep(1000); studentDao.save(student); return \"testVersion\"; &#125; @RequestMapping(\"updateNameById.html\") @ResponseBody public String updateNameById()&#123; studentDao.updateNameById(\"xuxuan2\",\"6ed16acc-61df-4a66-add9-d17c88b69755\"); return \"updateNameById\"; &#125;&#125; 这里面三个方法，主要是我们想用来测试的三个注意点。第一个方法student.html我们想看看springdata如何对version字段进行增长的。就不贴图了，直接给结论，对于添加了@Version的注解，我们不需要手动去控制，每一次save操作会在原来的基础上+1，如果初始为null，则springdata自动设置其为0。第二个方法testVersion.html是乐观锁的核心，当多个线程并发访问同一行记录时，添加了@Version乐观锁之后，程序会进行怎么样的控制呢？ 1org.hibernate.StaleObjectStateException: Row was updated or deleted by another transaction (or unsaved-value mapping was incorrect) : [com.example.jpa.Student#6ed16acc-61df-4a66-add9-d17c88b69755] 异常信息如上，主线程和新线程获取了同一行记录，并且新线程优先提交了事务，版本号一致，修改成功。等到了主线程再想save提交事务时，便得到一个版本号不一致的异常，那么在项目开发中就应该自己捕获这个异常根据业务内容做对应处理，是重试还是放弃etc… 第三个方法，updateNameById.html是想强调一下，@Query中的update，delete操作是不会触发springdata的相关代理操作的，而是转化为原生sql的方式，所以在项目中使用时也要注意这点。 总结乐观锁，用在一些敏感业务数据上，而其本身的修饰：乐观，代表的含义便是相信大多数场景下version是一致的。但是从业务角度出发又要保证数据的严格一致性，避免脏读等问题，使用的场景需要斟酌。记得前面一片博文简单介绍了一下行级锁的概念，其实本质上和乐观锁都是想要再数据库层面加锁控制并发，那么什么时候该用乐观锁，行级锁，什么时候得在程序级别加同步锁，又要根据具体的业务场景去判断。找到能够满足自己项目需求的方案，找到性能和可靠性的平衡点，才是一个程序员的价值所在。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lexburner.github.io/categories/JAVA/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://lexburner.github.io/tags/多线程/"},{"name":"数据库","slug":"数据库","permalink":"http://lexburner.github.io/tags/数据库/"}]},{"title":"Hello World","slug":"hello-world","date":"2016-08-16T07:52:52.000Z","updated":"2017-08-22T04:31:48.389Z","comments":true,"path":"2016/08/16/hello-world/","link":"","permalink":"http://lexburner.github.io/2016/08/16/hello-world/","excerpt":"","text":"","categories":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lexburner.github.io/tags/Spring/"},{"name":"Validation","slug":"Validation","permalink":"http://lexburner.github.io/tags/Validation/"}]}]}